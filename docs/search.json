[
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Part\nWeek\nDay\nTopics\nLabs\nPapers\nReadings\n\n\n\n\nI\n1\nWed, Jan 15\nIntroduction\nLab 0\n\nCase study 1 files\n\n\n\n2\nWed, Jan 22\nFoundations\n\n\nPelham (2013, ch. 1)\n\n\n\n3\nWed, Jan 29\nTheory construction\n\nAnnotated bibliography\nFrisby (2024)\n\n\n\n4\nWed, Feb 5\nProbability theory\n\n\nGarcia, López, & Vélez (2018)\n\n\nII\n5\nWed, Feb 12\nUnivariate analysis\nLab 1\n\nCase study 2 files\n\n\n\n6\nWed, Feb 19\nBivariate analysis\n\n\n\n\n\n\n7\nWed, Feb 26\nExploratory analysis\n\nPaper 1\n\n\n\n\n8\nWed, Mar 12\nHypothesis testing\n\n\n\n\n\nIII\n9\nWed, Mar 19\nNotes on causal theories\n\n\nCase study 3 files\n\n\n\n10\nWed, Mar 26\nModeling social in/justice\nLab 2\n\n\n\n\n\n11\nWed, Apr 2\nOn regression\n\n\n\n\n\n\n12\nWed, Apr 9\nMore on regression\n\n\n\n\n\nIV\n13\nWed, Apr 16\nStatistical learning\n\nPaper 2\nCase study 4 files\n\n\n\n14\nWed, Apr 23\nOverview of additional models"
  },
  {
    "objectID": "computing.html",
    "href": "computing.html",
    "title": "Computing",
    "section": "",
    "text": "For Lab #0 you will download two software programs to your computer.\nPlease watch the below video to help you download R and RStudio.\n\n\n\n\nWatch this video to complete the steps below:\n\n\n\n\n\n\n\n\n\n\n\nOnce you have completed your downloads, check to see that you can open RStudio (not R).\nWe’ll pick up here in class next week.\n\n\n\nAfter completing Lab 0, you will use OpenAlex to support your literature identification process. OpenAlex is a comprehensive open catalog of the global research system that can help you find relevant publications for your research.\nOnce you are set up in R, try working with the code below:\n\nlibrary(openalexR)\n\n# Search for works related to your social justice topic\nworks_search &lt;- oa_fetch(\n  entity = \"works\",\n  title.search = c(\"keyword1\", \"your specific topic\"),\n  from_publication_date = \"2020-01-01\",\n  options = list(sort = \"cited_by_count:desc\"),\n  verbose = TRUE\n)\n\n# Display the top results\nworks_search |&gt;\n  head(10) |&gt;\n  show_works() |&gt;\n  knitr::kable()",
    "crumbs": [
      "Course information",
      "Computing"
    ]
  },
  {
    "objectID": "computing.html#lab-0-downloading-r-and-rstudio.",
    "href": "computing.html#lab-0-downloading-r-and-rstudio.",
    "title": "Computing",
    "section": "",
    "text": "For Lab #0 you will download two software programs to your computer.\nPlease watch the below video to help you download R and RStudio.\n\n\n\n\nWatch this video to complete the steps below:\n\n\n\n\n\n\n\n\n\n\n\nOnce you have completed your downloads, check to see that you can open RStudio (not R).\nWe’ll pick up here in class next week.\n\n\n\nAfter completing Lab 0, you will use OpenAlex to support your literature identification process. OpenAlex is a comprehensive open catalog of the global research system that can help you find relevant publications for your research.\nOnce you are set up in R, try working with the code below:\n\nlibrary(openalexR)\n\n# Search for works related to your social justice topic\nworks_search &lt;- oa_fetch(\n  entity = \"works\",\n  title.search = c(\"keyword1\", \"your specific topic\"),\n  from_publication_date = \"2020-01-01\",\n  options = list(sort = \"cited_by_count:desc\"),\n  verbose = TRUE\n)\n\n# Display the top results\nworks_search |&gt;\n  head(10) |&gt;\n  show_works() |&gt;\n  knitr::kable()",
    "crumbs": [
      "Course information",
      "Computing"
    ]
  },
  {
    "objectID": "course-overview.html",
    "href": "course-overview.html",
    "title": "Course overview",
    "section": "",
    "text": "On the previous page, we ended with the following three points:\n\nThe syllabus provides a high-level view of the course.\nCanvas is the go-to for submissions and provides readings and assignments.\nThis course site provides technical items and code to help you complete your assignments.\n\nWe’ll focus on the syllabus, Canvas, and this companion site to get a detailed overview of our course.\n\n\nA high-level view of the course.\nPlease note: this section is only a summary of our syllabus. The full syllabus is available on our Canvas site here. I have summarized some important items found on the syllabus below:\nInstructor: Nathan Alexander, PhD\n\nContact information:\nEmail: nathan.alexander@howard.edu\nOffice hours: By appointment at https://nathanalexander.youcanbook.me\nCourse information:\nCourse meeting times: Wednesdays at 5:10pm EST (10:10pm UTC)\nCourse meeting location: Zoom (see Canvas)\n\n\n\n\nTable 1: DATA 202 course schedule (spring 2025)\n\n\n\n\n\nCourse component\nWeeks\nDates1\n\n\n\n\nPart I: Statistics and society\n4 weeks\nJan 15 - Feb 5\n\n\nPart II: Data and people\n4 weeks\nFeb 12 - Mar 12\n\n\nPart III: Data and policy\n4 weeks\nMar 19 - Apr 9\n\n\nPart IV: Data in practice\n2 weeks\nApr 16 - Apr 23\n\n\n\n\n\n\n\n\n\nCanvas is the go-to for submissions, and where you can find readings and assignment rubrics. Here is another link to Canvas, I’ll try to drop these as often as possible. Two things about Canvas: it has your assignments and it has your readings.\n\n\n\nThe four categories for assignments in this course are as follows:\n\nAnnotated Bibliography. Statistical studies [10 points]\nLabs reports. Two long-form lab reports [30 points]\nPapers. Two 20-page papers [60 points]\n\n\n\n\n\n\n\n\n\n\n\nThis course should be viewed as an intense but remote introduction to what we might label “critical statistics” or “social justice statistics”. It will be important that we gain both a conceptual and theoretical understanding of the various concepts we will explore, and then a technical understanding of the complex issues that relate to them. We also want to communicate our ideas. These can be difficult tasks.\nBut I encourage you to explore and to be creative, and to take chances. Errors should mostly remind us that there is always the backspace button. Below are some general expectations to keep us all moving forward as a community:\n\n\nCommunicate early and often\nShow up to class on time\nDo not schedule other meetings during class time\nNo late work will be accepted without prior discussion\n\n\n\n\n\n\nYou can return to this page to remind yourself of the four parts of our course, especially when we dive deep into specific projects. This page will remain static (it will not be updated), so use it as an archive that you can return to; start in this page position if you have questions about the schedule, procedures, expectations (above) …then assignments (next).\n\n\n\nIn the next section, we’ll start with a brief overview of assignments and dive into your first two assignments (ungraded). The HW #0 and Lab #0 assignments will serve two purposes: first, they will help introduce you to the tools we’ll use as we begin to review statistical concepts; second, they will set you up for your first graded assignment coming up in two weeks.",
    "crumbs": [
      "Course information",
      "Overview"
    ]
  },
  {
    "objectID": "course-overview.html#a-slightly-more-detailed-overview.",
    "href": "course-overview.html#a-slightly-more-detailed-overview.",
    "title": "Course overview",
    "section": "",
    "text": "On the previous page, we ended with the following three points:\n\nThe syllabus provides a high-level view of the course.\nCanvas is the go-to for submissions and provides readings and assignments.\nThis course site provides technical items and code to help you complete your assignments.\n\nWe’ll focus on the syllabus, Canvas, and this companion site to get a detailed overview of our course.\n\n\nA high-level view of the course.\nPlease note: this section is only a summary of our syllabus. The full syllabus is available on our Canvas site here. I have summarized some important items found on the syllabus below:\nInstructor: Nathan Alexander, PhD\n\nContact information:\nEmail: nathan.alexander@howard.edu\nOffice hours: By appointment at https://nathanalexander.youcanbook.me\nCourse information:\nCourse meeting times: Wednesdays at 5:10pm EST (10:10pm UTC)\nCourse meeting location: Zoom (see Canvas)\n\n\n\n\nTable 1: DATA 202 course schedule (spring 2025)\n\n\n\n\n\nCourse component\nWeeks\nDates1\n\n\n\n\nPart I: Statistics and society\n4 weeks\nJan 15 - Feb 5\n\n\nPart II: Data and people\n4 weeks\nFeb 12 - Mar 12\n\n\nPart III: Data and policy\n4 weeks\nMar 19 - Apr 9\n\n\nPart IV: Data in practice\n2 weeks\nApr 16 - Apr 23\n\n\n\n\n\n\n\n\n\nCanvas is the go-to for submissions, and where you can find readings and assignment rubrics. Here is another link to Canvas, I’ll try to drop these as often as possible. Two things about Canvas: it has your assignments and it has your readings.\n\n\n\nThe four categories for assignments in this course are as follows:\n\nAnnotated Bibliography. Statistical studies [10 points]\nLabs reports. Two long-form lab reports [30 points]\nPapers. Two 20-page papers [60 points]\n\n\n\n\n\n\n\n\n\n\n\nThis course should be viewed as an intense but remote introduction to what we might label “critical statistics” or “social justice statistics”. It will be important that we gain both a conceptual and theoretical understanding of the various concepts we will explore, and then a technical understanding of the complex issues that relate to them. We also want to communicate our ideas. These can be difficult tasks.\nBut I encourage you to explore and to be creative, and to take chances. Errors should mostly remind us that there is always the backspace button. Below are some general expectations to keep us all moving forward as a community:\n\n\nCommunicate early and often\nShow up to class on time\nDo not schedule other meetings during class time\nNo late work will be accepted without prior discussion\n\n\n\n\n\n\nYou can return to this page to remind yourself of the four parts of our course, especially when we dive deep into specific projects. This page will remain static (it will not be updated), so use it as an archive that you can return to; start in this page position if you have questions about the schedule, procedures, expectations (above) …then assignments (next).\n\n\n\nIn the next section, we’ll start with a brief overview of assignments and dive into your first two assignments (ungraded). The HW #0 and Lab #0 assignments will serve two purposes: first, they will help introduce you to the tools we’ll use as we begin to review statistical concepts; second, they will set you up for your first graded assignment coming up in two weeks.",
    "crumbs": [
      "Course information",
      "Overview"
    ]
  },
  {
    "objectID": "course-overview.html#footnotes",
    "href": "course-overview.html#footnotes",
    "title": "Course overview",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPlease note that these are rough estimates↩︎",
    "crumbs": [
      "Course information",
      "Overview"
    ]
  },
  {
    "objectID": "cases/case02-pt2.html",
    "href": "cases/case02-pt2.html",
    "title": "Case Study 2, Part 2: The True Size of Africa",
    "section": "",
    "text": "In Case Study 2, we will explore the social politics of maps and globally oriented data to help us make sense of what we mean by a “population.” The case study will integrate a series of new packages, functions, and code to support our explorations.\nThere are two parts to this case study, this is part 2.",
    "crumbs": [
      "Appendix",
      "Case Studies",
      "Case Study 2, pt. 2"
    ]
  },
  {
    "objectID": "cases/case02-pt2.html#learning-objectives",
    "href": "cases/case02-pt2.html#learning-objectives",
    "title": "Case Study 2, Part 2: The True Size of Africa",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nThis case study component will teach you how to use functions in dplyr to manipulate variables and data frames. You will also learn some base-R functions to conduct univariate analysis in the RStudio IDE.",
    "crumbs": [
      "Appendix",
      "Case Studies",
      "Case Study 2, pt. 2"
    ]
  },
  {
    "objectID": "cases/case02-pt2.html#learning-activities",
    "href": "cases/case02-pt2.html#learning-activities",
    "title": "Case Study 2, Part 2: The True Size of Africa",
    "section": "Learning Activities",
    "text": "Learning Activities\nBy the end of this case study you will be able to:\n\nInstall and/or update R packages\nAssign data frames to different names for efficient exploration\nGenerate a set of outputs using the dplyr package\nOverwrite a data frame while using the pipe operator\nProduce simple plots using data located in an R package\n\n\nReminder: Load lackages and libraries\n\n# install package\n# install.packages(\"tidyverse\", repos = \"http://cran.us.r-project.org\")\n# install.packages(\"remotes\", repos = \"http://cran.us.r-project.org\")\n \n# load the necessary libraries\nlibrary(tidyverse) #collection of R packages designed for data science\nlibrary(dplyr)\nlibrary(remotes)",
    "crumbs": [
      "Appendix",
      "Case Studies",
      "Case Study 2, pt. 2"
    ]
  },
  {
    "objectID": "cases/case02-pt2.html#task-2.1-load-the-africa_data_all-data",
    "href": "cases/case02-pt2.html#task-2.1-load-the-africa_data_all-data",
    "title": "Case Study 2, Part 2: The True Size of Africa",
    "section": "Task 2.1: Load the africa_data_all data",
    "text": "Task 2.1: Load the africa_data_all data\nThe africa_data_all data set will be used for instructional purposes only.\nThe data in this data frame was collected from the internet.\nThe data frame contains data on African countries and territories for two years: 2020 and 2023.\n\nTask 2.1.1: Call the africa_data_all data\nWe begin by taking a look at the data.\n\ncritstats::africa_data_all \n\nWhat are some of your early observations?\n\n\nTask 2.1.2: Inspect africa_data_all documentation\nThe documentation can help us get a better idea of the data frame’s content.\n\n??critstats::africa_data_all \n\nIf, by chance, you cannot load the documentation or experience issues with the critstats data package, you may need to restart your RStudio session.",
    "crumbs": [
      "Appendix",
      "Case Studies",
      "Case Study 2, pt. 2"
    ]
  },
  {
    "objectID": "cases/case02-pt2.html#task-2.2-prepare-the-africa_data_all-data",
    "href": "cases/case02-pt2.html#task-2.2-prepare-the-africa_data_all-data",
    "title": "Case Study 2, Part 2: The True Size of Africa",
    "section": "Task 2.2: Prepare the africa_data_all data",
    "text": "Task 2.2: Prepare the africa_data_all data\nTo be efficient, we will only use specific functions to explore the data set.\nPlease note that there are many other approaches to exploration.\nThe approach taken below is one of many possibiliites.\n\nTask 2.2.1: Assign africa_data_all to df2\nUse the assignment operator to assign the africa_data_all data frame to the object df2.\n\ndf2 &lt;- critstats::africa_data_all\n\nWe can now work more efficiently by typing df2 when we want to call the data frame. Notice that I did not overwrite df1 in the event you want to return to Part 1 of this case study.\n\n\nTask 2.2.2: Inspect your data\nThe str() function displays the structure of R objects.\n\nstr(df2)\n\ntibble [116 × 13] (S3: tbl_df/tbl/data.frame)\n $ country          : chr [1:116] \"Nigeria\" \"Ethiopia\" \"Egypt\" \"DR Congo\" ...\n $ pop              : num [1:116] 2.06e+08 1.15e+08 1.02e+08 8.96e+07 5.93e+07 ...\n $ pop.yearly.change: num [1:116] 2.58 2.57 1.94 3.19 1.28 2.98 2.28 3.32 1.85 2.42 ...\n $ pop.net.change   : num [1:116] 5175990 2884858 1946331 2770836 750420 ...\n $ density          : num [1:116] 226 115 103 40 49 67 94 229 18 25 ...\n $ area             : num [1:116] 910770 1000000 995450 2267050 1213090 ...\n $ migrants         : num [1:116] -60000 30000 -38033 23861 145405 ...\n $ fertility.rate   : num [1:116] 5.4 4.3 3.3 6 2.4 4.9 3.5 5 3.1 4.4 ...\n $ med.age          : num [1:116] 18 19 25 17 28 18 20 17 29 20 ...\n $ urban.pop        : num [1:116] 52 21 43 46 67 37 28 26 73 35 ...\n $ world.share      : num [1:116] 2.64 1.47 1.31 1.15 0.76 0.77 0.69 0.59 0.56 0.56 ...\n $ pop_in_mill      : num [1:116] 206.1 115 102.3 89.6 59.3 ...\n $ year             : num [1:116] 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 ...\n\n\nUse head() to view the “top” of your data.\n\nhead(df2)\n\n# A tibble: 6 × 13\n  country           pop pop.yearly.change pop.net.change density   area migrants\n  &lt;chr&gt;           &lt;dbl&gt;             &lt;dbl&gt;          &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n1 Nigeria        2.06e8              2.58        5175990     226 9.11e5   -60000\n2 Ethiopia       1.15e8              2.57        2884858     115 1   e6    30000\n3 Egypt          1.02e8              1.94        1946331     103 9.95e5   -38033\n4 DR Congo       8.96e7              3.19        2770836      40 2.27e6    23861\n5 South Africa   5.93e7              1.28         750420      49 1.21e6   145405\n6 Tanzania       5.97e7              2.98        1728755      67 8.86e5   -40076\n# ℹ 6 more variables: fertility.rate &lt;dbl&gt;, med.age &lt;dbl&gt;, urban.pop &lt;dbl&gt;,\n#   world.share &lt;dbl&gt;, pop_in_mill &lt;dbl&gt;, year &lt;dbl&gt;\n\n\nUse tail() to view the “bottom” of your data.\n\ntail(df2)\n\n# A tibble: 6 × 13\n  country           pop pop.yearly.change pop.net.change density   area migrants\n  &lt;chr&gt;           &lt;dbl&gt;             &lt;dbl&gt;          &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n1 Cabo Verde     598682              0.93           5533     149   4030    -1227\n2 Western Sahara 587259              1.96          11273       2 266000     5600\n3 Mayotte        335995              3.03           9894     896    375        0\n4 Sao Tome & Pr… 231856              1.97           4476     242    960     -600\n5 Seychelles     107660              0.51            542     234    460     -200\n6 Saint Helena     5314             -1.12            -60      14    390        0\n# ℹ 6 more variables: fertility.rate &lt;dbl&gt;, med.age &lt;dbl&gt;, urban.pop &lt;dbl&gt;,\n#   world.share &lt;dbl&gt;, pop_in_mill &lt;dbl&gt;, year &lt;dbl&gt;\n\n\nView your data using the view() command.\n\nView(df2)\n\nGet a summary of your data with summary().\n\nsummary(df2)\n\n   country               pop            pop.yearly.change pop.net.change   \n Length:116         Min.   :     5314   Min.   :-1.120    Min.   :    -60  \n Class :character   1st Qu.:  2346300   1st Qu.: 1.567    1st Qu.:  46843  \n Mode  :character   Median : 12705220   Median : 2.400    Median : 320017  \n                    Mean   : 24147241   Mean   : 2.141    Mean   : 571374  \n                    3rd Qu.: 29236208   3rd Qu.: 2.732    3rd Qu.: 700895  \n                    Max.   :223804632   Max.   : 3.840    Max.   :5263420  \n                                                                           \n    density           area            migrants       fertility.rate \n Min.   :  2.0   Min.   :    375   Min.   :-174200   Min.   :1.400  \n 1st Qu.: 25.0   1st Qu.:  28120   1st Qu.: -10024   1st Qu.:3.150  \n Median : 64.0   Median : 269800   Median :  -4000   Median :4.100  \n Mean   :124.5   Mean   : 511181   Mean   :  -8680   Mean   :3.983  \n 3rd Qu.:137.2   3rd Qu.: 823290   3rd Qu.:   -100   3rd Qu.:4.700  \n Max.   :896.0   Max.   :2381740   Max.   : 168694   Max.   :7.000  \n                                   NA's   :1         NA's   :1      \n    med.age       urban.pop       world.share      pop_in_mill       \n Min.   :15.0   Min.   : 14.00   Min.   :0.0000   Min.   :  0.00531  \n 1st Qu.:18.0   1st Qu.: 35.00   1st Qu.:0.0300   1st Qu.:  2.34630  \n Median :19.0   Median : 46.00   Median :0.1600   Median : 12.70522  \n Mean   :21.3   Mean   : 49.15   Mean   :0.3048   Mean   : 24.14724  \n 3rd Qu.:22.5   3rd Qu.: 66.25   3rd Qu.:0.3650   3rd Qu.: 29.23621  \n Max.   :53.0   Max.   :100.00   Max.   :2.7800   Max.   :223.80463  \n NA's   :1                                                           \n      year     \n Min.   :2020  \n 1st Qu.:2020  \n Median :2022  \n Mean   :2022  \n 3rd Qu.:2023  \n Max.   :2023  \n               \n\n\nTake note of the content and values of the data and its structure.",
    "crumbs": [
      "Appendix",
      "Case Studies",
      "Case Study 2, pt. 2"
    ]
  },
  {
    "objectID": "cases/case02-pt2.html#task-2.3-examine-the-data-in-detail",
    "href": "cases/case02-pt2.html#task-2.3-examine-the-data-in-detail",
    "title": "Case Study 2, Part 2: The True Size of Africa",
    "section": "Task 2.3: Examine the data in detail",
    "text": "Task 2.3: Examine the data in detail\nBefore starting any analyses, we want to make sure we really understand our data.\nYou may have noticed that there is a year variable in the data set.\nSpecifically, the variable year separates the 2020 and 2023 data.\n\nTask 2.3.1: Make a table of a single variable’s contents\nTo examine how many data values are listed by year we can use table().\n\n# create a table of the number of observations by year\ntable(df2$year)\n\n\n2020 2023 \n  58   58 \n\n\nWe notice 58 values for each year. What does this say about our data?\n\n\nTask 2.3.2: Gather summary statistics for the data by year\nThere are different ways to gather summary statistics by year.\nI can nest the request using some logic and the year 2020 as follows:\n\nsummary(filter(df2, year == 2020))\n\n   country               pop            pop.yearly.change pop.net.change   \n Length:58          Min.   :     6077   Min.   :0.170     Min.   :     18  \n Class :character   1st Qu.:  2257207   1st Qu.:1.555     1st Qu.:  47292  \n Mode  :character   Median : 12006992   Median :2.450     Median : 269752  \n                    Mean   : 23113761   Mean   :2.212     Mean   : 560930  \n                    3rd Qu.: 27404729   3rd Qu.:2.810     3rd Qu.: 667545  \n                    Max.   :206139589   Max.   :3.840     Max.   :5175990  \n                                                                           \n    density           area            migrants       fertility.rate \n Min.   :  2.0   Min.   :    375   Min.   :-174200   Min.   :1.400  \n 1st Qu.: 25.0   1st Qu.:  28680   1st Qu.: -10047   1st Qu.:3.300  \n Median : 61.5   Median : 269800   Median :  -4000   Median :4.400  \n Mean   :119.0   Mean   : 511181   Mean   :  -8124   Mean   :4.144  \n 3rd Qu.:131.5   3rd Qu.: 814062   3rd Qu.:      0   3rd Qu.:4.800  \n Max.   :728.0   Max.   :2381740   Max.   : 168694   Max.   :7.000  \n                                   NA's   :1         NA's   :1      \n    med.age        urban.pop       world.share      pop_in_mill       \n Min.   :15.00   Min.   : 14.00   Min.   :0.0000   Min.   :  0.00608  \n 1st Qu.:18.00   1st Qu.: 35.50   1st Qu.:0.0300   1st Qu.:  2.25721  \n Median :19.00   Median : 46.00   Median :0.1550   Median : 12.00699  \n Mean   :21.46   Mean   : 48.90   Mean   :0.2964   Mean   : 23.11376  \n 3rd Qu.:23.00   3rd Qu.: 63.75   3rd Qu.:0.3550   3rd Qu.: 27.40473  \n Max.   :37.00   Max.   :100.00   Max.   :2.6400   Max.   :206.13959  \n NA's   :1                                                            \n      year     \n Min.   :2020  \n 1st Qu.:2020  \n Median :2020  \n Mean   :2020  \n 3rd Qu.:2020  \n Max.   :2020  \n               \n\n\nConsider why filter(summary(df2, year == 2020)) returns an error.\nWe can also use the %&gt;% operator to list the commands in order for 2020.\n\n# gather summary statistics for all variables for year == 2020\ndf2 %&gt;% \n  filter(year == 2020) %&gt;% \n  summary()\n\n   country               pop            pop.yearly.change pop.net.change   \n Length:58          Min.   :     6077   Min.   :0.170     Min.   :     18  \n Class :character   1st Qu.:  2257207   1st Qu.:1.555     1st Qu.:  47292  \n Mode  :character   Median : 12006992   Median :2.450     Median : 269752  \n                    Mean   : 23113761   Mean   :2.212     Mean   : 560930  \n                    3rd Qu.: 27404729   3rd Qu.:2.810     3rd Qu.: 667545  \n                    Max.   :206139589   Max.   :3.840     Max.   :5175990  \n                                                                           \n    density           area            migrants       fertility.rate \n Min.   :  2.0   Min.   :    375   Min.   :-174200   Min.   :1.400  \n 1st Qu.: 25.0   1st Qu.:  28680   1st Qu.: -10047   1st Qu.:3.300  \n Median : 61.5   Median : 269800   Median :  -4000   Median :4.400  \n Mean   :119.0   Mean   : 511181   Mean   :  -8124   Mean   :4.144  \n 3rd Qu.:131.5   3rd Qu.: 814062   3rd Qu.:      0   3rd Qu.:4.800  \n Max.   :728.0   Max.   :2381740   Max.   : 168694   Max.   :7.000  \n                                   NA's   :1         NA's   :1      \n    med.age        urban.pop       world.share      pop_in_mill       \n Min.   :15.00   Min.   : 14.00   Min.   :0.0000   Min.   :  0.00608  \n 1st Qu.:18.00   1st Qu.: 35.50   1st Qu.:0.0300   1st Qu.:  2.25721  \n Median :19.00   Median : 46.00   Median :0.1550   Median : 12.00699  \n Mean   :21.46   Mean   : 48.90   Mean   :0.2964   Mean   : 23.11376  \n 3rd Qu.:23.00   3rd Qu.: 63.75   3rd Qu.:0.3550   3rd Qu.: 27.40473  \n Max.   :37.00   Max.   :100.00   Max.   :2.6400   Max.   :206.13959  \n NA's   :1                                                            \n      year     \n Min.   :2020  \n 1st Qu.:2020  \n Median :2020  \n Mean   :2020  \n 3rd Qu.:2020  \n Max.   :2020  \n               \n\n\nWe can use the same commands to filter the data for 2023.\n\n# gather summary statistics for all variables for year == 2023\ndf2 %&gt;% \n  filter(year == 2023) %&gt;% \n  summary()\n\n   country               pop            pop.yearly.change pop.net.change   \n Length:58          Min.   :     5314   Min.   :-1.120    Min.   :    -60  \n Class :character   1st Qu.:  2478468   1st Qu.: 1.580    1st Qu.:  45111  \n Mode  :character   Median : 13475694   Median : 2.300    Median : 324628  \n                    Mean   : 25180720   Mean   : 2.071    Mean   : 581818  \n                    3rd Qu.: 29962558   3rd Qu.: 2.667    3rd Qu.: 702468  \n                    Max.   :223804632   Max.   : 3.800    Max.   :5263420  \n    density            area            migrants       fertility.rate \n Min.   :  2.00   Min.   :    375   Min.   :-126181   Min.   :1.400  \n 1st Qu.: 27.25   1st Qu.:  28680   1st Qu.: -10000   1st Qu.:2.825  \n Median : 65.50   Median : 269800   Median :  -4000   Median :3.900  \n Mean   :130.05   Mean   : 511181   Mean   :  -9228   Mean   :3.824  \n 3rd Qu.:143.50   3rd Qu.: 814062   3rd Qu.:   -300   3rd Qu.:4.375  \n Max.   :896.00   Max.   :2381740   Max.   :  58496   Max.   :6.700  \n    med.age        urban.pop      world.share      pop_in_mill       \n Min.   :15.00   Min.   :15.00   Min.   :0.0000   Min.   :  0.00531  \n 1st Qu.:17.00   1st Qu.:35.50   1st Qu.:0.0300   1st Qu.:  2.47847  \n Median :19.00   Median :46.00   Median :0.1650   Median : 13.47569  \n Mean   :21.14   Mean   :49.40   Mean   :0.3133   Mean   : 25.18072  \n 3rd Qu.:22.00   3rd Qu.:66.75   3rd Qu.:0.3750   3rd Qu.: 29.96256  \n Max.   :53.00   Max.   :95.00   Max.   :2.7800   Max.   :223.80463  \n      year     \n Min.   :2023  \n 1st Qu.:2023  \n Median :2023  \n Mean   :2023  \n 3rd Qu.:2023  \n Max.   :2023",
    "crumbs": [
      "Appendix",
      "Case Studies",
      "Case Study 2, pt. 2"
    ]
  },
  {
    "objectID": "cases/case02-pt2.html#task-2.4-get-univariate-statistics",
    "href": "cases/case02-pt2.html#task-2.4-get-univariate-statistics",
    "title": "Case Study 2, Part 2: The True Size of Africa",
    "section": "Task 2.4: Get univariate statistics",
    "text": "Task 2.4: Get univariate statistics\nTo help advance our understanding of statistical analyses in the RStudio IDE, we will lean a few tasks to compute univariate statistics. You are likely familiar with univariate statistics. Univariate statistics are statistics done on a single variable. Some base R functions for univariate statistics are as follows:\n\nmean() returns the mean of a single numeric variable\nmedian() returns the middle value of a single numeric variable\nmode() returns the variable type for the mode of a single variable\ntable() returns a frequency table with counts of each level for a single variable\nmax() returns the maximum value of a single numeric variable\nmin() returns the minimum value of a single numeric variable\nrange() returns the min() and max() values of a single numeric variable\nIQR() returns the interquartile range values for a single numeric variable\nsd() returns the standard deviation for a single numeric variable\nboxplot() returns a boxplot of a numeric variable or variables\nhist() returns a histogram of a single numeric variable\nstem() provides a stem-and-leaf plot when a single numeric variable is input.\nplot() provides a scatter plot of data values by its index, \\(i\\).\nplot(density()) provides a density plot of a single numeric variable\n\nEach of the above functions provides a different perspective on the distribution of values for a single variable.\n\nTask 2.4.1: Inspect data prior to analysis\nFrom our earlier observations, we see that the africa_data_all (df2) contains data across two years: 2020 and 2023. Let’s use pipes %&gt;% to create separate data from for each year.\n\n# create a separate data frame for 2020\ndata_2020 &lt;- df2 %&gt;% \n  filter(year == 2020)\n\ndata_2020 # view the data for 2020\n\n# A tibble: 58 × 13\n   country          pop pop.yearly.change pop.net.change density   area migrants\n   &lt;chr&gt;          &lt;dbl&gt;             &lt;dbl&gt;          &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n 1 Nigeria       2.06e8              2.58        5175990     226 9.11e5   -60000\n 2 Ethiopia      1.15e8              2.57        2884858     115 1   e6    30000\n 3 Egypt         1.02e8              1.94        1946331     103 9.95e5   -38033\n 4 DR Congo      8.96e7              3.19        2770836      40 2.27e6    23861\n 5 South Africa  5.93e7              1.28         750420      49 1.21e6   145405\n 6 Tanzania      5.97e7              2.98        1728755      67 8.86e5   -40076\n 7 Kenya         5.38e7              2.28        1197323      94 5.69e5   -10000\n 8 Uganda        4.57e7              3.32        1471413     229 2.00e5   168694\n 9 Algeria       4.39e7              1.85         797990      18 2.38e6   -10000\n10 Sudan         4.38e7              2.42        1036022      25 1.77e6   -50000\n# ℹ 48 more rows\n# ℹ 6 more variables: fertility.rate &lt;dbl&gt;, med.age &lt;dbl&gt;, urban.pop &lt;dbl&gt;,\n#   world.share &lt;dbl&gt;, pop_in_mill &lt;dbl&gt;, year &lt;dbl&gt;\n\n\nIt is not clear that this data is for 2020. As a result, we can reorganize the columns and overwrite the data frame.\nI am moving the year variable to the second position in the data frame.\n\ndata_2020 &lt;- df2 %&gt;% \n  filter(year == 2020) %&gt;% \n  relocate(country, year)\n\ndata_2020 # view the data for 2020\n\n# A tibble: 58 × 13\n   country  year    pop pop.yearly.change pop.net.change density   area migrants\n   &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;             &lt;dbl&gt;          &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n 1 Nigeria  2020 2.06e8              2.58        5175990     226 9.11e5   -60000\n 2 Ethiop…  2020 1.15e8              2.57        2884858     115 1   e6    30000\n 3 Egypt    2020 1.02e8              1.94        1946331     103 9.95e5   -38033\n 4 DR Con…  2020 8.96e7              3.19        2770836      40 2.27e6    23861\n 5 South …  2020 5.93e7              1.28         750420      49 1.21e6   145405\n 6 Tanzan…  2020 5.97e7              2.98        1728755      67 8.86e5   -40076\n 7 Kenya    2020 5.38e7              2.28        1197323      94 5.69e5   -10000\n 8 Uganda   2020 4.57e7              3.32        1471413     229 2.00e5   168694\n 9 Algeria  2020 4.39e7              1.85         797990      18 2.38e6   -10000\n10 Sudan    2020 4.38e7              2.42        1036022      25 1.77e6   -50000\n# ℹ 48 more rows\n# ℹ 5 more variables: fertility.rate &lt;dbl&gt;, med.age &lt;dbl&gt;, urban.pop &lt;dbl&gt;,\n#   world.share &lt;dbl&gt;, pop_in_mill &lt;dbl&gt;\n\n\nWe can run the same functions for year == 2023.\n\n# create a separate data frame for 2023\ndata_2023 &lt;- df2 %&gt;% \n  filter(year == 2023) %&gt;% \n  relocate(country, year)\n\ndata_2023 # view the data for 2023\n\n# A tibble: 58 × 13\n   country  year    pop pop.yearly.change pop.net.change density   area migrants\n   &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;             &lt;dbl&gt;          &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n 1 Nigeria  2023 2.24e8              2.41        5263420     246 9.11e5   -59996\n 2 Ethiop…  2023 1.27e8              2.55        3147136     127 1   e6   -11999\n 3 Egypt    2023 1.13e8              1.56        1726495     113 9.95e5   -29998\n 4 DR Con…  2023 1.02e8              3.29        3252596      45 2.27e6   -14999\n 5 Tanzan…  2023 6.74e7              2.96        1940358      76 8.86e5   -39997\n 6 South …  2023 6.04e7              0.87         520610      50 1.21e6    58496\n 7 Kenya    2023 5.51e7              1.99        1073099      97 5.69e5   -10000\n 8 Sudan    2023 4.81e7              2.63        1234802      27 1.77e6    -9999\n 9 Uganda   2023 4.86e7              2.82        1332749     243 2.00e5  -126181\n10 Algeria  2023 4.56e7              1.57         703255      19 2.38e6    -9999\n# ℹ 48 more rows\n# ℹ 5 more variables: fertility.rate &lt;dbl&gt;, med.age &lt;dbl&gt;, urban.pop &lt;dbl&gt;,\n#   world.share &lt;dbl&gt;, pop_in_mill &lt;dbl&gt;\n\n\nIt is now clearer which year we are loading when we view the data frames.\n\n\nTask 2.4.1a: Check for missing data\nWe can check the entire data frame for missing values using is.na().\n\nis.na(data_2020)\n\nis.na(data_2023)\n\nYou can see that this output is far too extensive.\nWe should check for missing values for a specific variable first.\nLet’s use the migrants variable in the df2 data frame by inserting df2$migrants.\nThe code to check for missing values in the 2020 data frame for the variable migrant is as follows:\n\nis.na(data_2020$migrants)\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n\n\nNotice that there is a missing value for the variable migrant in the 2020 data.\nIt may be important to see the total number of missing values in the data frame. We can check the number of missing values for each variable using sapply(). The sapply() is a base-R function and it is used for different purposes. Here is an example:\n\nsapply(data_2020, function(x) sum(is.na(x)))\n\n          country              year               pop pop.yearly.change \n                0                 0                 0                 0 \n   pop.net.change           density              area          migrants \n                0                 0                 0                 1 \n   fertility.rate           med.age         urban.pop       world.share \n                1                 1                 0                 0 \n      pop_in_mill \n                0 \n\n\nsapply() is a loop function and the above code iterates the function for each variable in the df2 data set.\nThe logic of the function is sapply(data, FUNction...).\nFrom the output, we notice that a few different variables have a missing value.\nYou can use the view() function to take a closer look at the data to find the missing values.\nWhat do you notice about the source of the missing values when using view()?\n\n\nTask 2.4.1b: Working with missing values\nIn class, we will learn how to deal with missing values. For now, you can remove missing values using the code below or select those variables that has no missing values for your case study reports.\nWhen conducting univariate statistics, we can simply tell R to ignore missing values.\n\n# mean returns `NA` since there are missing values\nmean(data_2020$migrants)\n\n[1] NA\n\n\nUse na.rm = TRUE to remove missing values from a numeric variable\n\n# instruct R to remove missing values from the analysis using `\nmean(data_2020$migrants, na.rm = TRUE)\n\n[1] -8123.684\n\n\nThere are some exceptions here and it relates to the type of variable being used. We’ll explore missing values in class.\n\nFor our next set of tasks, we will focus on variables in the data_2020 data frame.\nRecall that data_2020 was used as a label for africa_data_all when year == 2020.\n\n\nTask 2.4.2: mean()\nThe mean() function returns the mean of a single numeric variable.\n\n# find the average population in 2020\nmean(data_2020$pop)\n\n[1] 23113761\n\n# find the average percent urban population in 2020\nmean(data_2020$urban.pop)\n\n[1] 48.89655\n\n# find the average of median age in 2020\nmean(data_2020$med.age)\n\n[1] NA\n\n\nNotice that the last command returns NA due to missing values.\nWe can correct this by using na.rm = TRUE.\n\n# check to see missing data in the variable\nis.na(data_2020$med.age)\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n\n# compute the mean using the na.rm = TRUE\nmean(data_2020$med.age, na.rm = TRUE)\n\n[1] 21.45614\n\n\n\n\nTask 2.4.3: median()\nThe median() function returns the middle value of a single numeric variable.\n\nmedian(data_2020$pop)\n\n[1] 12006992\n\n\nWe can also use pipe operators to filter the countries that are above and below the median values.\n\n# filter data to show countries that are below the median\ndata_2020 %&gt;% \n  filter(pop &lt; median(pop)) %&gt;% \n  select(country, pop)\n\n# A tibble: 29 × 2\n   country                       pop\n   &lt;chr&gt;                       &lt;dbl&gt;\n 1 Tunisia                  11818619\n 2 Burundi                  11890784\n 3 South Sudan              11193725\n 4 Togo                      8278724\n 5 Sierra Leone              7976983\n 6 Libya                     6871292\n 7 Congo                     5518087\n 8 Liberia                   5057681\n 9 Central African Republic  4829767\n10 Mauritania                4649658\n# ℹ 19 more rows\n\n# filter data to show countries that are above the median\ndata_2020 %&gt;% \n  filter(pop &gt; median(pop)) %&gt;% \n  select(country, pop)\n\n# A tibble: 29 × 2\n   country            pop\n   &lt;chr&gt;            &lt;dbl&gt;\n 1 Nigeria      206139589\n 2 Ethiopia     114963588\n 3 Egypt        102334404\n 4 DR Congo      89561403\n 5 South Africa  59308690\n 6 Tanzania      59734218\n 7 Kenya         53771296\n 8 Uganda        45741007\n 9 Algeria       43851044\n10 Sudan         43849260\n# ℹ 19 more rows\n\n\n\n\nTask 2.4.4: mode() and table()\nThe mode() function returns the variable type for the mode of a single variable.\n\n# what does this output tell us about the mode of the med.age variable?\nmode(data_2020$med.age) \n\n[1] \"numeric\"\n\n\nWe can use the table() function to get our mode.\nThe table() function returns a frequency table with counts of each level for a single variable.\n\n# what does this updated output tell us about the mode of the med.age variable?\ntable(data_2020$med.age) \n\n\n15 16 17 18 19 20 21 22 23 24 25 27 28 29 30 33 34 36 37 \n 1  1  6  9 14  7  1  3  1  2  1  1  3  2  1  1  1  1  1 \n\n\n\n\nTask 2.4.5: max() and min()\nThe max() function returns the maximum value of a single numeric variable.\n\n# get the maximum value\nmax(data_2020$pop)\n\n[1] 206139589\n\n# use pipes to gather details about which country or territory has the max value\ndata_2020 %&gt;% \n  filter(pop == max(pop)) %&gt;% \n  select(country, pop)\n\n# A tibble: 1 × 2\n  country       pop\n  &lt;chr&gt;       &lt;dbl&gt;\n1 Nigeria 206139589\n\n\nThe min() function returns the minimum value of a single numeric variable.\n\n# get the minimum value\nmin(data_2020$pop)\n\n[1] 6077\n\n# use pipes to gather details about which country or territory has the min value\ndata_2020 %&gt;% \n  filter(pop == min(pop)) %&gt;% \n  select(country, pop)\n\n# A tibble: 1 × 2\n  country        pop\n  &lt;chr&gt;        &lt;dbl&gt;\n1 Saint Helena  6077\n\n\n\n\nTask 2.4.6: range()\nThe range() function returns the min() and max() values of a single numeric variable.\n\nrange(data_2020$pop)\n\n[1]      6077 206139589\n\n\nHowever, we can manually compute the range by using arithmetic with our functions.\n\n# generate a new variable call population range that is the maximum minus the minimum value\npop_range_2020 &lt;- max(data_2020$pop) - min(data_2020$pop)\npop_range_2020 # we must call the object back to see its value\n\n[1] 206133512\n\n\n\n\nTask 2.4.7: IQR()\n\n# find the IQR for the 2020 population variable\nIQR(data_2020$pop)\n\n[1] 25147522\n\n# find the IQR for the 2020 percent urban population variable\nIQR(data_2020$urban.pop)\n\n[1] 28.25\n\n\n\n\nTask 2.4.8: sd()\nThe sd() function returns the standard deviation for a single numeric variable.\n\n# find the standard deviation of the 2020 population variable\nsd(data_2020$pop)\n\n[1] 35061685\n\n# find the standard deviation of the 2020 urban population variable\nsd(data_2020$urban.pop)\n\n[1] 19.83501\n\n\n\n\nTask 2.4.8: Use summary()\nThe summary() function is often more efficient for a quick check of univariate statistics.\n\nsummary(data_2020)\n\n   country               year           pop            pop.yearly.change\n Length:58          Min.   :2020   Min.   :     6077   Min.   :0.170    \n Class :character   1st Qu.:2020   1st Qu.:  2257207   1st Qu.:1.555    \n Mode  :character   Median :2020   Median : 12006992   Median :2.450    \n                    Mean   :2020   Mean   : 23113761   Mean   :2.212    \n                    3rd Qu.:2020   3rd Qu.: 27404729   3rd Qu.:2.810    \n                    Max.   :2020   Max.   :206139589   Max.   :3.840    \n                                                                        \n pop.net.change       density           area            migrants      \n Min.   :     18   Min.   :  2.0   Min.   :    375   Min.   :-174200  \n 1st Qu.:  47292   1st Qu.: 25.0   1st Qu.:  28680   1st Qu.: -10047  \n Median : 269752   Median : 61.5   Median : 269800   Median :  -4000  \n Mean   : 560930   Mean   :119.0   Mean   : 511181   Mean   :  -8124  \n 3rd Qu.: 667545   3rd Qu.:131.5   3rd Qu.: 814062   3rd Qu.:      0  \n Max.   :5175990   Max.   :728.0   Max.   :2381740   Max.   : 168694  \n                                                     NA's   :1        \n fertility.rate     med.age        urban.pop       world.share    \n Min.   :1.400   Min.   :15.00   Min.   : 14.00   Min.   :0.0000  \n 1st Qu.:3.300   1st Qu.:18.00   1st Qu.: 35.50   1st Qu.:0.0300  \n Median :4.400   Median :19.00   Median : 46.00   Median :0.1550  \n Mean   :4.144   Mean   :21.46   Mean   : 48.90   Mean   :0.2964  \n 3rd Qu.:4.800   3rd Qu.:23.00   3rd Qu.: 63.75   3rd Qu.:0.3550  \n Max.   :7.000   Max.   :37.00   Max.   :100.00   Max.   :2.6400  \n NA's   :1       NA's   :1                                        \n  pop_in_mill       \n Min.   :  0.00608  \n 1st Qu.:  2.25721  \n Median : 12.00699  \n Mean   : 23.11376  \n 3rd Qu.: 27.40473  \n Max.   :206.13959  \n                    \n\n\nNot, however, the values that summary() returns and those that it does not. While the function can be used to be more efficient, it should not replace a thorough inspection of your data. We will discuss this more in exploratory data anlaysis.",
    "crumbs": [
      "Appendix",
      "Case Studies",
      "Case Study 2, pt. 2"
    ]
  },
  {
    "objectID": "cases/case02-pt2.html#task-2.5-create-basic-plots",
    "href": "cases/case02-pt2.html#task-2.5-create-basic-plots",
    "title": "Case Study 2, Part 2: The True Size of Africa",
    "section": "Task 2.5: Create basic plots",
    "text": "Task 2.5: Create basic plots\n\nTask 2.5.1: boxplot()\nboxplot() returns a box plot of a numeric variable or variables.\n\nboxplot(data_2020$pop)\n\n\n\n\n\n\n\n\n\n\nTask 2.5.2: hist()\nhist() returns a histogram of a single numeric variable.\n\nhist(data_2020$pop)\n\n\n\n\n\n\n\n\n\n\nTask 2.5.3: stem()\nstem() provides a stem-and-leaf plot when a single numeric variable is input.\n\nstem(data_2020$med.age)\n\n\n  The decimal point is at the |\n\n  14 | 0\n  16 | 0000000\n  18 | 00000000000000000000000\n  20 | 00000000\n  22 | 0000\n  24 | 000\n  26 | 0\n  28 | 00000\n  30 | 0\n  32 | 0\n  34 | 0\n  36 | 00\n\n\n\n\nTask 2.5.4: plot()\nplot() provides a scatter plot of data values by its index, \\(i\\).\n\nplot(data_2020$med.age)\n\n\n\n\n\n\n\n\n\n\nTask 2.5.5: plot(density())\nplot(density()) provides a density plot of a single numeric variable\n\nplot(density(data_2020$pop))",
    "crumbs": [
      "Appendix",
      "Case Studies",
      "Case Study 2, pt. 2"
    ]
  },
  {
    "objectID": "cases/case02-pt2.html#report-1.6",
    "href": "cases/case02-pt2.html#report-1.6",
    "title": "Case Study 2, Part 2: The True Size of Africa",
    "section": "Report 1.6",
    "text": "Report 1.6\nState each variable and variable type in the africa_data_all data frame.\nWhat code provides us with the requested information most efficiently?",
    "crumbs": [
      "Appendix",
      "Case Studies",
      "Case Study 2, pt. 2"
    ]
  },
  {
    "objectID": "cases/case02-pt2.html#report-1.7",
    "href": "cases/case02-pt2.html#report-1.7",
    "title": "Case Study 2, Part 2: The True Size of Africa",
    "section": "Report 1.7",
    "text": "Report 1.7\nWhy does filter(summary(df2, year == 2020)) return an error?\nRecall that df2 was used as a label for africa_data_all.",
    "crumbs": [
      "Appendix",
      "Case Studies",
      "Case Study 2, pt. 2"
    ]
  },
  {
    "objectID": "cases/case02-pt2.html#report-1.8",
    "href": "cases/case02-pt2.html#report-1.8",
    "title": "Case Study 2, Part 2: The True Size of Africa",
    "section": "Report 1.8",
    "text": "Report 1.8\nWhen using view(data_2020) after running the sapply(data_2020, function(x) sum(is.na(x))), what do you notice? Specifically, what country (or territory) is the source of the missing data values?\nRecall that data_2020 was used as a label for africa_data_all when year == 2020.",
    "crumbs": [
      "Appendix",
      "Case Studies",
      "Case Study 2, pt. 2"
    ]
  },
  {
    "objectID": "cases/case02-pt2.html#report-1.9",
    "href": "cases/case02-pt2.html#report-1.9",
    "title": "Case Study 2, Part 2: The True Size of Africa",
    "section": "Report 1.9",
    "text": "Report 1.9\nCompute univariate statistics for two numeric variables in africa_data_all for year == 2023.\nUnivariate statistics for each variable should include: mean(), median(), a method to find the mode of the varaible, if it exists, max() and min(), a method to find the range of the varaible, IQR(), and sd().",
    "crumbs": [
      "Appendix",
      "Case Studies",
      "Case Study 2, pt. 2"
    ]
  },
  {
    "objectID": "cases/case02-pt2.html#report-1.10",
    "href": "cases/case02-pt2.html#report-1.10",
    "title": "Case Study 2, Part 2: The True Size of Africa",
    "section": "Report 1.10",
    "text": "Report 1.10\nCreate basic plots (i.e., boxplot(), hist(), plot(), and plot(density())) for the two numeric variables that you used in Report 1.9 (above). Add a title to each plot.\n{Save each plot in your directory}.",
    "crumbs": [
      "Appendix",
      "Case Studies",
      "Case Study 2, pt. 2"
    ]
  },
  {
    "objectID": "papers/papers-more-info.html",
    "href": "papers/papers-more-info.html",
    "title": "More information for course papers",
    "section": "",
    "text": "As noted before, the course papers all require that you do original data analysis.\nPapers should be written as research reports (try to make the text clear but sound professional, though the research questions may be relatively simple), and they should include the elements described below in narrative form. Please give each paper a substantive title (in addition to “Paper #_“).\nEach paper should have, at minimum, five sections: Introduction, Theory, Method, Analysis, and Conclusion.\n\n\nUsing research literature, either from OpenAlex, Web of Science (WoS), or some other search (such as Research Rabbit), you should outline - based on the literature - the main goals of your paper.\n\n\n\nThe research question(s) should close out the introductory section of your paper.\n\n\n\nEach theory (or logic graph) should be described fully in about a paragraph or two.\nDescribe and justify the relationships involving the theoretical variables that will be examined (including what’s the unit of analysis and “what’s the story,” in terms of why we care about the issue(s); this is a great place to insert historical work). Include a graph along with this at the beginning of the paper.\n\n\n\nNext, describe the data that you will be using to examine the theory. Describe the data source that you are using, and for any survey data describe the sampling method. For the General Social Survey (GSS) someone could say something like “The 2012 GSS sample is a multistage area probability sample to the segment or block level. At the block level, households are enumerated and a full probability sample is drawn.” You will need to find this type of detailed information in the code provided. Describe the specific measures you are using and how they were obtained. Comment on how well the measures fit your theoretical variables (i.e., the face validity of the measures). In the case of survey data, report the full question wordings (verbatim) and specify the response categories and report how the variables were coded and re-coded. Present the frequency distributions (so your coding and treatment of missing data can be checked); if you want to comment on the frequency distributions, you should do so in no more than a couple sentences.\n\n\n\nSpecify the hypotheses for the expected relationships (coefficients / correlations) in the context of the specified equations and operational flow graph. Then report the statistical results, coefficients, etc. and other relevant statistics and any additional calculations required. Describe to what extent the evidence is consistent with your hypothesis or hypotheses and lends support to your theorizing.\n\n\n\nWhat can you conclude about your original theorizing based upon your analysis? Were you on the right track? Were your findings different from what you originally theorized?",
    "crumbs": [
      "Appendix",
      "Papers",
      "More information for papers"
    ]
  },
  {
    "objectID": "papers/papers-more-info.html#introduction",
    "href": "papers/papers-more-info.html#introduction",
    "title": "More information for course papers",
    "section": "",
    "text": "Using research literature, either from OpenAlex, Web of Science (WoS), or some other search (such as Research Rabbit), you should outline - based on the literature - the main goals of your paper.",
    "crumbs": [
      "Appendix",
      "Papers",
      "More information for papers"
    ]
  },
  {
    "objectID": "papers/papers-more-info.html#research-question",
    "href": "papers/papers-more-info.html#research-question",
    "title": "More information for course papers",
    "section": "",
    "text": "The research question(s) should close out the introductory section of your paper.",
    "crumbs": [
      "Appendix",
      "Papers",
      "More information for papers"
    ]
  },
  {
    "objectID": "papers/papers-more-info.html#theory",
    "href": "papers/papers-more-info.html#theory",
    "title": "More information for course papers",
    "section": "",
    "text": "Each theory (or logic graph) should be described fully in about a paragraph or two.\nDescribe and justify the relationships involving the theoretical variables that will be examined (including what’s the unit of analysis and “what’s the story,” in terms of why we care about the issue(s); this is a great place to insert historical work). Include a graph along with this at the beginning of the paper.",
    "crumbs": [
      "Appendix",
      "Papers",
      "More information for papers"
    ]
  },
  {
    "objectID": "papers/papers-more-info.html#method-or-measurement",
    "href": "papers/papers-more-info.html#method-or-measurement",
    "title": "More information for course papers",
    "section": "",
    "text": "Next, describe the data that you will be using to examine the theory. Describe the data source that you are using, and for any survey data describe the sampling method. For the General Social Survey (GSS) someone could say something like “The 2012 GSS sample is a multistage area probability sample to the segment or block level. At the block level, households are enumerated and a full probability sample is drawn.” You will need to find this type of detailed information in the code provided. Describe the specific measures you are using and how they were obtained. Comment on how well the measures fit your theoretical variables (i.e., the face validity of the measures). In the case of survey data, report the full question wordings (verbatim) and specify the response categories and report how the variables were coded and re-coded. Present the frequency distributions (so your coding and treatment of missing data can be checked); if you want to comment on the frequency distributions, you should do so in no more than a couple sentences.",
    "crumbs": [
      "Appendix",
      "Papers",
      "More information for papers"
    ]
  },
  {
    "objectID": "papers/papers-more-info.html#statistical-analysis",
    "href": "papers/papers-more-info.html#statistical-analysis",
    "title": "More information for course papers",
    "section": "",
    "text": "Specify the hypotheses for the expected relationships (coefficients / correlations) in the context of the specified equations and operational flow graph. Then report the statistical results, coefficients, etc. and other relevant statistics and any additional calculations required. Describe to what extent the evidence is consistent with your hypothesis or hypotheses and lends support to your theorizing.",
    "crumbs": [
      "Appendix",
      "Papers",
      "More information for papers"
    ]
  },
  {
    "objectID": "papers/papers-more-info.html#conclusion",
    "href": "papers/papers-more-info.html#conclusion",
    "title": "More information for course papers",
    "section": "",
    "text": "What can you conclude about your original theorizing based upon your analysis? Were you on the right track? Were your findings different from what you originally theorized?",
    "crumbs": [
      "Appendix",
      "Papers",
      "More information for papers"
    ]
  },
  {
    "objectID": "papers/paper2.html",
    "href": "papers/paper2.html",
    "title": "Paper 2",
    "section": "",
    "text": "For review, you may also refer to more information for course papers in two additional documents:",
    "crumbs": [
      "Appendix",
      "Papers",
      "Paper 2"
    ]
  },
  {
    "objectID": "papers/paper2.html#path-diagram",
    "href": "papers/paper2.html#path-diagram",
    "title": "Paper 2",
    "section": "Path Diagram",
    "text": "Path Diagram\n\n\n\n\n\nflowchart LR\n  A[Income] --&gt; B[Strength of support for a political party]\n  A --&gt; C[Religious affiliation]\n  \n\n\n\n\n\n\nIn the code below, we will focus on the relationship between A and B.",
    "crumbs": [
      "Appendix",
      "Papers",
      "Paper 2"
    ]
  },
  {
    "objectID": "papers/paper2.html#prepare-files",
    "href": "papers/paper2.html#prepare-files",
    "title": "Paper 2",
    "section": "Prepare files",
    "text": "Prepare files\nTo investigate the relationship outlined in the path diagram, we will use data from the General Social Survey (GSS). To begin your exploratory analysis, start a new RScript in your stats-pt2 RStudio project directory and give your RScript a proper preamble.",
    "crumbs": [
      "Appendix",
      "Papers",
      "Paper 2"
    ]
  },
  {
    "objectID": "papers/paper2.html#data-set",
    "href": "papers/paper2.html#data-set",
    "title": "Paper 2",
    "section": "Data set",
    "text": "Data set\nFor paper #2, you will use the gss_cat data located in the forcats package.\nTo test our hypothesis, two survey variables/measures are selected from the gss_cat data:\n\nrincome (respondent’s reported income)\npartyid (respondent’s levels of support for one of three major US political parties)\n\n\nLoad the libraries\n\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(tidycensus)\n\n\n\nView data in your current R session\n\ndata()\n\nLocate the gss_cat data in the forcats pacakge.\nThis is a data set for us to examine the use of categorical data and factor variable types.\n\n\nView documentation for your data\n\n?gss_cat\n\nView your data\n\ngss_cat\n\n# A tibble: 21,483 × 9\n    year marital         age race  rincome        partyid    relig denom tvhours\n   &lt;int&gt; &lt;fct&gt;         &lt;int&gt; &lt;fct&gt; &lt;fct&gt;          &lt;fct&gt;      &lt;fct&gt; &lt;fct&gt;   &lt;int&gt;\n 1  2000 Never married    26 White $8000 to 9999  Ind,near … Prot… Sout…      12\n 2  2000 Divorced         48 White $8000 to 9999  Not str r… Prot… Bapt…      NA\n 3  2000 Widowed          67 White Not applicable Independe… Prot… No d…       2\n 4  2000 Never married    39 White Not applicable Ind,near … Orth… Not …       4\n 5  2000 Divorced         25 White Not applicable Not str d… None  Not …       1\n 6  2000 Married          25 White $20000 - 24999 Strong de… Prot… Sout…      NA\n 7  2000 Never married    36 White $25000 or more Not str r… Chri… Not …       3\n 8  2000 Divorced         44 White $7000 to 7999  Ind,near … Prot… Luth…      NA\n 9  2000 Married          44 White $25000 or more Not str d… Prot… Other       0\n10  2000 Married          47 White $25000 or more Strong re… Prot… Sout…       3\n# ℹ 21,473 more rows\n\n\nBe sure to check the categories and codes for your variables.\n\nCategories for rincome\n\nsummary(gss_cat$rincome)\n\n     No answer     Don't know        Refused $25000 or more $20000 - 24999 \n           183            267            975           7363           1283 \n$15000 - 19999 $10000 - 14999  $8000 to 9999  $7000 to 7999  $6000 to 6999 \n          1048           1168            340            188            215 \n $5000 to 5999  $4000 to 4999  $3000 to 3999  $1000 to 2999       Lt $1000 \n           227            226            276            395            286 \nNot applicable \n          7043 \n\n\n\n\nCategories for partyid\n\nsummary(gss_cat$partyid)\n\n         No answer         Don't know        Other party  Strong republican \n               154                  1                393               2314 \nNot str republican       Ind,near rep        Independent       Ind,near dem \n              3032               1791               4119               2499 \n  Not str democrat    Strong democrat \n              3690               3490 \n\n\nNotice how each variable is measured and take notes for the measurement section of your paper.\nYou will need to re-code these values prior to any statistical analysis.",
    "crumbs": [
      "Appendix",
      "Papers",
      "Paper 2"
    ]
  },
  {
    "objectID": "papers/paper2.html#prepare-data-for-analysis",
    "href": "papers/paper2.html#prepare-data-for-analysis",
    "title": "Paper 2",
    "section": "Prepare data for analysis",
    "text": "Prepare data for analysis\nBegin exploratory analyses on each variable.\nWhile doing exploratory analyses, you should prepare your data for statistical analysis.\n\nInspect your data using str()\nCheck the distribution of your variables using count()\nDecide how you will work with missing data, such as na.omit()\n\n\nRemove missing values from your analysis\nFor this paper, it is fine to drop all missing observations.\nWe will also explore the data from the year 2000.\n\ndf &lt;- gss_cat %&gt;% \n  na.omit() %&gt;% \n  filter(year == 2000) %&gt;% \n  select(year, rincome, partyid)\n\nExamine the reduced data frame.\n\ndf\n\n# A tibble: 1,824 × 3\n    year rincome        partyid           \n   &lt;int&gt; &lt;fct&gt;          &lt;fct&gt;             \n 1  2000 $8000 to 9999  Ind,near rep      \n 2  2000 Not applicable Independent       \n 3  2000 Not applicable Ind,near rep      \n 4  2000 Not applicable Not str democrat  \n 5  2000 $25000 or more Not str republican\n 6  2000 $25000 or more Not str democrat  \n 7  2000 $25000 or more Strong republican \n 8  2000 $25000 or more Not str democrat  \n 9  2000 $25000 or more Strong democrat   \n10  2000 $25000 or more Ind,near dem      \n# ℹ 1,814 more rows\n\n\nView the top and bottom of your data using head and tail, respectively.\n\nhead(df)\n\n# A tibble: 6 × 3\n   year rincome        partyid           \n  &lt;int&gt; &lt;fct&gt;          &lt;fct&gt;             \n1  2000 $8000 to 9999  Ind,near rep      \n2  2000 Not applicable Independent       \n3  2000 Not applicable Ind,near rep      \n4  2000 Not applicable Not str democrat  \n5  2000 $25000 or more Not str republican\n6  2000 $25000 or more Not str democrat  \n\ntail(df)\n\n# A tibble: 6 × 3\n   year rincome        partyid          \n  &lt;int&gt; &lt;fct&gt;          &lt;fct&gt;            \n1  2000 $25000 or more Not str democrat \n2  2000 $25000 or more Strong republican\n3  2000 $10000 - 14999 Independent      \n4  2000 $25000 or more Strong republican\n5  2000 $25000 or more Ind,near rep     \n6  2000 $7000 to 7999  Strong republican\n\n\nIn paper 2, you will conduct original statistical analyses.\n\n\nGet frequency tables for your data\nUsing the count() function, we will get frequency tables for each variable.\n\ndf %&gt;% \n  count(rincome)\n\n# A tibble: 16 × 2\n   rincome            n\n   &lt;fct&gt;          &lt;int&gt;\n 1 No answer         15\n 2 Don't know        22\n 3 Refused           92\n 4 $25000 or more   590\n 5 $20000 - 24999   126\n 6 $15000 - 19999   115\n 7 $10000 - 14999   139\n 8 $8000 to 9999     45\n 9 $7000 to 7999     18\n10 $6000 to 6999     24\n11 $5000 to 5999     17\n12 $4000 to 4999     20\n13 $3000 to 3999     22\n14 $1000 to 2999     36\n15 Lt $1000          24\n16 Not applicable   519\n\n\n\ndf %&gt;% \n  count(partyid)\n\n# A tibble: 9 × 2\n  partyid                n\n  &lt;fct&gt;              &lt;int&gt;\n1 No answer              3\n2 Other party           22\n3 Strong republican    180\n4 Not str republican   244\n5 Ind,near rep         168\n6 Independent          392\n7 Ind,near dem         213\n8 Not str democrat     331\n9 Strong democrat      271\n\n\nFrom our frequency tables, it is clear that we will need to transform our data.",
    "crumbs": [
      "Appendix",
      "Papers",
      "Paper 2"
    ]
  },
  {
    "objectID": "papers/paper2.html#recode-categories-into-two-levels-dichotomous",
    "href": "papers/paper2.html#recode-categories-into-two-levels-dichotomous",
    "title": "Paper 2",
    "section": "Recode categories into two-levels (dichotomous)",
    "text": "Recode categories into two-levels (dichotomous)\nFor this analysis, we will transform our data into two dichotomous variables. Let’s examine the outputs before overwriting our data frame.\nWe will use the logic operator != to imply we do not want to keep these values (i.e., we will filter the values that are not equal to the right hand side).\n\ndf %&gt;% \n  filter(year == 2000) %&gt;% \n  filter(rincome != \"No answer\"\n         & rincome != \"Refused\"\n         & rincome != \"Not applicable\")\n\n# A tibble: 1,198 × 3\n    year rincome        partyid           \n   &lt;int&gt; &lt;fct&gt;          &lt;fct&gt;             \n 1  2000 $8000 to 9999  Ind,near rep      \n 2  2000 $25000 or more Not str republican\n 3  2000 $25000 or more Not str democrat  \n 4  2000 $25000 or more Strong republican \n 5  2000 $25000 or more Not str democrat  \n 6  2000 $25000 or more Strong democrat   \n 7  2000 $25000 or more Ind,near dem      \n 8  2000 $25000 or more Strong democrat   \n 9  2000 $25000 or more Independent       \n10  2000 $10000 - 14999 Not str democrat  \n# ℹ 1,188 more rows\n\n\nWe now recode the categories using mutuate() and recode().\n\ndf %&gt;% \n  mutate(rincome = fct_recode(rincome, \n          \"More than 10000\" = \"$25000 or more\",\n          \"More than 10000\" = \"$20000 to 24999\",\n          \"More than 10000\" = \"$15000 to 19999\",\n          \"More than 10000\" = \"$10000 to 14999\",\n          \"Less than 10000\" = \"$8000 to 9999\",\n          \"Less than 10000\" = \"$7000 to 7999\",\n          \"Less than 10000\" = \"$6000 to 6999\",\n          \"Less than 10000\" = \"$5000 to 5999\",\n          \"Less than 10000\" = \"$4000 to 4999\",\n          \"Less than 10000\" = \"$3000 to 3999\",\n          \"Less than 10000\" = \"$1000 to 2999\",\n          \"Less than 10000\" = \"$Lt $1000\"))\n\n# A tibble: 1,824 × 3\n    year rincome         partyid           \n   &lt;int&gt; &lt;fct&gt;           &lt;fct&gt;             \n 1  2000 Less than 10000 Ind,near rep      \n 2  2000 Not applicable  Independent       \n 3  2000 Not applicable  Ind,near rep      \n 4  2000 Not applicable  Not str democrat  \n 5  2000 More than 10000 Not str republican\n 6  2000 More than 10000 Not str democrat  \n 7  2000 More than 10000 Strong republican \n 8  2000 More than 10000 Not str democrat  \n 9  2000 More than 10000 Strong democrat   \n10  2000 More than 10000 Ind,near dem      \n# ℹ 1,814 more rows\n\n\n\ndf %&gt;% \n  mutate(partyid = fct_recode(partyid,\n                              \"Republican\" = \"Strong republican\",\n                              \"Republican\" = \"Not str republican\",\n                              \"Republican\" = \"Ind,near rep\",\n                              \"Democrat\" = \"Ind,near dem\",\n                              \"Democrat\" = \"Not str democrat\",\n                              \"Democrat\" = \"Strong democrat\"))\n\n# A tibble: 1,824 × 3\n    year rincome        partyid    \n   &lt;int&gt; &lt;fct&gt;          &lt;fct&gt;      \n 1  2000 $8000 to 9999  Republican \n 2  2000 Not applicable Independent\n 3  2000 Not applicable Republican \n 4  2000 Not applicable Democrat   \n 5  2000 $25000 or more Republican \n 6  2000 $25000 or more Democrat   \n 7  2000 $25000 or more Republican \n 8  2000 $25000 or more Democrat   \n 9  2000 $25000 or more Democrat   \n10  2000 $25000 or more Democrat   \n# ℹ 1,814 more rows\n\n\nWe can stack our variable transformations together into one chunk of code.\nTake note of the way we are creating our new data set for analysis.\n\ndf %&gt;% \n  filter(year == 2000) %&gt;% \n  filter(rincome != \"No answer\"\n         & rincome != \"Refused\"\n         & rincome != \"Not applicable\") %&gt;% \n  mutate(rincome = fct_recode(rincome, \n          \"More than 20000\" = \"$25000 or more\",\n          \"More than 20000\" = \"$20000 - 24999\",\n          \"Less than 20000\" = \"$15000 - 19999\",\n          \"Less than 20000\" = \"$10000 - 14999\",\n          \"Less than 20000\" = \"$8000 to 9999\",\n          \"Less than 20000\" = \"$7000 to 7999\",\n          \"Less than 20000\" = \"$6000 to 6999\",\n          \"Less than 20000\" = \"$5000 to 5999\",\n          \"Less than 20000\" = \"$4000 to 4999\",\n          \"Less than 20000\" = \"$3000 to 3999\",\n          \"Less than 20000\" = \"$1000 to 2999\",\n          \"Less than 20000\" = \"Lt $1000\")) %&gt;%\n  mutate(partyid = fct_recode(partyid,\n                              \"Republican\" = \"Strong republican\",\n                              \"Republican\" = \"Not str republican\",\n                              \"Republican\" = \"Ind,near rep\",\n                              \"Democrat\" = \"Ind,near dem\",\n                              \"Democrat\" = \"Not str democrat\",\n                              \"Democrat\" = \"Strong democrat\"))\n\n# A tibble: 1,198 × 3\n    year rincome         partyid    \n   &lt;int&gt; &lt;fct&gt;           &lt;fct&gt;      \n 1  2000 Less than 20000 Republican \n 2  2000 More than 20000 Republican \n 3  2000 More than 20000 Democrat   \n 4  2000 More than 20000 Republican \n 5  2000 More than 20000 Democrat   \n 6  2000 More than 20000 Democrat   \n 7  2000 More than 20000 Democrat   \n 8  2000 More than 20000 Democrat   \n 9  2000 More than 20000 Independent\n10  2000 Less than 20000 Democrat   \n# ℹ 1,188 more rows\n\n\nAs you examine the code more closely, you will notice that I created two categories:\n\nPeople making less than $20,000\nPeople making more than $20,000",
    "crumbs": [
      "Appendix",
      "Papers",
      "Paper 2"
    ]
  },
  {
    "objectID": "papers/paper1.html",
    "href": "papers/paper1.html",
    "title": "Paper 1",
    "section": "",
    "text": "This is an individual paper. All papers should be no less than twenty (20) pages in length and written as narrative essays in 12-point Times New Roman font with one-inch margins.\nIn writing the paper, you should structure it by using section headings and relevant citations for any referenced work.",
    "crumbs": [
      "Appendix",
      "Papers",
      "Paper 1"
    ]
  },
  {
    "objectID": "papers/paper1.html#paper-requirements",
    "href": "papers/paper1.html#paper-requirements",
    "title": "Paper 1",
    "section": "",
    "text": "This is an individual paper. All papers should be no less than twenty (20) pages in length and written as narrative essays in 12-point Times New Roman font with one-inch margins.\nIn writing the paper, you should structure it by using section headings and relevant citations for any referenced work.",
    "crumbs": [
      "Appendix",
      "Papers",
      "Paper 1"
    ]
  },
  {
    "objectID": "papers/paper1.html#paper-overview",
    "href": "papers/paper1.html#paper-overview",
    "title": "Paper 1",
    "section": "Paper overview",
    "text": "Paper overview\nIn this paper, you should write about a theory concerning the relationships between and among two variables, that is some dependent variable and particular independent variables (you may have one or more than one independent variable of interest at the outset). After introducing your theory you will use real-world data and statistical analysis to examine the theory.\nFor review, you may also refer to more information for course papers in two additional documents:\n\nInstructions for papers\nMore information for course papers",
    "crumbs": [
      "Appendix",
      "Papers",
      "Paper 1"
    ]
  },
  {
    "objectID": "papers/paper1.html#paper-contents",
    "href": "papers/paper1.html#paper-contents",
    "title": "Paper 1",
    "section": "Paper contents",
    "text": "Paper contents\nYour paper should be clear on the following:\n- What are the variables (especially the dependent one that is explained by the independent variable(s))?\n\n- What is the unit of analysis?\n\n- Draw and discuss a path diagram (flow graph) for the theory.\n\n- What are the measurement, data collection, and sampling strategies?\n\n- How are the variables to be measured?\n\n- What are the categories of the variables/measures?\n\n- What are the hypotheses concerning the relationships among the variables/measures?\n\n- Explain how this study offers evidence bearing on the relationship(s) of interest.",
    "crumbs": [
      "Appendix",
      "Papers",
      "Paper 1"
    ]
  },
  {
    "objectID": "weeks/week02.html",
    "href": "weeks/week02.html",
    "title": "DATA 202 - Week 2",
    "section": "",
    "text": "Statistics is a science. As a result, it follows a set of well-defined steps or methods. As we explore new terms and definitions, we will gain a better understanding of what statistics encompasses.\n\n\n\n\n\n\nDEFINITION: Statistics\n\n\n\nStatistics is the science of collecting, organizing, analyzing, interpreting, communicating, and visualizing data and information.\n\n\nThere are a multitude of ways to describe the steps, terms, and various processes undertaken in a statistical study. Importantly, however, modern statistics calls for more critical questions where we explore difference or change within specified contexts. We then use theory and concepts of variation to understand differences within or between a set (or sets) of measurements, resulting in a more critical orientation to statistics.\n\n\n\n\nIn popular media, we might see information reported as follows:\n\nIn 2022, college enrollment for 18-24 year-olds in the United States was 39%, a two percent decrease from 41% in 2012. The enrollment rate for those aged 18-24 in 2022 who were Black was 36%, which was lower than the rates for Asian students, which was 61%, and White students, which was 41%.\n\nThe data above is from the U.S. National Center for Education Statistics (NCES). While reading the paragraph, what were some of your opinions? What questions did you have?\n\nHow was enrollment defined and measured?\nShould the differences be viewed as significant in some way?\nDo the percents mean that some students are less likely to go to college?\n\n\nIn traditional statistics, a quantitative research question is answered using the scientific method. A traditionalist may blindly follow the steps of this method, focusing on model selection and assumptions while also ignoring any broader social and historical contexts.\nIn the social sciences, for example, statistics continues to be used to perpetuate negative stereotypes – which does not simply give it a bad rap but it results in real-world harm to communities. Does this mean we are just in need of critical statistics?\n\n\n\n\n\n\n\nExample: Challenging Statistics in Media\n\n\n\nThe American sitcom A Different World challenged negative stereotypes associated with Black youth and families regarding the pursuit of higher education. By portraying a vibrant college life at Hillman, a predominately Black college in the U.S., the series highlighted the importance of education as a priority within the Black community.\n\n\n\nImage of ‘A Different World’ cast\n\n\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\nIn a discussion about the U.S. college enrollment rate of Black 18-24 year-olds, your colleague Rachel cites Bill Cosby and two shows that he produced, A Different World and The Cosby Show, as important examples to combat deficit racial stereotypes. However, during the discussion, another colleague mentions how Bill Cosby was found guilty of aggravated indecent assault and sexual predation, and mentions that we have to be mindful of the work we cite and the persons responsible for producing the work. The colleague states that ``one problematic case should not be combated with another problematic case” referring to Cosby; Rachel disagrees. Both colleagues ask for your opinion on the discussion. How might you respond?\n\n\n\n\n\nThe idea of traditional statistics vs. non-traditional statistics may mean, on the surface at least, that we are in need of a critical statistics, and fast. But what if this dichotomy is a false one? What other pathways exist? Pathways of refusal? Pathways of exploration?\nWe will make attempts to reside in the space between refusal and exploration, as two key aspects of what we will come to know as critical statistics. By employing a practice of refusal, we will have space to refuse harmful beliefs and scientific practices. In the spirit of learning, we will also explore the potential benefits (and drawbacks) of critical statistics.\n\n\n\n\n\n\nTraditional approaches to statistics\n\n\n\nCritical approaches to statistics require interdisciplinary thinking.\n\n\n\nTraditional approaches to statistics in some areas of study, like the environmental sciences, provide important information about changes and differences in global patterns. In other areas of study, such as education and the social sciences, statistics has a different history.\nIn the specific case of U.S. college enrollment rates, you may be concerned with some broader questions about data in the education sciences. In the spirit of exploration, you might ask:\n\nShould we interpret these enrollment statistics as indicative of broader societal issues affecting access to higher education?\nWhat are the historical and systemic barriers that could explain the differences in enrollment rates among students?\n\n\nAlternatively, in the spirit of refusal, some different questions come to the surface:\n\nWhat beliefs underlie the analysis of college enrollment rates?\nWhat assumptions are being made when quantifying educational pathways?\nHow does rejecting conventional racial comparisons challenge power dynamics?\n\nThis second set of questions, as early examples of a practice of refusal, encourage your critical thinking in a few ways. First, the questions ask you to think about the less explicit components of the paragraph. Second, the questions present an option to reject conventional approaches to how we measure educational outcomes and ask why. Third, and to the seeming contradiction between refusal and exploration, a set of new pathways arise.\n\n\n\n\n\n\n\nPractice\n\n\n\nBad Stats (or BS), outlined by Professor Ivory Toldson, are data points that are poorly contextualized, generally negative, and they are often incomplete or incorrect. They are part of a problematic trend in which statistics are used to reinforce negative stereotypes. Identify two examples of Bad Stats in popular media.",
    "crumbs": [
      "Weekly materials",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week02.html#what-is-statistics",
    "href": "weeks/week02.html#what-is-statistics",
    "title": "DATA 202 - Week 2",
    "section": "",
    "text": "Statistics is a science. As a result, it follows a set of well-defined steps or methods. As we explore new terms and definitions, we will gain a better understanding of what statistics encompasses.\n\n\n\n\n\n\nDEFINITION: Statistics\n\n\n\nStatistics is the science of collecting, organizing, analyzing, interpreting, communicating, and visualizing data and information.\n\n\nThere are a multitude of ways to describe the steps, terms, and various processes undertaken in a statistical study. Importantly, however, modern statistics calls for more critical questions where we explore difference or change within specified contexts. We then use theory and concepts of variation to understand differences within or between a set (or sets) of measurements, resulting in a more critical orientation to statistics.",
    "crumbs": [
      "Weekly materials",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week02.html#what-is-critical-statistics",
    "href": "weeks/week02.html#what-is-critical-statistics",
    "title": "DATA 202 - Week 2",
    "section": "",
    "text": "In popular media, we might see information reported as follows:\n\nIn 2022, college enrollment for 18-24 year-olds in the United States was 39%, a two percent decrease from 41% in 2012. The enrollment rate for those aged 18-24 in 2022 who were Black was 36%, which was lower than the rates for Asian students, which was 61%, and White students, which was 41%.\n\nThe data above is from the U.S. National Center for Education Statistics (NCES). While reading the paragraph, what were some of your opinions? What questions did you have?\n\nHow was enrollment defined and measured?\nShould the differences be viewed as significant in some way?\nDo the percents mean that some students are less likely to go to college?\n\n\nIn traditional statistics, a quantitative research question is answered using the scientific method. A traditionalist may blindly follow the steps of this method, focusing on model selection and assumptions while also ignoring any broader social and historical contexts.\nIn the social sciences, for example, statistics continues to be used to perpetuate negative stereotypes – which does not simply give it a bad rap but it results in real-world harm to communities. Does this mean we are just in need of critical statistics?\n\n\n\n\n\n\n\nExample: Challenging Statistics in Media\n\n\n\nThe American sitcom A Different World challenged negative stereotypes associated with Black youth and families regarding the pursuit of higher education. By portraying a vibrant college life at Hillman, a predominately Black college in the U.S., the series highlighted the importance of education as a priority within the Black community.\n\n\n\nImage of ‘A Different World’ cast\n\n\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\nIn a discussion about the U.S. college enrollment rate of Black 18-24 year-olds, your colleague Rachel cites Bill Cosby and two shows that he produced, A Different World and The Cosby Show, as important examples to combat deficit racial stereotypes. However, during the discussion, another colleague mentions how Bill Cosby was found guilty of aggravated indecent assault and sexual predation, and mentions that we have to be mindful of the work we cite and the persons responsible for producing the work. The colleague states that ``one problematic case should not be combated with another problematic case” referring to Cosby; Rachel disagrees. Both colleagues ask for your opinion on the discussion. How might you respond?",
    "crumbs": [
      "Weekly materials",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week02.html#a-different-world-of-statistics",
    "href": "weeks/week02.html#a-different-world-of-statistics",
    "title": "DATA 202 - Week 2",
    "section": "",
    "text": "The idea of traditional statistics vs. non-traditional statistics may mean, on the surface at least, that we are in need of a critical statistics, and fast. But what if this dichotomy is a false one? What other pathways exist? Pathways of refusal? Pathways of exploration?\nWe will make attempts to reside in the space between refusal and exploration, as two key aspects of what we will come to know as critical statistics. By employing a practice of refusal, we will have space to refuse harmful beliefs and scientific practices. In the spirit of learning, we will also explore the potential benefits (and drawbacks) of critical statistics.\n\n\n\n\n\n\nTraditional approaches to statistics\n\n\n\nCritical approaches to statistics require interdisciplinary thinking.\n\n\n\nTraditional approaches to statistics in some areas of study, like the environmental sciences, provide important information about changes and differences in global patterns. In other areas of study, such as education and the social sciences, statistics has a different history.\nIn the specific case of U.S. college enrollment rates, you may be concerned with some broader questions about data in the education sciences. In the spirit of exploration, you might ask:\n\nShould we interpret these enrollment statistics as indicative of broader societal issues affecting access to higher education?\nWhat are the historical and systemic barriers that could explain the differences in enrollment rates among students?\n\n\nAlternatively, in the spirit of refusal, some different questions come to the surface:\n\nWhat beliefs underlie the analysis of college enrollment rates?\nWhat assumptions are being made when quantifying educational pathways?\nHow does rejecting conventional racial comparisons challenge power dynamics?\n\nThis second set of questions, as early examples of a practice of refusal, encourage your critical thinking in a few ways. First, the questions ask you to think about the less explicit components of the paragraph. Second, the questions present an option to reject conventional approaches to how we measure educational outcomes and ask why. Third, and to the seeming contradiction between refusal and exploration, a set of new pathways arise.\n\n\n\n\n\n\n\nPractice\n\n\n\nBad Stats (or BS), outlined by Professor Ivory Toldson, are data points that are poorly contextualized, generally negative, and they are often incomplete or incorrect. They are part of a problematic trend in which statistics are used to reinforce negative stereotypes. Identify two examples of Bad Stats in popular media.",
    "crumbs": [
      "Weekly materials",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week02.html#what-should-it-mean-to-be-critical-in-the-context-of-statistics",
    "href": "weeks/week02.html#what-should-it-mean-to-be-critical-in-the-context-of-statistics",
    "title": "DATA 202 - Week 2",
    "section": "What should it mean to be critical in the context of statistics?",
    "text": "What should it mean to be critical in the context of statistics?\n\nNotes from Duncan (n.d.) reading.\nOn Canvas, in the Week 2 folder, there is a document titled “Critical Thinking” by Jennifer Duncan. This document is one example of how we can frame what it could or should mean to be critical in statistics. Please review this document.\n\n\nUsing a higher order of thinking. Duncan emphasizes that critical thinking is a higher order of thinking with different advanced thinking skills, and offers a few suggestions.\n\nWe base our thinking on logic and not on feelings.\nWe should look deeper into inferences for hidden assumptions or values.\nAsk complex questions that help build a critical inquiry.\n\n\n\n\nAsking complex questions. Duncan breaks down the process into a few sub-questions.\n\nWho is the implied audience?\nWhat are the strengths and weaknesses of the argument?\nWhat are the underlying assumptions and values?\n\n\n\n\nUsing a variety of thinking processes. Duncan defines a process around analyzing, synthesizing, interpreting, and evaluating information that helps with our thinking.\n\n\n\nReflecting on how we answer a question. Duncan ends with a set of questions that help us think about different points of view, if we have clarity, and if more details are needed.\n\n\nThe quantification of information can help us understand and represent important situations. We’ll begin by exploring the concept of a set, and how it is defined in mathematics and used to frame various situations.\n\n\n\n\n\n\n\nDEFINITION: Set\n\n\n\nA set is a well-defined collection of elements or items.\n\n\nA set is characterized by its contents, or what is generally referred to as a set’s elements. If we are given two sets, the sets are considered equal if and only if they have exactly the same elements. The basic relation for sets is that of membership in a particular set.\n\nWe write \\(x \\in X\\) to indicate that the object \\(x\\) is an element (or member) of the set \\(X\\).\n\n\nA FEW IMPORTANT NOTES:\nNote 1: We tend to label sets using capital letters.\n\nFor example, we may label two different sets as \\(X\\) and \\(Y\\) or as \\(A\\) and \\(B\\).\n\nNote 2: We use curly brackets { and } to enclose the elements of a set.\n\nParentheses ( and ) are often used to indicate a point like \\((x, f(x))\\) or an open interval\nSquare brackets [ and ] are often used to separate sets or to indicate a closed interval.\n\nNote 3: We tend to list the elements of a set using lower case letters with subscripts, \\(x_i\\).\n\nThe \\(i\\) in \\(x_i\\) is a subscript that is used as a “position indicator,” with \\(i = 1, 2, 3, ...\\)\nWe use subscripts to index the elements of a set: \\(X = \\{x_1, x_2, ..., x_n\\}\\) and \\(A = \\{a_1, a_2, ..., a_n\\}\\).\n\n\n\n\n\n\n\n\nEXAMPLES – Sets as a collection of elements or items\n\n\n\nSets can be defined in many different ways.\n\nConsider a set of electronics \\(E\\) = {laptop, phone, tablet, watch} = \\(\\{e_1, e_2, e_3, e_4\\}\\)\nConsider a set of friends \\(F\\) = {Akeah, Brandon, Cris, Daveon, Evelyn}\nConsider a set of numbers \\(N = \\{1, 2, 3, ...\\}\\) = \\(\\{n_1, n_2, n_3, ...\\}\\), so \\(n_1 = 1\\), \\(n_2 = 2\\), …\nConsider a set of sets \\(S\\) = {{laptop, phone, tablet, watch}, {1, 2, 3, …}, …}\n\n\n\n\nQuantifying elements of a set allows us to perform mathematical operations on those elements.\nTogether, the elements and operations combine to create equations, functions, and models that help us understand and communicate details about the elements of a set – which is a form of data.\nWe will need a host of math concepts. The different sets of numbers can be a fun starting point.",
    "crumbs": [
      "Weekly materials",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week02.html#sets-and-numbers",
    "href": "weeks/week02.html#sets-and-numbers",
    "title": "DATA 202 - Week 2",
    "section": "Sets and numbers",
    "text": "Sets and numbers\n\n\n\n\n\n\nDEFINITIONS: Sets of numbers\n\n\n\n– Natural numbers: \\(\\mathbb{N} = \\{1, 2, 3, ...\\}\\)\n– Whole numbers: \\(\\mathbb{N_0} = \\{0, 1, 2, 3, ...\\}\\)\n– Integers: \\(\\mathbb{Z} = \\{..., -3, -2, -1, 0, 1, 2, 3, ...\\}\\)\n– Rational numbers: \\(\\mathbb{Q} = \\Big\\{\\dfrac{p}{q}, p \\in \\mathbb{Z}, q \\in \\mathbb{Z}, q \\neq 0 \\Big\\}\\)\n\n\n\n\n\n\n\n\n\nDEFINITIONS: Sets of numbers, continued…\n\n\n\n\nIrrational numbers\n\nany number that is not a rational number; irrational means not rational (no ratio)\ne.g., you may know some irrational numbers such as \\(\\pi\\), \\(\\sqrt{2}\\), \\(\\sqrt{3}\\), \\(e\\) (Euler’s number)\n\nReal numbers: \\(\\mathbb{R}\\)\n\nThe set of numbers on the real number line\nThis set is constructed by combining the rational and irrational numbers\n\nImaginary numbers: \\(\\mathbb{I}\\)\n\na number that has a negative value when it is squared\n\\(i\\) is the unit imaginary number, \\(\\sqrt{-1} = i\\) by definition\nso \\(i\\) is a complex number since \\(i^2 = -1\\)\n\nComplex numbers: \\(\\mathbb{C}\\)\n\na number in the form \\(a + bi\\) where \\(a \\in \\mathbb{R}\\), \\(b \\in \\mathbb{R}\\), and \\(i\\) is an imaginary number",
    "crumbs": [
      "Weekly materials",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week02.html#getting-started-in-rstudio",
    "href": "weeks/week02.html#getting-started-in-rstudio",
    "title": "DATA 202 - Week 2",
    "section": "Getting started in RStudio",
    "text": "Getting started in RStudio\nIn Lab 0, you downloaded and installed base R and RStudio. In this section, we will learn more about R and RStudio.\nLet’s start with a little fun!\n\nFirst, install the ‘praise’ package.\n\n# install the package\ninstall.packages(\"praise\", repos = \"http://cran.us.r-project.org\")\n\n\nNext, load the library for the ‘praise’ package.\n\n# load library\nlibrary(praise)\n\n\nNow, get some praise!\n\n# get some praise\npraise()\n\nYou can keep inserting the code above to get praise when you need it!",
    "crumbs": [
      "Weekly materials",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week02.html#arithmetic-in-r",
    "href": "weeks/week02.html#arithmetic-in-r",
    "title": "DATA 202 - Week 2",
    "section": "Arithmetic in R",
    "text": "Arithmetic in R\nWe will learn how to calculate values in R.\n\n\n1 + 2  # the 'plus sign' computes the sum\n\n[1] 3\n\n\n\n\n2 - 3  # the 'minus sign' computes the difference\n\n[1] -1\n\n\n\n\n3 * 4  # the 'asterisk' computes the product\n\n[1] 12\n\n\n\n\n4 / 5 # the 'forward slash' computes the quotient\n\n[1] 0.8\n\n\n\n\n# from hw exercise 0.2, we can compute the sum of the first 100 positive integers\nsum(1:100) \n\n[1] 5050",
    "crumbs": [
      "Weekly materials",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week02.html#variables-in-r",
    "href": "weeks/week02.html#variables-in-r",
    "title": "DATA 202 - Week 2",
    "section": "Variables in R",
    "text": "Variables in R\nWe will learn to give a variable (or character) a value.\n\nUse the different assignment operators\n\ny = 2 # the equal sign can be used as an assignment operator\n\ny &lt;-2 # a \"less than\" sign and dash can also be used as an assignment operator\n\ny # R stores all values you assign, so you must \"call\" any variables to see their values\n\n[1] 2\n\n\n\nSet x equal to two added to three\n\nx = 2 + 3\nx\n\n[1] 5\n\n\n\nSet y equal to two minus three\n\ny = 2 - 3\ny\n\n[1] -1\n\n\n\nSet z equal to two times three\n\nz = 2 * 3\nz\n\n[1] 6\n\n\n\nOverwrite the value of y by setting y equal to x divided by z\n\ny = x / z\ny\n\n[1] 0.8333333\n\n\n\nIn the next module, we will continue our explorations in R by learning how to load data sets into our data frame, and perform some basic operations using some additional packages. These packages will allow us to consider how we can construct original data sets to develop unique questions for our analysis.",
    "crumbs": [
      "Weekly materials",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week02-slides.html#what-is-statistics",
    "href": "weeks/week02-slides.html#what-is-statistics",
    "title": "DATA 202 - Week 2",
    "section": "What is Statistics?",
    "text": "What is Statistics?\nStatistics is a science. As a result, it follows a set of well-defined steps or methods. As we explore new terms and definitions, we will gain a better understanding of what statistics encompasses.\n\n\n\nDEFINITION: Statistics\n\n\nStatistics is the science of collecting, organizing, analyzing, interpreting, communicating, and visualizing data and information.\n\n\n\nThere are a multitude of ways to describe the steps, terms, and various processes undertaken in a statistical study. Importantly, however, modern statistics calls for more critical questions where we explore difference or change within specified contexts. We then use theory and concepts of variation to understand differences within or between a set (or sets) of measurements, resulting in a more critical orientation to statistics."
  },
  {
    "objectID": "weeks/week02-slides.html#what-is-critical-statistics",
    "href": "weeks/week02-slides.html#what-is-critical-statistics",
    "title": "DATA 202 - Week 2",
    "section": "What is Critical Statistics?",
    "text": "What is Critical Statistics?\nIn popular media, we might see information reported as follows:\n\nIn 2022, college enrollment for 18-24 year-olds in the United States was 39%, a two percent decrease from 41% in 2012. The enrollment rate for those aged 18-24 in 2022 who were Black was 36%, which was lower than the rates for Asian students, which was 61%, and White students, which was 41%.\n\nThe data above is from the U.S. National Center for Education Statistics (NCES). While reading the paragraph, what were some of your opinions? What questions did you have?\n\nHow was enrollment defined and measured?\nShould the differences be viewed as significant in some way?\nDo the percents mean that some students are less likely to go to college?"
  },
  {
    "objectID": "weeks/week02-slides.html#a-different-world-of-statistics",
    "href": "weeks/week02-slides.html#a-different-world-of-statistics",
    "title": "DATA 202 - Week 2",
    "section": "A Different World of Statistics",
    "text": "A Different World of Statistics\nThe idea of traditional statistics vs. non-traditional statistics may mean, on the surface at least, that we are in need of a critical statistics, and fast. But what if this dichotomy is a false one? What other pathways exist? Pathways of refusal? Pathways of exploration?\nWe will make attempts to reside in the space between refusal and exploration, as two key aspects of what we will come to know as critical statistics. By employing a practice of refusal, we will have space to refuse harmful beliefs and scientific practices. In the spirit of learning, we will also explore the potential benefits (and drawbacks) of critical statistics.\n\n\n\n\n\n\nTraditional approaches to statistics\n\n\nCritical approaches to statistics require interdisciplinary thinking."
  },
  {
    "objectID": "weeks/week02-slides.html#what-should-it-mean-to-be-critical-in-the-context-of-statistics",
    "href": "weeks/week02-slides.html#what-should-it-mean-to-be-critical-in-the-context-of-statistics",
    "title": "DATA 202 - Week 2",
    "section": "What should it mean to be critical in the context of statistics?",
    "text": "What should it mean to be critical in the context of statistics?\nNotes from Duncan (n.d.) reading.\nOn Canvas, in the Week 2 folder, there is a document titled “Critical Thinking” by Jennifer Duncan. This document is one example of how we can frame what it could or should mean to be critical in statistics. Please review this document."
  },
  {
    "objectID": "weeks/week02-slides.html#sets-and-numbers",
    "href": "weeks/week02-slides.html#sets-and-numbers",
    "title": "DATA 202 - Week 2",
    "section": "Sets and numbers",
    "text": "Sets and numbers\n\n\n\nDEFINITIONS: Sets of numbers\n\n\n– Natural numbers: \\(\\mathbb{N} = \\{1, 2, 3, ...\\}\\)\n– Whole numbers: \\(\\mathbb{N_0} = \\{0, 1, 2, 3, ...\\}\\)\n– Integers: \\(\\mathbb{Z} = \\{..., -3, -2, -1, 0, 1, 2, 3, ...\\}\\)\n– Rational numbers: \\(\\mathbb{Q} = \\Big\\{\\dfrac{p}{q}, p \\in \\mathbb{Z}, q \\in \\mathbb{Z}, q \\neq 0 \\Big\\}\\)"
  },
  {
    "objectID": "weeks/week02-slides.html#getting-started-in-rstudio",
    "href": "weeks/week02-slides.html#getting-started-in-rstudio",
    "title": "DATA 202 - Week 2",
    "section": "Getting started in RStudio",
    "text": "Getting started in RStudio\nIn Lab 0, you downloaded and installed base R and RStudio. In this section, we will learn more about R and RStudio.\nLet’s start with a little fun!"
  },
  {
    "objectID": "weeks/week02-slides.html#arithmetic-in-r",
    "href": "weeks/week02-slides.html#arithmetic-in-r",
    "title": "DATA 202 - Week 2",
    "section": "Arithmetic in R",
    "text": "Arithmetic in R\nWe will learn how to calculate values in R."
  },
  {
    "objectID": "weeks/week02-slides.html#variables-in-r",
    "href": "weeks/week02-slides.html#variables-in-r",
    "title": "DATA 202 - Week 2",
    "section": "Variables in R",
    "text": "Variables in R\nWe will learn to give a variable (or character) a value."
  },
  {
    "objectID": "weeks/week15.html",
    "href": "weeks/week15.html",
    "title": "DATA 202 - Week 15",
    "section": "",
    "text": "Open session to support you in your final data analysis.\n\n\n\nThe rubric for the final assessment (paper 4) will be used to assess your growth over the course of the semester. In addition to the requirements outlined in the assignment, the rubric will be used to assess the overall quality of the submission.\n\nDATA 202 final assessment rubric\n\n\n\n\n\n\n\n\n\nCriteria\nExemplary\nGood\nAcceptable\nIncomplete\n\n\n\n\nPurpose\nThe central purpose or argument is readily apparent.\nThe writing has a clear purpose or argument, but may sometimes digress.\nThe central purpose or argument is not consistently clear throughout the paper.\nThe purpose or argument is generally unclear.\n\n\nContent\nThe focus of the paper relates to a relevant social issue and includes citations and support from the literature.\nThe content of the paper provides reasonable support for a social issue but may sometimes be unclear.\nThe content of the paper is not consistent in its relation to a social issue.\nThe content of the paper is generally not related to a social issue.\n\n\nOrganization\nThe ideas are arranged logically and the paper contains the required sections as outlined in the paper assignment.\nThe ideas are arranged logically for the most part but may not include all required details or sections.\nThe ideas are arranged in some logical fashion but fail to provide the required details and sections.\nThe ideas are not arranged logically and fail to attend to the required sections as outlined in the paper assignment.\n\n\nR Code\nAll code is original and directly relates to the logic model and theory.\nThe code is original but may sometimes digress from the logic model and theory.\nThe code is somewhat original but fails to to relate to the logic model or theory.\nThe code is not original and does not relate to the logical model and theory."
  },
  {
    "objectID": "weeks/week15.html#part-i-content",
    "href": "weeks/week15.html#part-i-content",
    "title": "DATA 202 - Week 15",
    "section": "",
    "text": "Open session to support you in your final data analysis.\n\n\n\nThe rubric for the final assessment (paper 4) will be used to assess your growth over the course of the semester. In addition to the requirements outlined in the assignment, the rubric will be used to assess the overall quality of the submission.\n\nDATA 202 final assessment rubric\n\n\n\n\n\n\n\n\n\nCriteria\nExemplary\nGood\nAcceptable\nIncomplete\n\n\n\n\nPurpose\nThe central purpose or argument is readily apparent.\nThe writing has a clear purpose or argument, but may sometimes digress.\nThe central purpose or argument is not consistently clear throughout the paper.\nThe purpose or argument is generally unclear.\n\n\nContent\nThe focus of the paper relates to a relevant social issue and includes citations and support from the literature.\nThe content of the paper provides reasonable support for a social issue but may sometimes be unclear.\nThe content of the paper is not consistent in its relation to a social issue.\nThe content of the paper is generally not related to a social issue.\n\n\nOrganization\nThe ideas are arranged logically and the paper contains the required sections as outlined in the paper assignment.\nThe ideas are arranged logically for the most part but may not include all required details or sections.\nThe ideas are arranged in some logical fashion but fail to provide the required details and sections.\nThe ideas are not arranged logically and fail to attend to the required sections as outlined in the paper assignment.\n\n\nR Code\nAll code is original and directly relates to the logic model and theory.\nThe code is original but may sometimes digress from the logic model and theory.\nThe code is somewhat original but fails to to relate to the logic model or theory.\nThe code is not original and does not relate to the logical model and theory."
  },
  {
    "objectID": "weeks/week15.html#part-ii-content",
    "href": "weeks/week15.html#part-ii-content",
    "title": "DATA 202 - Week 15",
    "section": "Part II: Content",
    "text": "Part II: Content\nLive coding help as needed."
  },
  {
    "objectID": "weeks/week15.html#part-iii-code",
    "href": "weeks/week15.html#part-iii-code",
    "title": "DATA 202 - Week 15",
    "section": "Part III: Code",
    "text": "Part III: Code\nData analysis examples:\nThere is a wealth of information on the web to support your future work in R/RStudio. These sources are useful for analyzing data that you find online and want to perform various tests on. Below, I outline some of these sources that can help extend your analysis as you move forward. I will also note that many of these examples are better selections than current AI software and sites (e.g., ChatGPT).\n\nUsing R for Multivariate Analysis\nAn Introduction to Political and Social Data Analysis Using R\nModern Statistics with R\nR for Graduate Students\nR for Social Scientists\nData Analysis Examples"
  },
  {
    "objectID": "weeks/week15-slides.html#part-i-content",
    "href": "weeks/week15-slides.html#part-i-content",
    "title": "DATA 202 - Week 15",
    "section": "Part I: Content",
    "text": "Part I: Content\nOpen session to support you in your final data analysis."
  },
  {
    "objectID": "weeks/week15-slides.html#part-ii-content",
    "href": "weeks/week15-slides.html#part-ii-content",
    "title": "DATA 202 - Week 15",
    "section": "Part II: Content",
    "text": "Part II: Content\nLive coding help as needed."
  },
  {
    "objectID": "weeks/week15-slides.html#part-iii-code",
    "href": "weeks/week15-slides.html#part-iii-code",
    "title": "DATA 202 - Week 15",
    "section": "Part III: Code",
    "text": "Part III: Code\nData analysis examples:\nThere is a wealth of information on the web to support your future work in R/RStudio. These sources are useful for analyzing data that you find online and want to perform various tests on. Below, I outline some of these sources that can help extend your analysis as you move forward. I will also note that many of these examples are better selections than current AI software and sites (e.g., ChatGPT).\n\nUsing R for Multivariate Analysis\nAn Introduction to Political and Social Data Analysis Using R\nModern Statistics with R\nR for Graduate Students\nR for Social Scientists\nData Analysis Examples\n\n\n\n\n\nCourse Data GitHub"
  },
  {
    "objectID": "weeks/week14.html",
    "href": "weeks/week14.html",
    "title": "DATA 202 - Week 14",
    "section": "",
    "text": "Now that we have many foundational elements identified and practiced - such as generating code to explore data, cleaning data for analysis, and some elements of theory construction - we can begin focusing on some of the important technical components of model building and analysis: interpretation.\n\nInterpretation relies very heavily on both your research question and the subsequent empirical study.\nWhile your research question may be based on a host of factors, your empirical study relies on a combination of:\n\nTheoretical frameworks\nAnalytic method\nInterpretations\n\n\n\n\n\n\nA suggestive and indicative mode of the triangulation method from Tzagkarakis & Kritas (2023).\n\n\n\n\n\nThe below research questions highlight the intersection of social justice issues in multiple variable quantitative analysis. Keep in mind that these questions can be further refined and tailored to specific contexts or issues of interest within the realm of social justice.\n\nHow does income inequality and geographical location affect access to quality education?\nWhat disparities in the criminal justice system by race and gender?\nHow does gender discrimination and age impact career advancement in the workplace?\nWhat are the effects of housing policies and income on residential segregation and access to affordable housing?\nHow does healthcare accessibility and affordability vary across different socioeconomic groups?\n\n\n\n\n\nLet us continue with a sample analysis.\nWe will assume that state data collected for a sample of 100 randomly selected cities requesting funding after the approval of a new bill on affordable housing. The data set includes three key variables.\n\nResearch question\n\nWhat is the relationship between state funding for affordable housing initiatives and the availability of new affordable housing units?\n\n\n\nDetails about each variable are provided below:\n\ncity is a marker (which matches the data index) used to indicate a randomly selected city.\nfunding is the total amount of funding provided to families (in thousands of dollars) in a given 3-week period\nhousing_availability is the average of city housing units allocated over the same funding period\nadvocacy is the average number of calls to the state representatives’ hotline four months prior\n\nThe advocacy variable was generated as a result of a similar study conducted in a neighboring state, which noticed that there was a potential lag-relationship between advocacy and funding allocations approved at the state-level.\n\n\nhead(data)\n\n  city funding housing_availability advocacy\n1    1  251.32                34.60    21.15\n2    2  422.71                50.32    35.27\n3    3  418.33                45.47    28.54\n4    4  423.08                46.43    27.23\n5    5  503.13                55.97    36.67\n6    6  428.78                60.77    27.49\n\ntail(data)\n\n    city funding housing_availability advocacy\n95    95  248.48                55.39    18.27\n96    96  385.40                58.23    33.48\n97    97  314.17                67.69    26.19\n98    98  222.00                52.38    22.95\n99    99  317.35                57.37    18.10\n100  100  463.09                59.65    27.02\n\n\n\nsummary(data)\n\n      city           funding      housing_availability    advocacy    \n Min.   :  1.00   Min.   :216.2   Min.   :34.60        Min.   :16.19  \n 1st Qu.: 25.75   1st Qu.:280.4   1st Qu.:48.54        1st Qu.:22.52  \n Median : 50.50   Median :343.4   Median :54.97        Median :27.04  \n Mean   : 50.50   Mean   :360.4   Mean   :54.54        Mean   :26.63  \n 3rd Qu.: 75.25   3rd Qu.:437.7   3rd Qu.:59.55        3rd Qu.:30.30  \n Max.   :100.00   Max.   :547.4   Max.   :77.86        Max.   :37.34  \n\n\n\n\n\n\nWe can use some base-R commands to get a quick summary of each variable.\n\n# get plots of variables\nhist(funding)\n\n\n\n\n\n\n\nhist(housing_availability)\n\n\n\n\n\n\n\n\n\n\n\n\n\n# get summary statistics for variables\nsummary(funding)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  216.2   280.4   343.4   360.4   437.7   547.4 \n\nsummary(housing_availability)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  34.60   48.54   54.97   54.54   59.55   77.86 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can also produce quick plots to examine the relationship between each variable.\nHere, we include code to get the correlation coefficient.\n\n# perform correlation analysis\nplot(funding, housing_availability)\n\n\n\n\n\n\n\ncor(funding, housing_availability)\n\n[1] 0.266359\n\nplot(advocacy, funding)\n\n\n\n\n\n\n\ncor(advocacy, funding)\n\n[1] 0.4757307\n\nplot(advocacy, housing_availability)\n\n\n\n\n\n\n\ncor(advocacy, housing_availability)\n\n[1] 0.1444811\n\n\n\n\n\n\nFirst, researchers decided to run a linear regression model on housing_availability and funding.\n\n# perform linear regression analysis\nmodel1 &lt;- lm(housing_availability ~ funding)\n\n# summary of the regression model\nsummary(model1)\n\n\nCall:\nlm(formula = housing_availability ~ funding)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-18.1793  -5.9060  -0.6551   5.0543  22.4049 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 45.49080    3.41826  13.308  &lt; 2e-16 ***\nfunding      0.02511    0.00918   2.736  0.00739 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.579 on 98 degrees of freedom\nMultiple R-squared:  0.07095,   Adjusted R-squared:  0.06147 \nF-statistic: 7.484 on 1 and 98 DF,  p-value: 0.007391\n\n\n\n\n\n\nggplot(data, aes(x = funding, y = housing_availability)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"City Funding\", y = \"Housing Availability\", title = \"Relationship between City Funding and Housing Availability\")\n\n\n\n\n\n\n\n\n\nOne researcher, however, suggested that a more robust regression analysis should be used with OLS techniques. Robust regression analysis, as you may recall, helps us reduce outlier effects.\nNote: we need to load the MASS package and library to run the following code.\n\nols &lt;- lm(housing_availability ~ funding)\nsummary(ols)\n\n\nCall:\nlm(formula = housing_availability ~ funding)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-18.1793  -5.9060  -0.6551   5.0543  22.4049 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 45.49080    3.41826  13.308  &lt; 2e-16 ***\nfunding      0.02511    0.00918   2.736  0.00739 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.579 on 98 degrees of freedom\nMultiple R-squared:  0.07095,   Adjusted R-squared:  0.06147 \nF-statistic: 7.484 on 1 and 98 DF,  p-value: 0.007391\n\nopar &lt;- par(mfrow = c(2, 2), oma = c(0, 0, 1.1, 0))\nplot(ols, las = 1)\n\n\n\n\n\n\n\npar(opar) # we use the par() function to restore graphical parameters to their original values\n\n\nFrom this analysis, we see that a few observations are possibly problematic to our model.\nWe can explore some of these observations in more detail.\n\ndata[c(12, 50, 73), 1:4]\n\n   city funding housing_availability advocacy\n12   12  396.66                77.86    30.15\n50   50  470.96                75.79    22.38\n73   73  217.93                68.29    27.94\n\n\n\nThe three cities noted (and there may be others) have large residuals.\nWe can examine these in more detail.\n\ndistance &lt;- cooks.distance(ols) # we get a measure of the Cook's distance values.\nres &lt;- stdres(ols)\na &lt;- cbind(data, distance, res)\na[distance &gt; 4/100, ]\n\n   city funding housing_availability advocacy   distance       res\n1     1  251.32                34.60    21.15 0.04984715 -2.029432\n7     7  216.20                64.53    17.56 0.04561739  1.614402\n12   12  396.66                77.86    30.15 0.04014294  2.626744\n16   16  495.17                73.25    25.20 0.05225347  1.813880\n50   50  470.96                75.79    22.38 0.05837888  2.179622\n73   73  217.93                68.29    27.94 0.07252929  2.053558\n75   75  243.32                67.02    20.18 0.04371737  1.820393\n78   78  236.61                36.76    16.89 0.04260811 -1.734093\n\n\n\nThe decisions were made based on the following notes:\n\nCook’s distance cooks.distance() provides a measure of the influence of a data point when performing regression.\nstdres standardized the residuals from our model\ncbind() attaches the two measures to our data frame\n\nWe can use a cutoff point \\(4/n\\) where \\(n\\) is the sample size recommend by others to select the values to display.\n\nWe then get the absolute value of the residuals (remember that the sign does not matter in distance), and we print the observations with the highest residuals (here we focus on the top 10 values).\n\nabsres &lt;- abs(res)\ndata1 &lt;- cbind(data, distance, res, absres)\nassorted &lt;- data1[order(-absres), ]\nassorted[1:10,]\n\n   city funding housing_availability advocacy   distance       res   absres\n12   12  396.66                77.86    30.15 0.04014294  2.626744 2.626744\n50   50  470.96                75.79    22.38 0.05837888  2.179622 2.179622\n88   88  317.76                35.29    20.73 0.02780093 -2.131960 2.131960\n25   25  286.74                70.57    22.02 0.03637332  2.100555 2.100555\n73   73  217.93                68.29    27.94 0.07252929  2.053558 2.053558\n1     1  251.32                34.60    21.15 0.04984715 -2.029432 2.029432\n85   85  279.11                36.86    19.93 0.03027410 -1.839822 1.839822\n75   75  243.32                67.02    20.18 0.04371737  1.820393 1.820393\n16   16  495.17                73.25    25.20 0.05225347  1.813880 1.813880\n78   78  236.61                36.76    16.89 0.04260811 -1.734093 1.734093\n\n\n\nWe now run our robust regression analysis.\nWe do this by using the rlm() function in the MASS package.\nThere are several weights that can be used for the iterated re-weighted least squares technique (IRLS)1.\n\nrrmodel &lt;- rlm(housing_availability ~ funding, data = data)\nsummary(rrmodel)\n\n\nCall: rlm(formula = housing_availability ~ funding, data = data)\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.9198  -5.4416  -0.3424   5.2609  22.8048 \n\nCoefficients:\n            Value   Std. Error t value\n(Intercept) 45.7779  3.5798    12.7880\nfunding      0.0234  0.0096     2.4328\n\nResidual standard error: 8.213 on 98 degrees of freedom\n\n\n\nThe default weight is the Huber weight.\nHuber weights are a type of weight function used to downweight or mitigate the influence of outliers on the estimation procedure.\nIn traditional least squares regression, all data points are given equal weight, and the estimation procedure is sensitive to the presence of outliers. The use of weights in our robust regression model aims to provide more robust estimates by assigning different weights to the observations, giving less influence to outliers.\n\n\nhweights &lt;- data.frame(city = data$city, resid = rrmodel$resid, weight = rrmodel$w)\nhweights2 &lt;- hweights[order(rrmodel$w),]\nhweights2[1:15,]\n\n   city     resid    weight\n12   12  22.80484 0.4843946\n50   50  18.99708 0.5814916\n25   25  18.08570 0.6107904\n88   88 -17.91981 0.6164041\n73   73  17.41506 0.6343102\n1     1 -17.05588 0.6476276\n16   16  15.89084 0.6951638\n75   75  15.55123 0.7103366\n85   85 -15.44584 0.7151312\n43   43  14.97271 0.7377868\n97   97  14.56416 0.7584838\n78   78 -14.55183 0.7590662\n9     9  14.36036 0.7692537\n7     7  13.69553 0.8065877\n33   33 -12.74093 0.8669457\n\n\nHuber weights assign larger weights to observations that are close to the regression line and smaller weights to observations that deviate significantly from the line. The weight assigned to each observation depends on its residuals (the difference between the observed values and the predicted values).\n\n\n\n\nDespite our work on the initial model, the issue of causality needs to be discussed.\nThere are a few considerations that need to be taken into account:\n\nConfounding variables: There may be other factors that influence the model apart from city funding. For example, economic conditions, housing availability, and social policies can also play significant roles. Failing to account for these confounding variables may lead to erroneous conclusions about the causal relationship.\nReverse causality: The relationships can be bidirectional. Higher housing availability rates may lead to increased city funding directed at addressing the issue. Thus, it’s possible that the relationship is driven by reverse causality, where higher levels of housing availability cause increased funding rather than the other way around.\nOmitted variable bias: There may be unobserved or unmeasured factors that affect both city funding and housing availability. Failing to include these variables in the analysis can lead to omitted variable bias, potentially distorting the estimated relationships.\nEcological fallacy: Analyzing aggregated data across the state- and city- levels may not capture the correct level of nuances within the relationship. Aggregating data can lead to an ecological fallacy, where conclusions made at the aggregate level may not hold true at different levels.\n\n\n\n\n\nMulticollinearity refers to a high correlation or linear relationship between two or more predictor variables in a regression model. In the case of three variables, multicollinearity occurs when there is a strong linear relationship between any pair of the three variables, making it difficult to separate their individual effects on the response variable. This can cause instability in the regression model, inflated standard errors, and difficulties in interpreting the coefficients.\n\nAssume we updated our theoretical statement and research question and add the advocacy variable to our model.\n\n# perform linear regression analysis\nmodel2 &lt;- lm(housing_availability ~ funding + advocacy)\n\n# summary of the regression model\nsummary(model2)\n\n\nCall:\nlm(formula = housing_availability ~ funding + advocacy)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.9890  -6.1250  -0.6158   4.9763  22.3024 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 44.80516    4.77827   9.377 2.97e-15 ***\nfunding      0.02408    0.01049   2.296   0.0238 *  \nadvocacy     0.03969    0.19229   0.206   0.8369    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.621 on 97 degrees of freedom\nMultiple R-squared:  0.07136,   Adjusted R-squared:  0.05221 \nF-statistic: 3.727 on 2 and 97 DF,  p-value: 0.02759\n\n\n\n\n\nNext, we add an interaction term to our model.\n\n# get a summary of the advocacy data\nsummary(advocacy)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  16.19   22.52   27.04   26.63   30.30   37.34 \n\n# examine the relationship between funding and advocacy\ncor(advocacy, funding)\n\n[1] 0.4757307\n\n# perform linear regression analysis\nmodel3 &lt;- lm(housing_availability ~ funding + advocacy + funding*advocacy)\n\n# summary of the regression model\nsummary(model3)\n\n\nCall:\nlm(formula = housing_availability ~ funding + advocacy + funding * \n    advocacy)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.9963  -6.2218  -0.5457   4.8889  22.3465 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)      49.0944885 17.5511591   2.797  0.00623 **\nfunding           0.0117777  0.0495659   0.238  0.81268   \nadvocacy         -0.1236422  0.6712607  -0.184  0.85425   \nfunding:advocacy  0.0004576  0.0018009   0.254  0.79997   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.663 on 96 degrees of freedom\nMultiple R-squared:  0.07198,   Adjusted R-squared:  0.04298 \nF-statistic: 2.482 on 3 and 96 DF,  p-value: 0.06555\n\n\nPlease note that we may need to run additional tests or more robust models to inform interpretation.\n\n\n\n\nWhen analyzing the relationship between state funding and housing availability, it is important to consider both statistical significance and practical significance.\nStatistical significance refers to the likelihood that the observed relationship or difference between variables is not due to chance. It is determined through statistical tests, such as hypothesis testing or p-values. In this context, statistical significance would indicate whether there is evidence to suggest that state funding has a statistically significant effect on housing availability. A statistically significant result suggests that the relationship between the variables is unlikely to have occurred by random chance.\nPractical significance focuses on the magnitude or practical importance of the observed relationship. It asks whether the observed effect size is meaningful or substantial in real-world terms. In the case of state funding and housing availability, practical significance would involve evaluating whether the observed impact of state funding on housing availability is large enough to have a meaningful or substantial effect on the availability of housing units.\nNote, however, that while statistical significance provides evidence of a relationship, it does not necessarily imply practical importance. A statistically significant relationship may exist but have a negligible or trivial effect in practice. Conversely, a relationship may have practical significance, even if it does not reach statistical significance due to limited sample size or other factors.\n\n\n\n\n\nExploring varied statistical outputs and their significance in a social justice context requires care, both in terms of the underlying theories that relate to the variables themselves and their use across different context. An additional factor that we have discussed relates to the role of the theoretical constructions and their applicability to issues of social injustice.\nMore often than not, caution should take the lead when developing new models. In these instances, some variation on what is known as a replication study can become a valuable tool. A replication study is a type of study that aims to reproduce or replicate the findings of a previous study. In the context of our course, the replication frameworks can be applied to examine the relationships between variables across contexts and different populations.\nThere are different types of replication studies.\n\nDirect replication: In this replication study type, researchers attempt to reproduce the original study as closely as possible, meaning they follow the same research design, methodologies, and data analysis procedures.\nPartial replication: In this replication study type, researchers attempt to replicate only a portion of the original study. Often, researchers doing a partial replication study focus on a specific aspect, variable, or component of the study.\nConceptual replication: In this replication study type, researchers conduct a replication analysis that focuses on the same research question(s) but through the use of different methods, measures, or population groups.\n\nWhile replication studies are often used to help ensure the credibility and seeming generalizations found in statistical research findings, they can also serve as a part of a broader process to examine the role of context in statistical models. Importantly, failure to replicate the findings of a study do not mean that the original study findings were incorrect or flawed. Together, these types of explorations can contribute to scientific knowledge and provide evidence to help us understand the role of theory and the practice of social justice.\n\n\n\n\nResearchers have access to a wide range of advanced statistical techniques and methodologies that provide deeper insights into complex relationships and patterns within data. These approaches go beyond the linear relationships examined in regression analysis and allow researchers to explore non-linear, interactive, and dynamic effects among variables. By utilizing these advanced techniques, researchers can uncover hidden patterns, make more accurate predictions, account for complex interactions, and gain a more comprehensive understanding of the phenomena under investigation.\nSome of these methods often provide greater flexibility in handling missing data, dealing with outliers, and accommodating various types of data structures. Overall, the utilization of these advanced statistical techniques expands the availability of tools to consider ways to delve deeper into the complexities of their data and extract meaningful insights.",
    "crumbs": [
      "Weekly materials",
      "Week 14"
    ]
  },
  {
    "objectID": "weeks/week14.html#part-i-context",
    "href": "weeks/week14.html#part-i-context",
    "title": "DATA 202 - Week 14",
    "section": "",
    "text": "Now that we have many foundational elements identified and practiced - such as generating code to explore data, cleaning data for analysis, and some elements of theory construction - we can begin focusing on some of the important technical components of model building and analysis: interpretation.\n\nInterpretation relies very heavily on both your research question and the subsequent empirical study.\nWhile your research question may be based on a host of factors, your empirical study relies on a combination of:\n\nTheoretical frameworks\nAnalytic method\nInterpretations\n\n\n\n\n\n\nA suggestive and indicative mode of the triangulation method from Tzagkarakis & Kritas (2023).\n\n\n\n\n\nThe below research questions highlight the intersection of social justice issues in multiple variable quantitative analysis. Keep in mind that these questions can be further refined and tailored to specific contexts or issues of interest within the realm of social justice.\n\nHow does income inequality and geographical location affect access to quality education?\nWhat disparities in the criminal justice system by race and gender?\nHow does gender discrimination and age impact career advancement in the workplace?\nWhat are the effects of housing policies and income on residential segregation and access to affordable housing?\nHow does healthcare accessibility and affordability vary across different socioeconomic groups?\n\n\n\n\n\nLet us continue with a sample analysis.\nWe will assume that state data collected for a sample of 100 randomly selected cities requesting funding after the approval of a new bill on affordable housing. The data set includes three key variables.\n\nResearch question\n\nWhat is the relationship between state funding for affordable housing initiatives and the availability of new affordable housing units?\n\n\n\nDetails about each variable are provided below:\n\ncity is a marker (which matches the data index) used to indicate a randomly selected city.\nfunding is the total amount of funding provided to families (in thousands of dollars) in a given 3-week period\nhousing_availability is the average of city housing units allocated over the same funding period\nadvocacy is the average number of calls to the state representatives’ hotline four months prior\n\nThe advocacy variable was generated as a result of a similar study conducted in a neighboring state, which noticed that there was a potential lag-relationship between advocacy and funding allocations approved at the state-level.\n\n\nhead(data)\n\n  city funding housing_availability advocacy\n1    1  251.32                34.60    21.15\n2    2  422.71                50.32    35.27\n3    3  418.33                45.47    28.54\n4    4  423.08                46.43    27.23\n5    5  503.13                55.97    36.67\n6    6  428.78                60.77    27.49\n\ntail(data)\n\n    city funding housing_availability advocacy\n95    95  248.48                55.39    18.27\n96    96  385.40                58.23    33.48\n97    97  314.17                67.69    26.19\n98    98  222.00                52.38    22.95\n99    99  317.35                57.37    18.10\n100  100  463.09                59.65    27.02\n\n\n\nsummary(data)\n\n      city           funding      housing_availability    advocacy    \n Min.   :  1.00   Min.   :216.2   Min.   :34.60        Min.   :16.19  \n 1st Qu.: 25.75   1st Qu.:280.4   1st Qu.:48.54        1st Qu.:22.52  \n Median : 50.50   Median :343.4   Median :54.97        Median :27.04  \n Mean   : 50.50   Mean   :360.4   Mean   :54.54        Mean   :26.63  \n 3rd Qu.: 75.25   3rd Qu.:437.7   3rd Qu.:59.55        3rd Qu.:30.30  \n Max.   :100.00   Max.   :547.4   Max.   :77.86        Max.   :37.34  \n\n\n\n\n\n\nWe can use some base-R commands to get a quick summary of each variable.\n\n# get plots of variables\nhist(funding)\n\n\n\n\n\n\n\nhist(housing_availability)\n\n\n\n\n\n\n\n\n\n\n\n\n\n# get summary statistics for variables\nsummary(funding)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  216.2   280.4   343.4   360.4   437.7   547.4 \n\nsummary(housing_availability)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  34.60   48.54   54.97   54.54   59.55   77.86 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can also produce quick plots to examine the relationship between each variable.\nHere, we include code to get the correlation coefficient.\n\n# perform correlation analysis\nplot(funding, housing_availability)\n\n\n\n\n\n\n\ncor(funding, housing_availability)\n\n[1] 0.266359\n\nplot(advocacy, funding)\n\n\n\n\n\n\n\ncor(advocacy, funding)\n\n[1] 0.4757307\n\nplot(advocacy, housing_availability)\n\n\n\n\n\n\n\ncor(advocacy, housing_availability)\n\n[1] 0.1444811\n\n\n\n\n\n\nFirst, researchers decided to run a linear regression model on housing_availability and funding.\n\n# perform linear regression analysis\nmodel1 &lt;- lm(housing_availability ~ funding)\n\n# summary of the regression model\nsummary(model1)\n\n\nCall:\nlm(formula = housing_availability ~ funding)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-18.1793  -5.9060  -0.6551   5.0543  22.4049 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 45.49080    3.41826  13.308  &lt; 2e-16 ***\nfunding      0.02511    0.00918   2.736  0.00739 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.579 on 98 degrees of freedom\nMultiple R-squared:  0.07095,   Adjusted R-squared:  0.06147 \nF-statistic: 7.484 on 1 and 98 DF,  p-value: 0.007391\n\n\n\n\n\n\nggplot(data, aes(x = funding, y = housing_availability)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"City Funding\", y = \"Housing Availability\", title = \"Relationship between City Funding and Housing Availability\")\n\n\n\n\n\n\n\n\n\nOne researcher, however, suggested that a more robust regression analysis should be used with OLS techniques. Robust regression analysis, as you may recall, helps us reduce outlier effects.\nNote: we need to load the MASS package and library to run the following code.\n\nols &lt;- lm(housing_availability ~ funding)\nsummary(ols)\n\n\nCall:\nlm(formula = housing_availability ~ funding)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-18.1793  -5.9060  -0.6551   5.0543  22.4049 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 45.49080    3.41826  13.308  &lt; 2e-16 ***\nfunding      0.02511    0.00918   2.736  0.00739 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.579 on 98 degrees of freedom\nMultiple R-squared:  0.07095,   Adjusted R-squared:  0.06147 \nF-statistic: 7.484 on 1 and 98 DF,  p-value: 0.007391\n\nopar &lt;- par(mfrow = c(2, 2), oma = c(0, 0, 1.1, 0))\nplot(ols, las = 1)\n\n\n\n\n\n\n\npar(opar) # we use the par() function to restore graphical parameters to their original values\n\n\nFrom this analysis, we see that a few observations are possibly problematic to our model.\nWe can explore some of these observations in more detail.\n\ndata[c(12, 50, 73), 1:4]\n\n   city funding housing_availability advocacy\n12   12  396.66                77.86    30.15\n50   50  470.96                75.79    22.38\n73   73  217.93                68.29    27.94\n\n\n\nThe three cities noted (and there may be others) have large residuals.\nWe can examine these in more detail.\n\ndistance &lt;- cooks.distance(ols) # we get a measure of the Cook's distance values.\nres &lt;- stdres(ols)\na &lt;- cbind(data, distance, res)\na[distance &gt; 4/100, ]\n\n   city funding housing_availability advocacy   distance       res\n1     1  251.32                34.60    21.15 0.04984715 -2.029432\n7     7  216.20                64.53    17.56 0.04561739  1.614402\n12   12  396.66                77.86    30.15 0.04014294  2.626744\n16   16  495.17                73.25    25.20 0.05225347  1.813880\n50   50  470.96                75.79    22.38 0.05837888  2.179622\n73   73  217.93                68.29    27.94 0.07252929  2.053558\n75   75  243.32                67.02    20.18 0.04371737  1.820393\n78   78  236.61                36.76    16.89 0.04260811 -1.734093\n\n\n\nThe decisions were made based on the following notes:\n\nCook’s distance cooks.distance() provides a measure of the influence of a data point when performing regression.\nstdres standardized the residuals from our model\ncbind() attaches the two measures to our data frame\n\nWe can use a cutoff point \\(4/n\\) where \\(n\\) is the sample size recommend by others to select the values to display.\n\nWe then get the absolute value of the residuals (remember that the sign does not matter in distance), and we print the observations with the highest residuals (here we focus on the top 10 values).\n\nabsres &lt;- abs(res)\ndata1 &lt;- cbind(data, distance, res, absres)\nassorted &lt;- data1[order(-absres), ]\nassorted[1:10,]\n\n   city funding housing_availability advocacy   distance       res   absres\n12   12  396.66                77.86    30.15 0.04014294  2.626744 2.626744\n50   50  470.96                75.79    22.38 0.05837888  2.179622 2.179622\n88   88  317.76                35.29    20.73 0.02780093 -2.131960 2.131960\n25   25  286.74                70.57    22.02 0.03637332  2.100555 2.100555\n73   73  217.93                68.29    27.94 0.07252929  2.053558 2.053558\n1     1  251.32                34.60    21.15 0.04984715 -2.029432 2.029432\n85   85  279.11                36.86    19.93 0.03027410 -1.839822 1.839822\n75   75  243.32                67.02    20.18 0.04371737  1.820393 1.820393\n16   16  495.17                73.25    25.20 0.05225347  1.813880 1.813880\n78   78  236.61                36.76    16.89 0.04260811 -1.734093 1.734093\n\n\n\nWe now run our robust regression analysis.\nWe do this by using the rlm() function in the MASS package.\nThere are several weights that can be used for the iterated re-weighted least squares technique (IRLS)1.\n\nrrmodel &lt;- rlm(housing_availability ~ funding, data = data)\nsummary(rrmodel)\n\n\nCall: rlm(formula = housing_availability ~ funding, data = data)\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.9198  -5.4416  -0.3424   5.2609  22.8048 \n\nCoefficients:\n            Value   Std. Error t value\n(Intercept) 45.7779  3.5798    12.7880\nfunding      0.0234  0.0096     2.4328\n\nResidual standard error: 8.213 on 98 degrees of freedom\n\n\n\nThe default weight is the Huber weight.\nHuber weights are a type of weight function used to downweight or mitigate the influence of outliers on the estimation procedure.\nIn traditional least squares regression, all data points are given equal weight, and the estimation procedure is sensitive to the presence of outliers. The use of weights in our robust regression model aims to provide more robust estimates by assigning different weights to the observations, giving less influence to outliers.\n\n\nhweights &lt;- data.frame(city = data$city, resid = rrmodel$resid, weight = rrmodel$w)\nhweights2 &lt;- hweights[order(rrmodel$w),]\nhweights2[1:15,]\n\n   city     resid    weight\n12   12  22.80484 0.4843946\n50   50  18.99708 0.5814916\n25   25  18.08570 0.6107904\n88   88 -17.91981 0.6164041\n73   73  17.41506 0.6343102\n1     1 -17.05588 0.6476276\n16   16  15.89084 0.6951638\n75   75  15.55123 0.7103366\n85   85 -15.44584 0.7151312\n43   43  14.97271 0.7377868\n97   97  14.56416 0.7584838\n78   78 -14.55183 0.7590662\n9     9  14.36036 0.7692537\n7     7  13.69553 0.8065877\n33   33 -12.74093 0.8669457\n\n\nHuber weights assign larger weights to observations that are close to the regression line and smaller weights to observations that deviate significantly from the line. The weight assigned to each observation depends on its residuals (the difference between the observed values and the predicted values).\n\n\n\n\nDespite our work on the initial model, the issue of causality needs to be discussed.\nThere are a few considerations that need to be taken into account:\n\nConfounding variables: There may be other factors that influence the model apart from city funding. For example, economic conditions, housing availability, and social policies can also play significant roles. Failing to account for these confounding variables may lead to erroneous conclusions about the causal relationship.\nReverse causality: The relationships can be bidirectional. Higher housing availability rates may lead to increased city funding directed at addressing the issue. Thus, it’s possible that the relationship is driven by reverse causality, where higher levels of housing availability cause increased funding rather than the other way around.\nOmitted variable bias: There may be unobserved or unmeasured factors that affect both city funding and housing availability. Failing to include these variables in the analysis can lead to omitted variable bias, potentially distorting the estimated relationships.\nEcological fallacy: Analyzing aggregated data across the state- and city- levels may not capture the correct level of nuances within the relationship. Aggregating data can lead to an ecological fallacy, where conclusions made at the aggregate level may not hold true at different levels.\n\n\n\n\n\nMulticollinearity refers to a high correlation or linear relationship between two or more predictor variables in a regression model. In the case of three variables, multicollinearity occurs when there is a strong linear relationship between any pair of the three variables, making it difficult to separate their individual effects on the response variable. This can cause instability in the regression model, inflated standard errors, and difficulties in interpreting the coefficients.\n\nAssume we updated our theoretical statement and research question and add the advocacy variable to our model.\n\n# perform linear regression analysis\nmodel2 &lt;- lm(housing_availability ~ funding + advocacy)\n\n# summary of the regression model\nsummary(model2)\n\n\nCall:\nlm(formula = housing_availability ~ funding + advocacy)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.9890  -6.1250  -0.6158   4.9763  22.3024 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 44.80516    4.77827   9.377 2.97e-15 ***\nfunding      0.02408    0.01049   2.296   0.0238 *  \nadvocacy     0.03969    0.19229   0.206   0.8369    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.621 on 97 degrees of freedom\nMultiple R-squared:  0.07136,   Adjusted R-squared:  0.05221 \nF-statistic: 3.727 on 2 and 97 DF,  p-value: 0.02759\n\n\n\n\n\nNext, we add an interaction term to our model.\n\n# get a summary of the advocacy data\nsummary(advocacy)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  16.19   22.52   27.04   26.63   30.30   37.34 \n\n# examine the relationship between funding and advocacy\ncor(advocacy, funding)\n\n[1] 0.4757307\n\n# perform linear regression analysis\nmodel3 &lt;- lm(housing_availability ~ funding + advocacy + funding*advocacy)\n\n# summary of the regression model\nsummary(model3)\n\n\nCall:\nlm(formula = housing_availability ~ funding + advocacy + funding * \n    advocacy)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.9963  -6.2218  -0.5457   4.8889  22.3465 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)      49.0944885 17.5511591   2.797  0.00623 **\nfunding           0.0117777  0.0495659   0.238  0.81268   \nadvocacy         -0.1236422  0.6712607  -0.184  0.85425   \nfunding:advocacy  0.0004576  0.0018009   0.254  0.79997   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.663 on 96 degrees of freedom\nMultiple R-squared:  0.07198,   Adjusted R-squared:  0.04298 \nF-statistic: 2.482 on 3 and 96 DF,  p-value: 0.06555\n\n\nPlease note that we may need to run additional tests or more robust models to inform interpretation.\n\n\n\n\nWhen analyzing the relationship between state funding and housing availability, it is important to consider both statistical significance and practical significance.\nStatistical significance refers to the likelihood that the observed relationship or difference between variables is not due to chance. It is determined through statistical tests, such as hypothesis testing or p-values. In this context, statistical significance would indicate whether there is evidence to suggest that state funding has a statistically significant effect on housing availability. A statistically significant result suggests that the relationship between the variables is unlikely to have occurred by random chance.\nPractical significance focuses on the magnitude or practical importance of the observed relationship. It asks whether the observed effect size is meaningful or substantial in real-world terms. In the case of state funding and housing availability, practical significance would involve evaluating whether the observed impact of state funding on housing availability is large enough to have a meaningful or substantial effect on the availability of housing units.\nNote, however, that while statistical significance provides evidence of a relationship, it does not necessarily imply practical importance. A statistically significant relationship may exist but have a negligible or trivial effect in practice. Conversely, a relationship may have practical significance, even if it does not reach statistical significance due to limited sample size or other factors.\n\n\n\n\n\nExploring varied statistical outputs and their significance in a social justice context requires care, both in terms of the underlying theories that relate to the variables themselves and their use across different context. An additional factor that we have discussed relates to the role of the theoretical constructions and their applicability to issues of social injustice.\nMore often than not, caution should take the lead when developing new models. In these instances, some variation on what is known as a replication study can become a valuable tool. A replication study is a type of study that aims to reproduce or replicate the findings of a previous study. In the context of our course, the replication frameworks can be applied to examine the relationships between variables across contexts and different populations.\nThere are different types of replication studies.\n\nDirect replication: In this replication study type, researchers attempt to reproduce the original study as closely as possible, meaning they follow the same research design, methodologies, and data analysis procedures.\nPartial replication: In this replication study type, researchers attempt to replicate only a portion of the original study. Often, researchers doing a partial replication study focus on a specific aspect, variable, or component of the study.\nConceptual replication: In this replication study type, researchers conduct a replication analysis that focuses on the same research question(s) but through the use of different methods, measures, or population groups.\n\nWhile replication studies are often used to help ensure the credibility and seeming generalizations found in statistical research findings, they can also serve as a part of a broader process to examine the role of context in statistical models. Importantly, failure to replicate the findings of a study do not mean that the original study findings were incorrect or flawed. Together, these types of explorations can contribute to scientific knowledge and provide evidence to help us understand the role of theory and the practice of social justice.\n\n\n\n\nResearchers have access to a wide range of advanced statistical techniques and methodologies that provide deeper insights into complex relationships and patterns within data. These approaches go beyond the linear relationships examined in regression analysis and allow researchers to explore non-linear, interactive, and dynamic effects among variables. By utilizing these advanced techniques, researchers can uncover hidden patterns, make more accurate predictions, account for complex interactions, and gain a more comprehensive understanding of the phenomena under investigation.\nSome of these methods often provide greater flexibility in handling missing data, dealing with outliers, and accommodating various types of data structures. Overall, the utilization of these advanced statistical techniques expands the availability of tools to consider ways to delve deeper into the complexities of their data and extract meaningful insights.",
    "crumbs": [
      "Weekly materials",
      "Week 14"
    ]
  },
  {
    "objectID": "weeks/week14.html#part-ii-content",
    "href": "weeks/week14.html#part-ii-content",
    "title": "DATA 202 - Week 14",
    "section": "Part II: Content",
    "text": "Part II: Content\nMultiple Variable Analysis and Multivariate Analysis are two terms often used in statistics and research methodology to describe different approaches to analyzing data involving multiple variables. While they share similarities, there are distinct differences between these two concepts.\n\n\nMultivariable vs. Multivariate\nMultiple variable analysis investigates the influence of individual independent variables on a single dependent variable, while multivariate analysis explores the relationships and patterns among multiple variables simultaneously.\nMultiple Variable Analysis is often used when studying the effects of specific factors, while multivariate analysis is employed to uncover broader patterns and structures within a dataset. Both approaches are valuable in data analysis, and the choice between them depends on the research objectives and the nature of the data being analyzed.\n\n\n\n\n\n\n\nDefinitions: Multiple variable analysis vs. Multivariate analysis\n\n\n\nMultiple Variable Analysis: Multiple Variable Analysis refers to the process of examining the relationships between several independent variables and a single dependent variable. It aims to understand how each independent variable influences or predicts the dependent variable individually, while controlling for other variables. In this analysis, each independent variable is analyzed separately, often using techniques such as regression analysis or analysis of variance (ANOVA).\nMultivariate Analysis: Multivariate Analysis involves the simultaneous analysis of multiple dependent and independent variables. It aims to explore the relationships and patterns among multiple variables, considering them as a whole. This analysis technique allows for the examination of complex interactions and associations between variables, providing a more comprehensive understanding of the data.\n\n\n\n\nKey characteristics of multiple variable analysis\n\nFocus: Examining the impact of individual independent variables on a single dependent variable.\nAnalytic approach: Each independent variable is analyzed separately, allowing for isolation of their effects.\nPurpose: To identify the individual contributions and significance of multiple variables in explaining the variation in the dependent variable.\nStatistical techniques: Common techniques include simple linear regression, multiple linear regression, and ANOVA.\n\n\n\n\nKey characteristics of multivariate analysis\n\nFocus: Examining the relationships and interactions among multiple variables simultaneously.\nAnalytic approach: Considering all variables together, accounting for their joint effects and potential interdependence.\nPurpose: To explore patterns, associations, and structures within the data, identifying underlying factors or dimensions.\nStatistical techniques: Common techniques include factor analysis, principal component analysis, cluster analysis, and structural equation modeling.\n\n\n\n\n\nExamples of multivariate analysis techniques\n\nPrincipal component analysis (PCA): PCA is used to reduce the dimensionality of data by transforming it into a new set of uncorrelated variables called principal components. R functions for PCA include prcomp() and princomp().\nFactor analysis: Factor Analysis aims to identify latent factors that explain the correlations among observed variables. R offers functions like factanal() and psych::fa() for conducting factor analysis.\nCanonical correlation analysis (CCA): CCA examines the relationships between two sets of variables and identifies the linear combinations of each set that have maximum correlation with each other. The CCA() function in the stats package can be used for this analysis.\nCluster analysis: Cluster Analysis groups similar observations into clusters based on the similarity of their characteristics. R provides various clustering techniques, such as k-means clustering (kmeans()), hierarchical clustering (hclust()), and model-based clustering (Mclust()).\nDiscriminant analysis: Discriminant Analysis aims to find a linear combination of variables that maximally separate predefined groups or classes. R offers functions like lda() and qda() for performing Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA), respectively.\nMultivariate regression: Multivariate Regression extends simple linear regression to multiple response variables. The lm() function in R can be used for multivariate regression analysis.\nMultivariate analysis of variance (MANOVA): MANOVA extends the analysis of variance (ANOVA) to multiple response variables simultaneously. The manova() function in R can be used for MANOVA.\nMultidimensional scaling (MDS): MDS visualizes the similarity or dissimilarity between objects in a lower-dimensional space. R provides functions like cmdscale() and isoMDS() for performing MDS.\nStructural Equation Modeling (SEM): SEM is a comprehensive framework for testing complex relationships among variables. R packages like lavaan and sem offer functionalities for conducting SEM.\nCorrespondence Analysis: Correspondence Analysis explores the associations between categorical variables and visualizes them in a low-dimensional space. The ca() function in the ca package is commonly used for correspondence analysis.\n\nWe will consider a few of these models in our final weeks for the course.",
    "crumbs": [
      "Weekly materials",
      "Week 14"
    ]
  },
  {
    "objectID": "weeks/week14.html#part-iii-code",
    "href": "weeks/week14.html#part-iii-code",
    "title": "DATA 202 - Week 14",
    "section": "Part III: Code",
    "text": "Part III: Code\nThis week, we use some standard data included in R to further discuss model interpretation.\nWhile these data sets do not directly connect to the content of our course, they provide some useful examples to return to as they are discussed on many websites that use R and that can be found in online forums.\nEach example illustrates different scenarios for interpreting linear models using the summary output. Remember to consider coefficients, standard errors, t-values, and p-values to assess the significance and direction of relationships between predictors and the response variable. Additionally, theory construction and relevant knowledge and context are crucial for a comprehensive interpretation of the results.\n\nThis data is from the 1974 Motor Trend US magazine. The data set comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973–74 models). You could run similar models using data in the critstats package.\n\nnames(mtcars)\n\n [1] \"mpg\"  \"cyl\"  \"disp\" \"hp\"   \"drat\" \"wt\"   \"qsec\" \"vs\"   \"am\"   \"gear\"\n[11] \"carb\"\n\nsummary(mtcars)\n\n      mpg             cyl             disp             hp       \n Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n      drat             wt             qsec             vs        \n Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n       am              gear            carb      \n Min.   :0.0000   Min.   :3.000   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \n Median :0.0000   Median :4.000   Median :2.000  \n Mean   :0.4062   Mean   :3.688   Mean   :2.812  \n 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :1.0000   Max.   :5.000   Max.   :8.000  \n\n\n\nExample 1: Simple Linear Regression\n\n# Fit a simple linear regression model\nmodel &lt;- lm(mpg ~ hp, data = mtcars)\n\n# Print the model summary\nsummary(model)\n\n\nCall:\nlm(formula = mpg ~ hp, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.7121 -2.1122 -0.8854  1.5819  8.2360 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 30.09886    1.63392  18.421  &lt; 2e-16 ***\nhp          -0.06823    0.01012  -6.742 1.79e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.863 on 30 degrees of freedom\nMultiple R-squared:  0.6024,    Adjusted R-squared:  0.5892 \nF-statistic: 45.46 on 1 and 30 DF,  p-value: 1.788e-07\n\n\nThe summary output provides information about the coefficients, standard errors, t-values, and p-values. In this case, the intercept represents the estimated baseline miles per gallon (mpg) when horsepower is zero. The coefficient for horsepower indicates the estimated change in mpg for each unit increase in horsepower.\n\n\n\nExample 2: Multiple Linear Regression\n\n# Fit a multiple linear regression model\nmodel &lt;- lm(mpg ~ hp + wt, data = mtcars)\n\n# Print the model summary\nsummary(model)\n\n\nCall:\nlm(formula = mpg ~ hp + wt, data = mtcars)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.941 -1.600 -0.182  1.050  5.854 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 37.22727    1.59879  23.285  &lt; 2e-16 ***\nhp          -0.03177    0.00903  -3.519  0.00145 ** \nwt          -3.87783    0.63273  -6.129 1.12e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.593 on 29 degrees of freedom\nMultiple R-squared:  0.8268,    Adjusted R-squared:  0.8148 \nF-statistic: 69.21 on 2 and 29 DF,  p-value: 9.109e-12\n\n\nThe summary output provides interpretation for each coefficient. For example, the coefficient for horsepower represents the estimated change in mpg for each unit increase in horsepower, holding weight constant. Similarly, the coefficient for weight represents the estimated change in mpg for each unit increase in weight, holding horsepower constant.\n\n\n\nExample 3: Categorical Predictor\n\n# Fit a linear regression model with a categorical predictor\nmodel &lt;- lm(mpg ~ factor(cyl), data = mtcars)\n\n# Print the model summary\nsummary(model)\n\n\nCall:\nlm(formula = mpg ~ factor(cyl), data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.2636 -1.8357  0.0286  1.3893  7.2364 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   26.6636     0.9718  27.437  &lt; 2e-16 ***\nfactor(cyl)6  -6.9208     1.5583  -4.441 0.000119 ***\nfactor(cyl)8 -11.5636     1.2986  -8.905 8.57e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.223 on 29 degrees of freedom\nMultiple R-squared:  0.7325,    Adjusted R-squared:  0.714 \nF-statistic:  39.7 on 2 and 29 DF,  p-value: 4.979e-09\n\n\nWhen a categorical predictor, such as “cyl” (number of cylinders), is included in the model, R automatically treats it as a set of dummy variables. The summary output provides the coefficients for each category level (e.g., 4 cylinders, 6 cylinders, 8 cylinders). These coefficients represent the estimated difference in the response variable (mpg) compared to the reference category (usually the intercept).\n\n\n\nExample 4: Interaction Effect\n\n# Fit a linear regression model with an interaction term\nmodel &lt;- lm(mpg ~ hp * wt, data = mtcars)\n\n# Print the model summary\nsummary(model)\n\n\nCall:\nlm(formula = mpg ~ hp * wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0632 -1.6491 -0.7362  1.4211  4.5513 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 49.80842    3.60516  13.816 5.01e-14 ***\nhp          -0.12010    0.02470  -4.863 4.04e-05 ***\nwt          -8.21662    1.26971  -6.471 5.20e-07 ***\nhp:wt        0.02785    0.00742   3.753 0.000811 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.153 on 28 degrees of freedom\nMultiple R-squared:  0.8848,    Adjusted R-squared:  0.8724 \nF-statistic: 71.66 on 3 and 28 DF,  p-value: 2.981e-13\n\n\nWhen an interaction term (e.g., horsepower * weight) is included in the model, the summary output provides coefficients for both main effects (horsepower and weight) as well as the interaction term. The interaction coefficient represents the change in the relationship between mpg and horsepower as weight increases.\n\n\n# do some exploratory analysis on the survey data in the MASS package\n\nlibrary(dplyr) \nsurvey \n\n       Sex Wr.Hnd NW.Hnd W.Hnd    Fold Pulse    Clap Exer Smoke Height      M.I\n1   Female   18.5   18.0 Right  R on L    92    Left Some Never 173.00   Metric\n2     Male   19.5   20.5  Left  R on L   104    Left None Regul 177.80 Imperial\n3     Male   18.0   13.3 Right  L on R    87 Neither None Occas     NA     &lt;NA&gt;\n4     Male   18.8   18.9 Right  R on L    NA Neither None Never 160.00   Metric\n5     Male   20.0   20.0 Right Neither    35   Right Some Never 165.00   Metric\n6   Female   18.0   17.7 Right  L on R    64   Right Some Never 172.72 Imperial\n7     Male   17.7   17.7 Right  L on R    83   Right Freq Never 182.88 Imperial\n8   Female   17.0   17.3 Right  R on L    74   Right Freq Never 157.00   Metric\n9     Male   20.0   19.5 Right  R on L    72   Right Some Never 175.00   Metric\n10    Male   18.5   18.5 Right  R on L    90   Right Some Never 167.00   Metric\n11  Female   17.0   17.2 Right  L on R    80   Right Freq Never 156.20 Imperial\n12    Male   21.0   21.0 Right  R on L    68    Left Freq Never     NA     &lt;NA&gt;\n13  Female   16.0   16.0 Right  L on R    NA   Right Some Never 155.00   Metric\n14  Female   19.5   20.2 Right  L on R    66 Neither Some Never 155.00   Metric\n15    Male   16.0   15.5 Right  R on L    60   Right Some Never     NA     &lt;NA&gt;\n16  Female   17.5   17.0 Right  R on L    NA   Right Freq Never 156.00   Metric\n17  Female   18.0   18.0 Right  L on R    89 Neither Freq Never 157.00   Metric\n18    Male   19.4   19.2  Left  R on L    74   Right Some Never 182.88 Imperial\n19    Male   20.5   20.5 Right  L on R    NA    Left Some Never 190.50 Imperial\n20    Male   21.0   20.9 Right  R on L    78   Right Freq Never 177.00   Metric\n21    Male   21.5   22.0 Right  R on L    72    Left Freq Never 190.50 Imperial\n22    Male   20.1   20.7 Right  L on R    72   Right Freq Never 180.34 Imperial\n23    Male   18.5   18.0 Right  L on R    64   Right Freq Never 180.34 Imperial\n24    Male   21.5   21.2 Right  R on L    62   Right Some Never 184.00   Metric\n25  Female   17.0   17.5 Right  R on L    64    Left Some Never     NA     &lt;NA&gt;\n26    Male   18.5   18.5 Right Neither    90 Neither Some Never     NA     &lt;NA&gt;\n27    Male   21.0   20.7 Right  R on L    90   Right Some Never 172.72 Imperial\n28    Male   20.8   21.4 Right  R on L    62 Neither Freq Never 175.26 Imperial\n29    Male   17.8   17.8 Right  L on R    76 Neither Freq Never     NA     &lt;NA&gt;\n30    Male   19.5   19.5 Right  L on R    79   Right Some Never 167.00   Metric\n31  Female   18.5   18.0 Right  R on L    76   Right None Occas     NA     &lt;NA&gt;\n32    Male   18.8   18.2 Right  L on R    78   Right Freq Never 180.00   Metric\n33  Female   17.1   17.5 Right  R on L    72   Right Freq Heavy 166.40 Imperial\n34    Male   20.1   20.0 Right  R on L    70   Right Some Never 180.00   Metric\n35    Male   18.0   19.0 Right  L on R    54 Neither Some Regul     NA     &lt;NA&gt;\n36    Male   22.2   21.0 Right  L on R    66   Right Freq Occas 190.00   Metric\n37  Female   16.0   16.5 Right  L on R    NA   Right Some Never 168.00   Metric\n38    Male   19.4   18.5 Right  R on L    72 Neither Freq Never 182.50   Metric\n39    Male   22.0   22.0 Right  R on L    80   Right Some Never 185.00   Metric\n40    Male   19.0   19.0 Right  R on L    NA Neither Freq Occas 171.00   Metric\n41  Female   17.5   16.0 Right  L on R    NA   Right Some Never 169.00   Metric\n42  Female   17.8   18.0 Right  R on L    72   Right Some Never 154.94 Imperial\n43    Male     NA     NA Right  R on L    60    &lt;NA&gt; Some Never 172.00   Metric\n44  Female   20.1   20.2 Right  L on R    80   Right Some Never 176.50 Imperial\n45  Female   13.0   13.0  &lt;NA&gt;  L on R    70    Left Freq Never 180.34 Imperial\n46    Male   17.0   17.5 Right  R on L    NA Neither Freq Never 180.34 Imperial\n47    Male   23.2   22.7 Right  L on R    84    Left Freq Regul 180.00   Metric\n48    Male   22.5   23.0 Right  R on L    96   Right None Never 170.00   Metric\n49  Female   18.0   17.6 Right  R on L    60   Right Some Occas 168.00   Metric\n50  Female   18.0   17.9 Right  R on L    50    Left None Never 165.00   Metric\n51    Male   22.0   21.5  Left  R on L    55    Left Freq Never 200.00   Metric\n52    Male   20.5   20.0 Right  L on R    68   Right Freq Never 190.00   Metric\n53    Male   17.0   18.0 Right  L on R    78    Left Some Never 170.18 Imperial\n54    Male   20.5   19.5 Right  L on R    56   Right Freq Never 179.00   Metric\n55    Male   22.5   22.5 Right  R on L    65   Right Freq Regul 182.00   Metric\n56    Male   18.5   18.5 Right  L on R    NA Neither Freq Never 171.00   Metric\n57  Female   15.5   15.4 Right  R on L    70 Neither None Never 157.48 Imperial\n58    Male   19.5   19.7 Right  R on L    72   Right Freq Never     NA     &lt;NA&gt;\n59    Male   19.5   19.0 Right  L on R    62   Right Freq Never 177.80 Imperial\n60    Male   20.6   21.0  Left  L on R    NA    Left Freq Occas 175.26 Imperial\n61    Male   22.8   23.2 Right  R on L    66 Neither Freq Never 187.00   Metric\n62  Female   18.5   18.2 Right  R on L    72 Neither Freq Never 167.64 Imperial\n63  Female   19.6   19.7 Right  L on R    70   Right Freq Never 178.00   Metric\n64  Female   18.7   18.0  Left  L on R    NA    Left None Never 170.00   Metric\n65  Female   17.3   18.0 Right  L on R    64 Neither Freq Never 164.00   Metric\n66    Male   19.5   19.8 Right Neither    NA   Right Freq Never 183.00   Metric\n67  Female   19.0   19.1 Right  L on R    NA Neither Freq Never 172.00   Metric\n68  Female   18.5   18.0 Right  R on L    64   Right Freq Never     NA     &lt;NA&gt;\n69    Male   19.0   19.0 Right  L on R    NA   Right Some Never 180.00   Metric\n70    Male   21.0   19.5 Right  L on R    80    Left None  &lt;NA&gt;     NA     &lt;NA&gt;\n71  Female   18.0   17.5 Right  L on R    64    Left Freq Never 170.00   Metric\n72    Male   19.4   19.5 Right  R on L    NA   Right Freq Heavy 176.00   Metric\n73  Female   17.0   16.6 Right  R on L    68   Right Some Never 171.00   Metric\n74  Female   16.5   17.0 Right  L on R    40    Left Freq Never 167.64 Imperial\n75  Female   15.6   15.8 Right  R on L    88    Left Some Never 165.00   Metric\n76  Female   17.5   17.5 Right Neither    68   Right Freq Heavy 170.00   Metric\n77  Female   17.0   17.6 Right  L on R    76   Right Some Never 165.00   Metric\n78  Female   18.6   18.0 Right  L on R    NA Neither Freq Heavy 165.10 Imperial\n79  Female   18.3   18.5 Right  R on L    68 Neither Some Never 165.10 Imperial\n80    Male   20.0   20.5 Right  L on R    NA   Right Freq Never 185.42 Imperial\n81    Male   19.5   19.5  Left  R on L    66    Left Some Never     NA     &lt;NA&gt;\n82    Male   19.2   18.9 Right  R on L    76   Right Freq Never 176.50 Imperial\n83  Female   17.5   17.5 Right  R on L    98    Left Freq Never     NA     &lt;NA&gt;\n84  Female   17.0   17.4 Right  R on L    NA Neither Some Never     NA     &lt;NA&gt;\n85    Male   23.0   23.5 Right  L on R    90   Right Freq Never 167.64 Imperial\n86  Female   17.7   17.0 Right  R on L    76   Right Some Never 167.00   Metric\n87  Female   18.2   18.0 Right  L on R    70   Right Some Never 162.56 Imperial\n88  Female   18.3   18.5 Right  R on L    75    Left Freq Never 170.00   Metric\n89    Male   18.0   18.0 Right Neither    60   Right Freq Never 179.00   Metric\n90  Female   18.0   17.7  Left  R on L    92    Left Some Never     NA     &lt;NA&gt;\n91    Male   20.5   20.0 Right  R on L    75    Left Some Never 183.00   Metric\n92  Female   17.5   18.0 Right Neither    NA   Right Some Never     NA     &lt;NA&gt;\n93  Female   18.2   17.5 Right  L on R    70   Right Some Never 165.00   Metric\n94  Female   18.2   18.5 Right  R on L    NA   Right Some Never 168.00   Metric\n95    Male   21.3   20.8 Right  R on L    65   Right Freq Heavy 179.00   Metric\n96  Female   19.0   18.8 Right  L on R    NA   Right Some Never     NA     &lt;NA&gt;\n97    Male   20.0   19.5 Right  R on L    68 Neither Freq Regul 190.00   Metric\n98  Female   17.5   17.5 Right  R on L    60   Right Freq Never 166.50   Metric\n99    Male   19.5   19.4 Right Neither    NA   Right Freq Never 165.00   Metric\n100 Female   19.4   19.6 Right  R on L    68 Neither Freq Never 175.26 Imperial\n101   Male   21.9   22.2 Right  R on L    NA   Right Some Never 187.00   Metric\n102   Male   18.9   19.1 Right  L on R    60 Neither None Never 170.00   Metric\n103 Female   16.0   16.0 Right Neither    NA   Right Some Never 159.00   Metric\n104 Female   17.5   17.3 Right  R on L    72   Right Freq Never 175.00   Metric\n105 Female   17.5   17.0 Right  R on L    80    Left Some Heavy 163.00   Metric\n106 Female   19.5   18.5 Right  R on L    80   Right Some Never 170.00   Metric\n107 Female   16.2   16.4 Right  R on L    NA   Right Freq Occas 172.00   Metric\n108 Female   17.0   15.9 Right  R on L    85   Right Freq Never     NA     &lt;NA&gt;\n109   Male   17.5   17.5 Right  L on R    64 Neither Freq Never 180.00   Metric\n110   Male   19.7   20.1 Right  R on L    67    Left Some Regul 180.34 Imperial\n111 Female   18.5   18.5 Right  R on L    76    Left Freq Never 175.00   Metric\n112   Male   19.2   19.6 Right  L on R    80   Right None Never 190.50 Imperial\n113 Female   17.2   16.7 Right  R on L    75   Right Freq Never 170.18 Imperial\n114   Male   20.5   21.0 Right  R on L    60   Right Freq Never 185.00   Metric\n115 Female   16.0   15.5 Right  L on R    60    Left Freq Never 162.56 Imperial\n116 Female   16.9   16.0 Right  L on R    70   Right None Never 158.00   Metric\n117 Female   17.0   16.7 Right  R on L    70   Right Some Never 159.00   Metric\n118   Male   23.0   22.0  Left  L on R    83    Left Some Heavy 193.04 Imperial\n119 Female   18.5   18.0  Left  L on R   100 Neither Some Never 171.00   Metric\n120   Male   21.0   20.4 Right  L on R   100   Right Freq Heavy 184.00   Metric\n121   Male   20.0   20.0 Right  R on L    80 Neither Freq Occas     NA     &lt;NA&gt;\n122   Male   22.5   22.5 Right  L on R    76   Right Freq Occas 177.00   Metric\n123 Female   18.5   18.0 Right  R on L    92   Right Freq Never 172.00   Metric\n124   Male   19.8   20.0  Left  L on R    59   Right Freq Never 180.00   Metric\n125   Male   18.5   18.1 Right  L on R    66    Left Freq Never 175.26 Imperial\n126   Male   19.3   19.4 Right  R on L    NA   Right Freq Never 180.34 Imperial\n127 Female   16.0   16.0 Right  R on L    68   Right Freq Never 172.72 Imperial\n128   Male   18.8   19.1 Right  L on R    66 Neither Freq Regul 178.50   Metric\n129 Female   17.5   17.0 Right  R on L    74   Right Freq Never 157.00   Metric\n130 Female   16.4   16.5 Right  L on R    90   Right Some Never 152.00   Metric\n131   Male   22.0   21.5 Right  R on L    86   Right Freq Never 187.96 Imperial\n132   Male   19.0   19.5 Right  L on R    60   Right Some Never 178.00   Metric\n133 Female   18.9   20.0 Right  R on L    86   Right Some Never     NA     &lt;NA&gt;\n134 Female   15.4   16.4  Left  L on R    80    Left Freq Occas 160.02 Imperial\n135   Male   17.9   17.8 Right  R on L    85    Left Some Never 175.26 Imperial\n136   Male   23.1   22.5 Right  L on R    90   Right Some Regul 189.00   Metric\n137   &lt;NA&gt;   19.8   19.0  Left  L on R    73 Neither Freq Never 172.00   Metric\n138   Male   22.0   22.0 Right  L on R    72   Right Freq Never 182.88 Imperial\n139   Male   20.0   19.5 Right  L on R    NA   Right Freq Never 170.00   Metric\n140 Female   19.5   18.5 Right  L on R    68   Right None Never 167.00   Metric\n141 Female   18.0   18.6 Right  R on L    84   Right Some Never 175.00   Metric\n142 Female   18.3   19.0 Right  R on L    NA   Right None Never 165.00   Metric\n143 Female   19.0   18.8 Right  R on L    65   Right Freq Never 172.72 Imperial\n144   Male   21.4   21.0 Right  L on R    96 Neither Some Never 180.00   Metric\n145 Female   20.0   19.5  Left  R on L    68 Neither Freq Never 172.00   Metric\n146   Male   18.5   18.5 Right  R on L    75 Neither Some Never 185.00   Metric\n147   Male   22.5   22.6 Right  L on R    64   Right Freq Regul 187.96 Imperial\n148   Male   19.5   20.2 Right  R on L    60 Neither Freq Never 185.42 Imperial\n149 Female   18.0   18.0 Right  L on R    92 Neither Freq Never 165.00   Metric\n150 Female   18.0   18.5 Right  R on L    64 Neither Freq Never 164.00   Metric\n151   Male   21.8   22.3 Right  R on L    76    Left Freq Never 195.00   Metric\n152 Female   13.0   12.5 Right  L on R    80   Right Freq Never 165.00   Metric\n153 Female   16.3   16.2 Right  L on R    92   Right Some Regul 152.40 Imperial\n154   Male   21.5   21.6 Right  R on L    69   Right Freq Never 172.72 Imperial\n155   Male   18.9   19.1 Right  L on R    68   Right None Never 180.34 Imperial\n156   Male   20.5   20.0 Right  R on L    76   Right Freq Never 173.00   Metric\n157   Male   14.0   15.5 Right  L on R    NA Neither Freq Heavy     NA     &lt;NA&gt;\n158 Female   18.9   19.2 Right  L on R    74   Right Some Never 167.64 Imperial\n159   Male   20.0   20.5 Right  R on L    NA   Right None Never 187.96 Imperial\n160   Male   18.5   19.0 Right  L on R    84   Right Freq Regul 187.00   Metric\n161 Female   17.5   17.1 Right  R on L    80    Left None Never 167.00   Metric\n162   Male   18.1   18.2  Left Neither    NA   Right Some Never 168.00   Metric\n163   Male   20.2   20.3 Right  L on R    72 Neither Some Never 191.80 Imperial\n164 Female   16.5   16.9 Right  R on L    60 Neither Freq Occas 169.20   Metric\n165   Male   19.1   19.1 Right Neither    NA   Right Some Never 177.00   Metric\n166 Female   17.6   17.2 Right  R on L    81    Left Some Never 168.00   Metric\n167 Female   19.5   19.2 Right  R on L    70   Right Some Never 170.00   Metric\n168 Female   16.5   15.0 Right  L on R    65   Right Some Regul 160.02 Imperial\n169   Male   19.0   18.5 Right  L on R    NA Neither Freq Never 189.00   Metric\n170   Male   19.0   18.5 Right  R on L    72   Right Freq Never 180.34 Imperial\n171 Female   16.5   17.0 Right  L on R    NA   Right Some Never 168.00   Metric\n172   Male   20.5   19.5  Left  L on R    80   Right Some Occas 182.88 Imperial\n173 Female   15.5   15.5 Right Neither    50   Right Some Regul     NA     &lt;NA&gt;\n174 Female   18.0   17.5 Right  R on L    48 Neither Freq Never 165.00   Metric\n175 Female   17.5   18.0 Right  R on L    68 Neither Freq Never 157.48 Imperial\n176 Female   19.0   18.5  Left  L on R   104    Left Freq Never 170.00   Metric\n177   Male   20.5   20.5 Right Neither    76   Right Freq Regul 172.72 Imperial\n178 Female   16.7   17.0 Right  L on R    84    Left Freq Never 164.00   Metric\n179 Female   20.5   20.5 Right  R on L    NA    Left Freq Regul     NA     &lt;NA&gt;\n180 Female   17.0   16.5 Right  R on L    70   Right Some Never 162.56 Imperial\n181   Male   19.0   19.5 Right  R on L    68   Right Freq Occas 172.00   Metric\n182 Female   14.0   13.5 Right  R on L    87 Neither Freq Occas 165.10 Imperial\n183 Female   17.5   17.6 Right  L on R    79   Right Some Never 162.50   Metric\n184   Male   18.5   19.0 Right  L on R    70    Left Freq Never 170.00   Metric\n185   Male   18.0   18.5 Right Neither    90   Right Some Never 175.00   Metric\n186   Male   20.5   20.7 Right  R on L    72   Right Some Never 168.00   Metric\n187 Female   17.0   17.0 Right  L on R    79   Right Some Never 163.00   Metric\n188   Male   18.5   18.5 Right  R on L    65   Right None Never 165.00   Metric\n189   Male   18.0   18.5 Right  R on L    62   Right Freq Never 173.00   Metric\n190   Male   18.5   18.0 Right Neither    63 Neither Freq Never 196.00   Metric\n191   Male   20.0   19.5 Right  R on L    92   Right Some Never 179.10 Imperial\n192   Male   22.0   22.5 Right  L on R    60   Right Some Never 180.00   Metric\n193   Male   17.9   18.4 Right  R on L    68    Left None Occas 176.00   Metric\n194 Female   17.6   17.8 Right  L on R    72    Left Some Never 160.02 Imperial\n195 Female   16.7   15.1 Right Neither    NA   Right None Never 157.48 Imperial\n196 Female   17.0   17.6 Right  L on R    76   Right Some Never 165.00   Metric\n197 Female   15.0   13.0 Right  R on L    80 Neither Freq Never 170.18 Imperial\n198   Male   16.0   15.5 Right Neither    71   Right Freq Never 154.94 Imperial\n199 Female   19.1   19.0 Right  R on L    80   Right Some Occas 170.00   Metric\n200 Female   17.5   16.5 Right  R on L    80 Neither Some Never 164.00   Metric\n201 Female   16.2   15.8 Right  R on L    61   Right Some Occas 167.00   Metric\n202   Male   21.0   21.0 Right  L on R    48 Neither Freq Never 174.00   Metric\n203 Female   18.8   17.8 Right  R on L    76   Right Some Never     NA     &lt;NA&gt;\n204 Female   18.5   18.0 Right Neither    86   Right None Never 160.00   Metric\n205   Male   17.0   17.5 Right  R on L    80   Right Some Regul 179.10   Metric\n206 Female   17.5   17.0 Right  R on L    83 Neither Freq Occas 168.00   Metric\n207 Female   17.5   17.6 Right  L on R    76   Right Some Never 153.50   Metric\n208   Male   17.5   17.6 Right  R on L    84   Right Some Never 160.00   Metric\n209   Male   17.5   17.0  Left  L on R    97 Neither None Never 165.00   Metric\n210 Female   20.8   20.7 Right  R on L    NA Neither Freq Never 171.50   Metric\n211 Female   18.6   18.6 Right  L on R    74   Right Some Never 160.00   Metric\n212 Female   17.5   17.5  Left  R on L    83 Neither Some Never 163.00   Metric\n213   Male   18.0   18.5 Right  R on L    78   Right Freq Never     NA     &lt;NA&gt;\n214   Male   17.0   17.5 Right  R on L    65   Right Some Never 165.00   Metric\n215 Female   18.0   17.8 Right  L on R    68   Right Some Never 168.90 Imperial\n216   Male   19.5   20.0 Right Neither    NA   Right Some Never 170.00   Metric\n217 Female   16.3   16.2 Right  L on R    NA   Right None Never     NA     &lt;NA&gt;\n218   Male   18.2   19.8 Right  R on L    88   Right Freq Never 185.00   Metric\n219 Female   17.0   17.3 Right  L on R    NA Neither Freq Never 173.00   Metric\n220   Male   23.2   23.2 Right  L on R    75   Right Freq Never 188.00   Metric\n221   Male   23.2   23.3 Right  L on R    NA   Right None Heavy 171.00   Metric\n222 Female   15.9   16.5 Right  R on L    70   Right Freq Never 167.64 Imperial\n223 Female   17.5   18.4 Right  R on L    88   Right Some Never 162.56 Imperial\n224 Female   17.5   17.6 Right  L on R    NA   Right Freq Never 150.00   Metric\n225 Female   17.6   17.2 Right  L on R    NA   Right Some Never     NA     &lt;NA&gt;\n226 Female   17.5   17.8 Right  R on L    96   Right Some Never     NA     &lt;NA&gt;\n227 Female   18.8   18.3 Right  R on L    80   Right Some Heavy 170.18 Imperial\n228   Male   20.0   19.8 Right  L on R    68   Right Freq Never 185.00   Metric\n229 Female   18.6   18.8 Right  L on R    70   Right Freq Regul 167.00   Metric\n230   Male   18.6   19.6 Right  L on R    71   Right Freq Occas 185.00   Metric\n231 Female   18.8   18.5 Right  R on L    80   Right Some Never 169.00   Metric\n232   Male   18.0   16.0 Right  R on L    NA   Right Some Never 180.34 Imperial\n233 Female   18.0   18.0 Right  L on R    85   Right Some Never 165.10 Imperial\n234 Female   18.5   18.0 Right  L on R    88   Right Some Never 160.00   Metric\n235 Female   17.5   16.5 Right  R on L    NA   Right Some Never 170.00   Metric\n236   Male   21.0   21.5 Right  R on L    90   Right Some Never 183.00   Metric\n237 Female   17.6   17.3 Right  R on L    85   Right Freq Never 168.50   Metric\n       Age\n1   18.250\n2   17.583\n3   16.917\n4   20.333\n5   23.667\n6   21.000\n7   18.833\n8   35.833\n9   19.000\n10  22.333\n11  28.500\n12  18.250\n13  18.750\n14  17.500\n15  17.167\n16  17.167\n17  19.333\n18  18.333\n19  19.750\n20  17.917\n21  17.917\n22  18.167\n23  17.833\n24  18.250\n25  19.167\n26  17.583\n27  17.500\n28  18.083\n29  21.917\n30  19.250\n31  41.583\n32  17.500\n33  39.750\n34  17.167\n35  17.750\n36  18.000\n37  19.000\n38  17.917\n39  35.500\n40  19.917\n41  17.500\n42  17.083\n43  28.583\n44  17.500\n45  17.417\n46  18.500\n47  18.917\n48  19.417\n49  18.417\n50  30.750\n51  18.500\n52  17.500\n53  18.333\n54  17.417\n55  20.000\n56  18.333\n57  17.167\n58  17.417\n59  17.667\n60  18.417\n61  20.333\n62  17.333\n63  17.500\n64  19.833\n65  18.583\n66  18.000\n67  30.667\n68  16.917\n69  19.917\n70  18.333\n71  17.583\n72  17.833\n73  17.667\n74  17.417\n75  17.750\n76  20.667\n77  23.583\n78  17.167\n79  17.083\n80  18.750\n81  16.750\n82  20.167\n83  17.667\n84  17.167\n85  17.167\n86  17.250\n87  18.000\n88  18.750\n89  21.583\n90  17.583\n91  19.667\n92  18.000\n93  19.667\n94  17.083\n95  22.833\n96  17.083\n97  19.417\n98  23.250\n99  18.083\n100 19.083\n101 18.917\n102 17.750\n103 20.833\n104 20.167\n105 17.667\n106 18.250\n107 17.000\n108 18.500\n109 18.583\n110 17.750\n111 24.167\n112 18.167\n113 21.167\n114 17.917\n115 17.417\n116 20.500\n117 22.917\n118 18.917\n119 18.917\n120 20.083\n121 17.500\n122 18.250\n123 17.500\n124 17.417\n125 21.000\n126 19.833\n127 17.667\n128 18.083\n129 18.000\n130 18.333\n131 20.000\n132 18.750\n133 19.083\n134 18.500\n135 18.417\n136 19.167\n137 21.500\n138 19.333\n139 21.417\n140 18.667\n141 17.500\n142 21.083\n143 17.250\n144 19.000\n145 19.167\n146 19.000\n147 23.000\n148 32.667\n149 20.000\n150 20.167\n151 25.500\n152 18.167\n153 23.500\n154 70.417\n155 43.833\n156 23.583\n157 21.083\n158 44.250\n159 19.667\n160 17.917\n161 18.417\n162 21.167\n163 17.500\n164 29.083\n165 19.917\n166 18.500\n167 18.167\n168 32.750\n169 17.417\n170 17.333\n171 73.000\n172 18.667\n173 18.500\n174 18.667\n175 17.750\n176 17.250\n177 36.583\n178 23.083\n179 19.250\n180 17.167\n181 23.417\n182 17.083\n183 17.250\n184 23.833\n185 18.750\n186 21.167\n187 24.667\n188 18.500\n189 20.333\n190 20.083\n191 18.917\n192 27.333\n193 18.917\n194 17.250\n195 18.167\n196 26.500\n197 17.000\n198 17.167\n199 19.167\n200 17.500\n201 19.250\n202 21.333\n203 18.583\n204 20.167\n205 18.667\n206 17.083\n207 17.417\n208 18.583\n209 19.500\n210 18.500\n211 17.167\n212 17.250\n213 17.500\n214 20.417\n215 17.083\n216 21.250\n217 19.250\n218 19.333\n219 19.167\n220 18.917\n221 20.917\n222 17.333\n223 18.167\n224 20.750\n225 19.917\n226 18.667\n227 18.417\n228 17.417\n229 20.333\n230 19.333\n231 18.167\n232 20.750\n233 17.667\n234 16.917\n235 18.583\n236 17.167\n237 17.750\n\nsurvey &lt;- as_tibble(survey)\n\n# check the structure of the data\n\nstr(survey) \n\ntibble [237 × 12] (S3: tbl_df/tbl/data.frame)\n $ Sex   : Factor w/ 2 levels \"Female\",\"Male\": 1 2 2 2 2 1 2 1 2 2 ...\n $ Wr.Hnd: num [1:237] 18.5 19.5 18 18.8 20 18 17.7 17 20 18.5 ...\n $ NW.Hnd: num [1:237] 18 20.5 13.3 18.9 20 17.7 17.7 17.3 19.5 18.5 ...\n $ W.Hnd : Factor w/ 2 levels \"Left\",\"Right\": 2 1 2 2 2 2 2 2 2 2 ...\n $ Fold  : Factor w/ 3 levels \"L on R\",\"Neither\",..: 3 3 1 3 2 1 1 3 3 3 ...\n $ Pulse : int [1:237] 92 104 87 NA 35 64 83 74 72 90 ...\n $ Clap  : Factor w/ 3 levels \"Left\",\"Neither\",..: 1 1 2 2 3 3 3 3 3 3 ...\n $ Exer  : Factor w/ 3 levels \"Freq\",\"None\",..: 3 2 2 2 3 3 1 1 3 3 ...\n $ Smoke : Factor w/ 4 levels \"Heavy\",\"Never\",..: 2 4 3 2 2 2 2 2 2 2 ...\n $ Height: num [1:237] 173 178 NA 160 165 ...\n $ M.I   : Factor w/ 2 levels \"Imperial\",\"Metric\": 2 1 NA 2 2 1 1 2 2 2 ...\n $ Age   : num [1:237] 18.2 17.6 16.9 20.3 23.7 ...\n\npairs(survey)\n\n\n\n\n\n\n\n# subset the data\n\nsurvey %&gt;% \n  select(Wr.Hnd, NW.Hnd, Pulse, Height, Age) -&gt; df1 \ndf1 \n\n# A tibble: 237 × 5\n   Wr.Hnd NW.Hnd Pulse Height   Age\n    &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1   18.5   18      92   173   18.2\n 2   19.5   20.5   104   178.  17.6\n 3   18     13.3    87    NA   16.9\n 4   18.8   18.9    NA   160   20.3\n 5   20     20      35   165   23.7\n 6   18     17.7    64   173.  21  \n 7   17.7   17.7    83   183.  18.8\n 8   17     17.3    74   157   35.8\n 9   20     19.5    72   175   19  \n10   18.5   18.5    90   167   22.3\n# ℹ 227 more rows\n\npairs(df1)\n\n\n\n\n\n\n\n# build our model with one indicator\n\nmlm1 &lt;- lm(cbind(df1$Height, df1$Pulse) ~ df1$Age) \n\nmlm1 &lt;- lm(cbind(Height, Pulse) ~ Age, data = df1) \n\nsummary(mlm1)\n\nResponse Height :\n\nCall:\nlm(formula = Height ~ Age, data = df1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-20.608  -7.528  -1.583   7.379  27.399 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 173.39245    2.66936  64.957   &lt;2e-16 ***\nAge          -0.04278    0.12503  -0.342    0.733    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.916 on 169 degrees of freedom\n  (66 observations deleted due to missingness)\nMultiple R-squared:  0.0006923, Adjusted R-squared:  -0.005221 \nF-statistic: 0.1171 on 1 and 169 DF,  p-value: 0.7327\n\n\nResponse Pulse :\n\nCall:\nlm(formula = Pulse ~ Age, data = df1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-38.130  -7.160  -0.721   6.360  29.381 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  78.9229     3.0761  25.657   &lt;2e-16 ***\nAge          -0.2448     0.1441  -1.699   0.0912 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.43 on 169 degrees of freedom\n  (66 observations deleted due to missingness)\nMultiple R-squared:  0.01679,   Adjusted R-squared:  0.01097 \nF-statistic: 2.886 on 1 and 169 DF,  p-value: 0.09118\n\n# build our model with more than one indicator\n\nmlm2 &lt;- lm(cbind(Height, Pulse) ~ Age + Wr.Hnd + NW.Hnd, data = df1) \n\nsummary(mlm2)\n\nResponse Height :\n\nCall:\nlm(formula = Height ~ Age + Wr.Hnd + NW.Hnd, data = df1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-20.5028  -4.9668  -0.9197   4.3439  25.6729 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 115.8383     5.9953  19.322   &lt;2e-16 ***\nAge          -0.1341     0.1004  -1.337   0.1832    \nWr.Hnd        2.7889     1.2009   2.322   0.0214 *  \nNW.Hnd        0.3776     1.1727   0.322   0.7478    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.823 on 166 degrees of freedom\n  (67 observations deleted due to missingness)\nMultiple R-squared:  0.389, Adjusted R-squared:  0.378 \nF-statistic: 35.23 on 3 and 166 DF,  p-value: &lt; 2.2e-16\n\n\nResponse Pulse :\n\nCall:\nlm(formula = Pulse ~ Age + Wr.Hnd + NW.Hnd, data = df1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-38.244  -6.902  -0.928   6.238  29.893 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  78.3243     8.8045   8.896 9.54e-16 ***\nAge          -0.2242     0.1474  -1.521    0.130    \nWr.Hnd        0.5060     1.7636   0.287    0.775    \nNW.Hnd       -0.4947     1.7221  -0.287    0.774    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.49 on 166 degrees of freedom\n  (67 observations deleted due to missingness)\nMultiple R-squared:  0.01518,   Adjusted R-squared:  -0.002614 \nF-statistic: 0.8531 on 3 and 166 DF,  p-value: 0.4668\n\nhead(resid(mlm1)) # residuals \n\n       Height      Pulse\n1   0.3883076  17.544343\n2   5.1597724  29.381073\n5  -7.3799460 -38.129668\n6   0.2259562  -9.782504\n7  10.2932491   8.687051\n8 -14.8594685   3.848360\n\nhead(fitted(mlm1)) # estimates fitted for the model\n\n    Height    Pulse\n1 172.6117 74.45566\n2 172.6402 74.61893\n5 172.3799 73.12967\n6 172.4940 73.78250\n7 172.5868 74.31295\n8 171.8595 70.15164\n\nhead(resid(mlm2)) # residuals \n\n      Height      Pulse\n1   1.217489  17.311649\n2   2.195023  29.893015\n5 -10.994527 -38.243539\n6   2.814107  -9.967349\n7  13.520106   8.698683\n8  -7.976292   3.665699\n\nhead(fitted(mlm2)) # estimates fitted for the model\n\n    Height    Pulse\n1 171.7825 74.68835\n2 175.6050 74.10699\n5 175.9945 73.24354\n6 169.9059 73.96735\n7 169.3599 74.30132\n8 164.9763 70.33430\n\n# gather coefficients\n\ncoef(mlm2)\n\n                 Height      Pulse\n(Intercept) 115.8382861 78.3242718\nAge          -0.1341361 -0.2241610\nWr.Hnd        2.7889054  0.5059615\nNW.Hnd        0.3776366 -0.4947372\n\n# variance-covariance matrix\n\nvcov(mlm2)\n\n                   Height:(Intercept)   Height:Age Height:Wr.Hnd Height:NW.Hnd\nHeight:(Intercept)        35.94337245 -0.154686949  -1.874139393   0.147649352\nHeight:Age                -0.15468695  0.010072486   0.012318156  -0.015095498\nHeight:Wr.Hnd             -1.87413939  0.012318156   1.442122756  -1.361112632\nHeight:NW.Hnd              0.14764935 -0.015095498  -1.361112632   1.375140647\nPulse:(Intercept)         -6.06537128  0.026103109   0.316257226  -0.024915529\nPulse:Age                  0.02610311 -0.001699712  -0.002078664   0.002547335\nPulse:Wr.Hnd               0.31625723 -0.002078664  -0.243355293   0.229684999\nPulse:NW.Hnd              -0.02491553  0.002547335   0.229684999  -0.232052198\n                   Pulse:(Intercept)    Pulse:Age Pulse:Wr.Hnd Pulse:NW.Hnd\nHeight:(Intercept)       -6.06537128  0.026103109  0.316257226 -0.024915529\nHeight:Age                0.02610311 -0.001699712 -0.002078664  0.002547335\nHeight:Wr.Hnd             0.31625723 -0.002078664 -0.243355293  0.229684999\nHeight:NW.Hnd            -0.02491553  0.002547335  0.229684999 -0.232052198\nPulse:(Intercept)        77.51891942 -0.333612690 -4.041948507  0.318434733\nPulse:Age                -0.33361269  0.021723288  0.026566515 -0.032556397\nPulse:Wr.Hnd             -4.04194851  0.026566515  3.110220051 -2.935505860\nPulse:NW.Hnd              0.31843473 -0.032556397 -2.935505860  2.965760021",
    "crumbs": [
      "Weekly materials",
      "Week 14"
    ]
  },
  {
    "objectID": "weeks/week14.html#footnotes",
    "href": "weeks/week14.html#footnotes",
    "title": "DATA 202 - Week 14",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMore information about the source of this code and the robust regression analysis can be found here.↩︎",
    "crumbs": [
      "Weekly materials",
      "Week 14"
    ]
  },
  {
    "objectID": "weeks/week14-slides.html#part-i-context",
    "href": "weeks/week14-slides.html#part-i-context",
    "title": "DATA 202 - Week 14",
    "section": "Part I: Context",
    "text": "Part I: Context\nNow that we have many foundational elements identified and practiced - such as generating code to explore data, cleaning data for analysis, and some elements of theory construction - we can begin focusing on some of the important technical components of model building and analysis: interpretation.\n\nInterpretation relies very heavily on both your research question and the subsequent empirical study.\nWhile your research question may be based on a host of factors, your empirical study relies on a combination of:\n\nTheoretical frameworks\nAnalytic method\nInterpretations"
  },
  {
    "objectID": "weeks/week14-slides.html#part-ii-content",
    "href": "weeks/week14-slides.html#part-ii-content",
    "title": "DATA 202 - Week 14",
    "section": "Part II: Content",
    "text": "Part II: Content\nMultiple Variable Analysis and Multivariate Analysis are two terms often used in statistics and research methodology to describe different approaches to analyzing data involving multiple variables. While they share similarities, there are distinct differences between these two concepts."
  },
  {
    "objectID": "weeks/week14-slides.html#part-iii-code",
    "href": "weeks/week14-slides.html#part-iii-code",
    "title": "DATA 202 - Week 14",
    "section": "Part III: Code",
    "text": "Part III: Code\nThis week, we use some standard data included in R to further discuss model interpretation.\nWhile these data sets do not directly connect to the content of our course, they provide some useful examples to return to as they are discussed on many websites that use R and that can be found in online forums.\nEach example illustrates different scenarios for interpreting linear models using the summary output. Remember to consider coefficients, standard errors, t-values, and p-values to assess the significance and direction of relationships between predictors and the response variable. Additionally, theory construction and relevant knowledge and context are crucial for a comprehensive interpretation of the results."
  },
  {
    "objectID": "weeks/week04.html",
    "href": "weeks/week04.html",
    "title": "DATA 202 - Week 4",
    "section": "",
    "text": "Last week, we considered one cyclical process of theory construction.\n\n\n\nTheory construction. Image from saylordotorg.github.io\n\n\n\nThis week, we return to this idea of theory construction but with more detail.\n\n\nResearch inquiry\nHypotheses\nAnalysis\nEvaluation\nRevision\n\n\n\n\n\nResearch inquiry\n– Transfer your reading and informal observations into pratical problems\nHypotheses\n– Utilize the research literature to understand your research problem\n– Extend this research literature by developing your own research questions\n– Based on your research question(s), develop a hypothesis (or a set of hypotheses)\nAnalysis\n– Outline your empirical study in specific details to conduct your analysis\n– Conduct your analysis and  consider the limitations of your methods \nEvaluation\n– Evaluate your analytic findings alongside your hypotheses\nRevision\n– Utilize the reseaerch literature to revise or confirm your hypotheses\n\n\n\n\n\nAcross research methodologies, a limitation is any feature of a study that may cause concerns. Limitations vary in both their scope and context. As a result, it is important to consider both the obvious limitations and hidden concerns.\n\nSome examples of limitations in statistics\n\n\n- Lack of reliable data\n\n- Limited sample size\n\n- Deficiencies in measurements of data\n\n\nNot limitations but “bad” statistical practices\n\n\n- Theory does not depict the entire story or phenomenon\n\n- Old data (and research citations)\n\n- Broad conclusions with no supporting data\n\n- Analyzing data for significant results. P-hacking!\n\n\n\n\n\nLogic is an important component in theory construction. Logical reasoning is the use of critical thinking in the applications of statistics, and relates to our forthcoming focus on probability. However, logic in relation to theory is based on a set of ideas that help build sound and consistent arguments that can be analyzed, measured, and support model construction and development.\nA few key considerations:\n\n\nPremises and conclusions\n– Clear statements are used; statements are focused and direct\nInternal structure\n– There is consistency in your premises and any conclusions\n– There are no contradictions in your structure\nArguments and inferences\n– Your argument follow your initial premises, internal structure, and conclusions\n\n\n\n\n\n\nOne way to visualize the relationships between your variables is in a path diagram.\n\nPath diagrams are used in path analysis, a subset of statistical methods that help researchers discern and assess the relationship(s) between multiple variables.\nPath analysis is based on a closed system of nested relationships.\n– These nested relationships must have a logical internal structure.\nTogether, a path diagram can represent a series of structured linear regression equations.\nPath models are often used in economics and political science.\n\nAs we move further into our analyses, we will learn more about path analysis.\n\nThis is a simple two-variable path diagram\n\n\n\n\n\nflowchart LR\n  A[X] --&gt; B[Y]\n\n\n\n\n\n\n\n\n\n\nIdentify two variables for a simple path diagram.\n\\(X\\) (independent variable) and \\(Y\\) (dependent variable)\n\nWhat is the logical relationship between the variables?\n– Write a logical statement.\nWhat is the question that structures the relationship between the variables?\n– Turn your logic statement into a question.\nWhat is your theory on how the two variables relate to one another?\n– Write a theoretical statement based on the research literature.\nWhat is your hypothesis?\n– Develop an educated guess based on the research literature.\n\n\nThree variable path diagram\n\n\n\n\n\nflowchart LR\n  A[Variable 1] --&gt; B[Y]\n  C[Variable 2] --&gt; B\n\n\n\n\n\n\nA path diagram made of multiple variables\n\n\n\n\n\nflowchart LR\n  A[Variable 1] --&gt; B[Y]\n  C[Variable 2] --&gt; B\n  D[Variable 3] --&gt; B",
    "crumbs": [
      "Weekly materials",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week04.html#more-on-theory-construction",
    "href": "weeks/week04.html#more-on-theory-construction",
    "title": "DATA 202 - Week 4",
    "section": "",
    "text": "Last week, we considered one cyclical process of theory construction.\n\n\n\nTheory construction. Image from saylordotorg.github.io\n\n\n\nThis week, we return to this idea of theory construction but with more detail.\n\n\nResearch inquiry\nHypotheses\nAnalysis\nEvaluation\nRevision\n\n\n\n\n\nResearch inquiry\n– Transfer your reading and informal observations into pratical problems\nHypotheses\n– Utilize the research literature to understand your research problem\n– Extend this research literature by developing your own research questions\n– Based on your research question(s), develop a hypothesis (or a set of hypotheses)\nAnalysis\n– Outline your empirical study in specific details to conduct your analysis\n– Conduct your analysis and  consider the limitations of your methods \nEvaluation\n– Evaluate your analytic findings alongside your hypotheses\nRevision\n– Utilize the reseaerch literature to revise or confirm your hypotheses\n\n\n\n\n\nAcross research methodologies, a limitation is any feature of a study that may cause concerns. Limitations vary in both their scope and context. As a result, it is important to consider both the obvious limitations and hidden concerns.\n\nSome examples of limitations in statistics\n\n\n- Lack of reliable data\n\n- Limited sample size\n\n- Deficiencies in measurements of data\n\n\nNot limitations but “bad” statistical practices\n\n\n- Theory does not depict the entire story or phenomenon\n\n- Old data (and research citations)\n\n- Broad conclusions with no supporting data\n\n- Analyzing data for significant results. P-hacking!\n\n\n\n\n\nLogic is an important component in theory construction. Logical reasoning is the use of critical thinking in the applications of statistics, and relates to our forthcoming focus on probability. However, logic in relation to theory is based on a set of ideas that help build sound and consistent arguments that can be analyzed, measured, and support model construction and development.\nA few key considerations:\n\n\nPremises and conclusions\n– Clear statements are used; statements are focused and direct\nInternal structure\n– There is consistency in your premises and any conclusions\n– There are no contradictions in your structure\nArguments and inferences\n– Your argument follow your initial premises, internal structure, and conclusions\n\n\n\n\n\n\nOne way to visualize the relationships between your variables is in a path diagram.\n\nPath diagrams are used in path analysis, a subset of statistical methods that help researchers discern and assess the relationship(s) between multiple variables.\nPath analysis is based on a closed system of nested relationships.\n– These nested relationships must have a logical internal structure.\nTogether, a path diagram can represent a series of structured linear regression equations.\nPath models are often used in economics and political science.\n\nAs we move further into our analyses, we will learn more about path analysis.\n\nThis is a simple two-variable path diagram\n\n\n\n\n\nflowchart LR\n  A[X] --&gt; B[Y]\n\n\n\n\n\n\n\n\n\n\nIdentify two variables for a simple path diagram.\n\\(X\\) (independent variable) and \\(Y\\) (dependent variable)\n\nWhat is the logical relationship between the variables?\n– Write a logical statement.\nWhat is the question that structures the relationship between the variables?\n– Turn your logic statement into a question.\nWhat is your theory on how the two variables relate to one another?\n– Write a theoretical statement based on the research literature.\nWhat is your hypothesis?\n– Develop an educated guess based on the research literature.\n\n\nThree variable path diagram\n\n\n\n\n\nflowchart LR\n  A[Variable 1] --&gt; B[Y]\n  C[Variable 2] --&gt; B\n\n\n\n\n\n\nA path diagram made of multiple variables\n\n\n\n\n\nflowchart LR\n  A[Variable 1] --&gt; B[Y]\n  C[Variable 2] --&gt; B\n  D[Variable 3] --&gt; B",
    "crumbs": [
      "Weekly materials",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week04.html#task-1-open-a-rmarkdown-file",
    "href": "weeks/week04.html#task-1-open-a-rmarkdown-file",
    "title": "DATA 202 - Week 4",
    "section": "Task 1: Open a RMarkdown file",
    "text": "Task 1: Open a RMarkdown file\nPrior to conducting any analyses in R, we want to include a brief summary of our work so that we (and others who read our code) know what we are prioritizing. This is an important step to documenting our research. We will write this preamble at the very top of our RMarkdown file.\nA preamble is simply an introduction to your code. The structure of a preamble can vary widely. In an RMarkdown file, we can use hashtags (#) to write certain sections in our preamble. The # symbol informs R that it should ignore that line in our code.\n---\ntitle: \"Lab No. #\"\nauthor: \n  - name: \"Insert Your Name\"\n    affiliation: \"DATA 202, Fall 2024, Howard University\"\ndate: \"2024-08-15\"\nalways_allow_html: true\noutput: \n  html_document:\n    toc: true\n    toc_depth: 2\n    number_sections: true\n    self_contained: yes\n    mode: selfcontained\n  pdf_document:\n    toc: true\n    toc_depth: 2\n    number_sections: true\ngeometry: margin=1.0in\n---\nLet’s head over to Lab 1 to continue.",
    "crumbs": [
      "Weekly materials",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week04-slides.html#more-on-theory-construction",
    "href": "weeks/week04-slides.html#more-on-theory-construction",
    "title": "DATA 202 - Week 4",
    "section": "More on theory construction",
    "text": "More on theory construction\nLast week, we considered one cyclical process of theory construction.\n\nTheory construction. Image from saylordotorg.github.io"
  },
  {
    "objectID": "weeks/week04-slides.html#task-1-open-a-rmarkdown-file",
    "href": "weeks/week04-slides.html#task-1-open-a-rmarkdown-file",
    "title": "DATA 202 - Week 4",
    "section": "Task 1: Open a RMarkdown file",
    "text": "Task 1: Open a RMarkdown file\nPrior to conducting any analyses in R, we want to include a brief summary of our work so that we (and others who read our code) know what we are prioritizing. This is an important step to documenting our research. We will write this preamble at the very top of our RMarkdown file.\nA preamble is simply an introduction to your code. The structure of a preamble can vary widely. In an RMarkdown file, we can use hashtags (#) to write certain sections in our preamble. The # symbol informs R that it should ignore that line in our code.\n---\ntitle: \"Lab No. #\"\nauthor: \n  - name: \"Insert Your Name\"\n    affiliation: \"DATA 202, Fall 2024, Howard University\"\ndate: \"2024-08-15\"\nalways_allow_html: true\noutput: \n  html_document:\n    toc: true\n    toc_depth: 2\n    number_sections: true\n    self_contained: yes\n    mode: selfcontained\n  pdf_document:\n    toc: true\n    toc_depth: 2\n    number_sections: true\ngeometry: margin=1.0in\n---\nLet’s head over to Lab 1 to continue.\n\n\n\n\nCourse Data GitHub"
  },
  {
    "objectID": "weeks/week11.html",
    "href": "weeks/week11.html",
    "title": "DATA 202 - Week 11",
    "section": "",
    "text": "Over the next several weeks, our goals will relate to the various ways we can integrate social and theoretical concepts (the context) into decisions around measurement (the content) and analysis (the code). Importantly, we will examine various historical contexts to make sense of what has been done. In doing so, we will make sense on how to build on theory using context.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) DuBois\n\n\n\n\n\n\n\n\n\n\n\n(b) Wells-Barnett\n\n\n\n\n\n\n\nFigure 1: W. E. B. Du Bois and Ida B. Wells-Barnett\n\n\n\n\nFollowing in line from our discussions of W.E.B. Du Bois and his infamous infographics, our next example will be based on the work of Ida B. Wells-Barnett (1862-1931), a late 19th century and early 20th century activist and research journalist.\nWhat do you know about Ida B. Wells-Barnett and her work in the late 1800s?\n\n\n\n\nA Red Record. Tabulated Statistics and Alleged Causes of Lynchings in the US.\n\n\nThe below summary of The Red Record is from the New York Public Library:\n\nThe investigative journalist and activist Ida B. Wells, later Wells-Barnett, spearheaded the anti-lynching movement in the United States. Expanding on her groundbreaking exposé Southern Horrors: Lynch Law in All Its Phases (1892), A Red Record used mainstream white newspapers to document a resurgence of white mob violence, finding that more than 10,000 African Americans had been killed by lynching in the South between 1864 and 1894. Wells compiled statistics on alleged offenses and the geographic distribution and extent of lynching, and tied whites’ increased brutality and violence to their fear of African Americans’ increased political power. Her conclusion exhorts anti-lynching advocates to “[t]ell the world the facts,” for “When the Christian world knows the alarming growth and extent of outlawry in our land, some means will be found to stop it.”\n\nThe Red Record is an important historical text worth reading. Further investigation of the data and content found in Wells-Barnett’s The Red Record will reveal a relationship by individual incidents of lynching and broader systemic issues of racism.\n\nIn connecting the contexts of the United States in the late 1800s to the modern issues of racism at the systems level, we can consider the conceptual link between lynching then and fatal police shootings now.\nSome important questions to consider:\n\nWhat terms need to be defined?\nWhat historical evidence may be needed to frame this conceptual link?\nWhat data and data sources might help to inform this conceptual link?\n\nConsider this article Shooting death of Ahmaud Arbery shows ‘culture of complicity’ akin to lynchings, says activist that connects the shooting of Ahmaud Arbery to lynching.\nConsider the song Strange Fruit by Billie Holiday.\n\n\n\nRecord Cover of Strange Fruit by Billie Holiday\n\n\n\nTo further explore the connections between theory, method, and analysis, we will explore the use of a more recent data set at the individual level in Section 4. The data we will use is the Fatal Force data set from the Washington Post.\nSince 2015, the Washington Post has collected a record of every fatal encounter with police in the United States.\nThis database is one resource that we’ll explore and connect to the Red Record.\nWhat are some theoretical conceptions between Ida B. Wells-Barnett’s Red Record and the data collected by the Washington Post? How might these relationships inform our analysis? What is a theoretical construction that can be developed in relation to these connections?",
    "crumbs": [
      "Weekly materials",
      "Week 11"
    ]
  },
  {
    "objectID": "weeks/week11.html#part-i-context",
    "href": "weeks/week11.html#part-i-context",
    "title": "DATA 202 - Week 11",
    "section": "",
    "text": "Over the next several weeks, our goals will relate to the various ways we can integrate social and theoretical concepts (the context) into decisions around measurement (the content) and analysis (the code). Importantly, we will examine various historical contexts to make sense of what has been done. In doing so, we will make sense on how to build on theory using context.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) DuBois\n\n\n\n\n\n\n\n\n\n\n\n(b) Wells-Barnett\n\n\n\n\n\n\n\nFigure 1: W. E. B. Du Bois and Ida B. Wells-Barnett\n\n\n\n\nFollowing in line from our discussions of W.E.B. Du Bois and his infamous infographics, our next example will be based on the work of Ida B. Wells-Barnett (1862-1931), a late 19th century and early 20th century activist and research journalist.\nWhat do you know about Ida B. Wells-Barnett and her work in the late 1800s?\n\n\n\n\nA Red Record. Tabulated Statistics and Alleged Causes of Lynchings in the US.\n\n\nThe below summary of The Red Record is from the New York Public Library:\n\nThe investigative journalist and activist Ida B. Wells, later Wells-Barnett, spearheaded the anti-lynching movement in the United States. Expanding on her groundbreaking exposé Southern Horrors: Lynch Law in All Its Phases (1892), A Red Record used mainstream white newspapers to document a resurgence of white mob violence, finding that more than 10,000 African Americans had been killed by lynching in the South between 1864 and 1894. Wells compiled statistics on alleged offenses and the geographic distribution and extent of lynching, and tied whites’ increased brutality and violence to their fear of African Americans’ increased political power. Her conclusion exhorts anti-lynching advocates to “[t]ell the world the facts,” for “When the Christian world knows the alarming growth and extent of outlawry in our land, some means will be found to stop it.”\n\nThe Red Record is an important historical text worth reading. Further investigation of the data and content found in Wells-Barnett’s The Red Record will reveal a relationship by individual incidents of lynching and broader systemic issues of racism.\n\nIn connecting the contexts of the United States in the late 1800s to the modern issues of racism at the systems level, we can consider the conceptual link between lynching then and fatal police shootings now.\nSome important questions to consider:\n\nWhat terms need to be defined?\nWhat historical evidence may be needed to frame this conceptual link?\nWhat data and data sources might help to inform this conceptual link?\n\nConsider this article Shooting death of Ahmaud Arbery shows ‘culture of complicity’ akin to lynchings, says activist that connects the shooting of Ahmaud Arbery to lynching.\nConsider the song Strange Fruit by Billie Holiday.\n\n\n\nRecord Cover of Strange Fruit by Billie Holiday\n\n\n\nTo further explore the connections between theory, method, and analysis, we will explore the use of a more recent data set at the individual level in Section 4. The data we will use is the Fatal Force data set from the Washington Post.\nSince 2015, the Washington Post has collected a record of every fatal encounter with police in the United States.\nThis database is one resource that we’ll explore and connect to the Red Record.\nWhat are some theoretical conceptions between Ida B. Wells-Barnett’s Red Record and the data collected by the Washington Post? How might these relationships inform our analysis? What is a theoretical construction that can be developed in relation to these connections?",
    "crumbs": [
      "Weekly materials",
      "Week 11"
    ]
  },
  {
    "objectID": "weeks/week11.html#sec-content",
    "href": "weeks/week11.html#sec-content",
    "title": "DATA 202 - Week 11",
    "section": "Part II: Content",
    "text": "Part II: Content\nThis week, we will discuss a series of data sources that can be used for your papers.\n\nGeneral Social Survey (GSS)\nThe main site for the GSS can be found here.\n\nProblem: There was no high quality, accessible data set documenting societal change in America.\n\n\nSolution: National Opinion Research Council (NORC) launched the General Social Survey (GSS)—our longest-running, most respected project.",
    "crumbs": [
      "Weekly materials",
      "Week 11"
    ]
  },
  {
    "objectID": "weeks/week11.html#regions-of-the-united-states-included-in-the-gss",
    "href": "weeks/week11.html#regions-of-the-united-states-included-in-the-gss",
    "title": "DATA 202 - Week 11",
    "section": "",
    "text": "Inter-university Consortium for Political and Social Research (ICPSR)\nThe main site for the ICPSR data can be found here.\n\nICPSR is research science data and resources on topics like social media, politics, economics, social sciences, government, GIS, & more. It was established in 1962. An integral part of the infrastructure of social science research, ICPSR maintains and provides access to a vast archive of social science data for research and instruction\n\n\n\n\nICPSR logo\n\n\n\n\n\nPew Research Center\nThe main site for the Pew Research Center can be found here.\n\nThe Pew Research Center (also simply known as Pew) is a nonpartisan American think tank based in Washington, D.C. It provides information on social issues, public opinion, and demographic trends shaping the United States and the world.\n\n\n\n\nPew Research Center logo\n\n\n\n\n\nAmerican National Election Studies (ANES)\nThe main site for the ANES can be found here.\n\nThe American National Election Studies are academically-run national surveys of voters in the United States, conducted before and after every presidential election.\n\n\n\n\nANES logo\n\n\n\n\n\nCensus data and resources linked directly to R/RStudio\n\ntidycensus Package\nIPUMS USA: “IPUMS provides census and survey data from around the world integrated across time and space. IPUMS integration and documentation makes it easy to study change, conduct comparative research, merge information across data types, and analyze individuals within family and community context. Data and services available free of charge.” (From IPUMS site)",
    "crumbs": [
      "Weekly materials",
      "Week 11"
    ]
  },
  {
    "objectID": "weeks/week11.html#sec-code",
    "href": "weeks/week11.html#sec-code",
    "title": "DATA 202 - Week 11",
    "section": "Part III: Code",
    "text": "Part III: Code\n\nWashington Post Fatal Force data\nThe main site for the Fatal Force data can be found here.\nThe Washington Post makes the fatal force data publicly accessible on their website and they provide a direct link to their GitHub repository (or repo). To access the data, we will pull it directly into R from Github.\n\nLoad the data\nWe will need to install packages and/or load libraries to import and explore the data.\n\n# install packages\ninstall.packages(\"tidyverse\", repos = \"http://cran.us.r-project.org\")\n\n# load libraries\nlibrary(tidyverse) # collection of essential packages for data science\nlibrary(dplyr) # the dplyr package makes data manipulation easier\n\nWe then load the data directly into R from GitHub.\n\nfatal &lt;- read.csv(\"https://raw.githubusercontent.com/washingtonpost/data-police-shootings/master/v2/fatal-police-shootings-data.csv\")\n\n\n\n\nUnderstand the data\nWe will first turn the file into a tibble using the as_tibble function.\nFrom there, we will use the glimpse(), names(), and str() functions to understand the data.\n\n# examine fatal force data\nfatal &lt;- as_tibble(fatal) # turn data into a tibble\n\nLet us take note of the variables and variable types.\n\nGet a glimpse of the data.\n\n# examine fatal force data\nglimpse(fatal)\n\nRows: 10,423\nColumns: 19\n$ id                         &lt;int&gt; 3, 4, 5, 8, 9, 11, 13, 15, 16, 17, 19, 21, …\n$ date                       &lt;chr&gt; \"2015-01-02\", \"2015-01-02\", \"2015-01-03\", \"…\n$ threat_type                &lt;chr&gt; \"point\", \"point\", \"move\", \"point\", \"point\",…\n$ flee_status                &lt;chr&gt; \"not\", \"not\", \"not\", \"not\", \"not\", \"not\", \"…\n$ armed_with                 &lt;chr&gt; \"gun\", \"gun\", \"unarmed\", \"replica\", \"other\"…\n$ city                       &lt;chr&gt; \"Shelton\", \"Aloha\", \"Wichita\", \"San Francis…\n$ county                     &lt;chr&gt; \"Mason\", \"Washington\", \"Sedgwick\", \"San Fra…\n$ state                      &lt;chr&gt; \"WA\", \"OR\", \"KS\", \"CA\", \"CO\", \"OK\", \"AZ\", \"…\n$ latitude                   &lt;dbl&gt; 47.24683, 45.48742, 37.69477, 37.76291, 40.…\n$ longitude                  &lt;dbl&gt; -123.12159, -122.89170, -97.28055, -122.422…\n$ location_precision         &lt;chr&gt; \"not_available\", \"not_available\", \"not_avai…\n$ name                       &lt;chr&gt; \"Tim Elliot\", \"Lewis Lee Lembke\", \"John Pau…\n$ age                        &lt;int&gt; 53, 47, 23, 32, 39, 18, 22, 35, 34, 47, 25,…\n$ gender                     &lt;chr&gt; \"male\", \"male\", \"male\", \"male\", \"male\", \"ma…\n$ race                       &lt;chr&gt; \"A\", \"W\", \"H\", \"W\", \"H\", \"W\", \"H\", \"W\", \"W\"…\n$ race_source                &lt;chr&gt; \"not_available\", \"not_available\", \"not_avai…\n$ was_mental_illness_related &lt;chr&gt; \"True\", \"False\", \"False\", \"True\", \"False\", …\n$ body_camera                &lt;chr&gt; \"False\", \"False\", \"False\", \"False\", \"False\"…\n$ agency_ids                 &lt;chr&gt; \"73\", \"70\", \"238\", \"196\", \"473\", \"101\", \"19…\n\n\n\nView the variable names.\n\n# examine fatal force data\nnames(fatal)\n\n [1] \"id\"                         \"date\"                      \n [3] \"threat_type\"                \"flee_status\"               \n [5] \"armed_with\"                 \"city\"                      \n [7] \"county\"                     \"state\"                     \n [9] \"latitude\"                   \"longitude\"                 \n[11] \"location_precision\"         \"name\"                      \n[13] \"age\"                        \"gender\"                    \n[15] \"race\"                       \"race_source\"               \n[17] \"was_mental_illness_related\" \"body_camera\"               \n[19] \"agency_ids\"                \n\n\n\nUnderstand the structure of the tibble.\n\n# examine fatal force data\nstr(fatal)\n\ntibble [10,423 × 19] (S3: tbl_df/tbl/data.frame)\n $ id                        : int [1:10423] 3 4 5 8 9 11 13 15 16 17 ...\n $ date                      : chr [1:10423] \"2015-01-02\" \"2015-01-02\" \"2015-01-03\" \"2015-01-04\" ...\n $ threat_type               : chr [1:10423] \"point\" \"point\" \"move\" \"point\" ...\n $ flee_status               : chr [1:10423] \"not\" \"not\" \"not\" \"not\" ...\n $ armed_with                : chr [1:10423] \"gun\" \"gun\" \"unarmed\" \"replica\" ...\n $ city                      : chr [1:10423] \"Shelton\" \"Aloha\" \"Wichita\" \"San Francisco\" ...\n $ county                    : chr [1:10423] \"Mason\" \"Washington\" \"Sedgwick\" \"San Francisco\" ...\n $ state                     : chr [1:10423] \"WA\" \"OR\" \"KS\" \"CA\" ...\n $ latitude                  : num [1:10423] 47.2 45.5 37.7 37.8 40.4 ...\n $ longitude                 : num [1:10423] -123.1 -122.9 -97.3 -122.4 -104.7 ...\n $ location_precision        : chr [1:10423] \"not_available\" \"not_available\" \"not_available\" \"not_available\" ...\n $ name                      : chr [1:10423] \"Tim Elliot\" \"Lewis Lee Lembke\" \"John Paul Quintero\" \"Matthew Hoffman\" ...\n $ age                       : int [1:10423] 53 47 23 32 39 18 22 35 34 47 ...\n $ gender                    : chr [1:10423] \"male\" \"male\" \"male\" \"male\" ...\n $ race                      : chr [1:10423] \"A\" \"W\" \"H\" \"W\" ...\n $ race_source               : chr [1:10423] \"not_available\" \"not_available\" \"not_available\" \"not_available\" ...\n $ was_mental_illness_related: chr [1:10423] \"True\" \"False\" \"False\" \"True\" ...\n $ body_camera               : chr [1:10423] \"False\" \"False\" \"False\" \"False\" ...\n $ agency_ids                : chr [1:10423] \"73\" \"70\" \"238\" \"196\" ...\n\n\n\nWe should also view the head() and tail() of the data.\n\nhead(fatal)\n\n# A tibble: 6 × 19\n     id date      threat_type flee_status armed_with city  county state latitude\n  &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;       &lt;chr&gt;       &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;\n1     3 2015-01-… point       not         gun        Shel… Mason  WA        47.2\n2     4 2015-01-… point       not         gun        Aloha Washi… OR        45.5\n3     5 2015-01-… move        not         unarmed    Wich… Sedgw… KS        37.7\n4     8 2015-01-… point       not         replica    San … San F… CA        37.8\n5     9 2015-01-… point       not         other      Evans Weld   CO        40.4\n6    11 2015-01-… attack      not         gun        Guth… Logan  OK        35.9\n# ℹ 10 more variables: longitude &lt;dbl&gt;, location_precision &lt;chr&gt;, name &lt;chr&gt;,\n#   age &lt;int&gt;, gender &lt;chr&gt;, race &lt;chr&gt;, race_source &lt;chr&gt;,\n#   was_mental_illness_related &lt;chr&gt;, body_camera &lt;chr&gt;, agency_ids &lt;chr&gt;\n\ntail(fatal)\n\n# A tibble: 6 × 19\n     id date      threat_type flee_status armed_with city  county state latitude\n  &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;       &lt;chr&gt;       &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;\n1 11277 2024-12-… point       \"not\"       gun        Kirb… Jasper TX        NA  \n2 11279 2024-12-… threat      \"\"          gun        Char… Meckl… NC        35.3\n3 11289 2024-12-… attack      \"car\"       undetermi… Lore… Crosby TX        NA  \n4 11271 2024-12-… undetermin… \"\"          undetermi… Belc… Rolet… ND        NA  \n5 11275 2024-12-… threat      \"\"          gun        Wood… Los A… CA        34.2\n6 11276 2024-12-… undetermin… \"\"          undetermi… Spok… Spoka… WA        47.7\n# ℹ 10 more variables: longitude &lt;dbl&gt;, location_precision &lt;chr&gt;, name &lt;chr&gt;,\n#   age &lt;int&gt;, gender &lt;chr&gt;, race &lt;chr&gt;, race_source &lt;chr&gt;,\n#   was_mental_illness_related &lt;chr&gt;, body_camera &lt;chr&gt;, agency_ids &lt;chr&gt;\n\n\nWhat do you notice? What do you wonder?\n\nThere are many ways to use R to understand and explore our data.\nWe will use a mixture of base R and tidyverse commands to clean up our data.\nWe can also use the R for Data Science, 2nd Edition text as a guide.\n\n\n\nR for Data Science, 2nd Edition\n\n\n\n\n\nCodebook\nPrior to beginning any cleaning and analysis, we need to know our data…\n…before loading our data, we should have become more familiar with the codebook.\nThe code book will allow us to understand how data was collected and input into the data set. This often includes the levels of measurement for each variable and other related details.\nThe codebook for the Fatal Force data can be found online. We will need to navigate the GitHub site.\n\nMain landing page: https://github.com/washingtonpost/data-police-shootings\nFatal Force Database (version 2): https://github.com/washingtonpost/data-police-shootings/tree/master/v2\n\nA codebook for the data we have uploaded can be found at the bottom of the database page.\n\nLet’s first remind ourselves of the variables and variable types.\n\nstr(fatal)\n\ntibble [10,423 × 19] (S3: tbl_df/tbl/data.frame)\n $ id                        : int [1:10423] 3 4 5 8 9 11 13 15 16 17 ...\n $ date                      : chr [1:10423] \"2015-01-02\" \"2015-01-02\" \"2015-01-03\" \"2015-01-04\" ...\n $ threat_type               : chr [1:10423] \"point\" \"point\" \"move\" \"point\" ...\n $ flee_status               : chr [1:10423] \"not\" \"not\" \"not\" \"not\" ...\n $ armed_with                : chr [1:10423] \"gun\" \"gun\" \"unarmed\" \"replica\" ...\n $ city                      : chr [1:10423] \"Shelton\" \"Aloha\" \"Wichita\" \"San Francisco\" ...\n $ county                    : chr [1:10423] \"Mason\" \"Washington\" \"Sedgwick\" \"San Francisco\" ...\n $ state                     : chr [1:10423] \"WA\" \"OR\" \"KS\" \"CA\" ...\n $ latitude                  : num [1:10423] 47.2 45.5 37.7 37.8 40.4 ...\n $ longitude                 : num [1:10423] -123.1 -122.9 -97.3 -122.4 -104.7 ...\n $ location_precision        : chr [1:10423] \"not_available\" \"not_available\" \"not_available\" \"not_available\" ...\n $ name                      : chr [1:10423] \"Tim Elliot\" \"Lewis Lee Lembke\" \"John Paul Quintero\" \"Matthew Hoffman\" ...\n $ age                       : int [1:10423] 53 47 23 32 39 18 22 35 34 47 ...\n $ gender                    : chr [1:10423] \"male\" \"male\" \"male\" \"male\" ...\n $ race                      : chr [1:10423] \"A\" \"W\" \"H\" \"W\" ...\n $ race_source               : chr [1:10423] \"not_available\" \"not_available\" \"not_available\" \"not_available\" ...\n $ was_mental_illness_related: chr [1:10423] \"True\" \"False\" \"False\" \"True\" ...\n $ body_camera               : chr [1:10423] \"False\" \"False\" \"False\" \"False\" ...\n $ agency_ids                : chr [1:10423] \"73\" \"70\" \"238\" \"196\" ...\n\n\nWhat do you notice? What do you wonder?\nThe use of # in line with your code is one valuable way to leave notes for yourself and any readers of your code. This also helps to support if others would like to reproduce your analysis.\nHere, you may want to take note of some of the issues you see that could cause potential issues later as you begin your analysis. For example, a few things noteworthy observations will inform some initial changes.\n\n\n\nClean up the data\nLet’s fix a few variables based on our previous observations and the codebook.\n\nThe date variable is listed as a character variable, however, we want to transform this into a more appropriate variable for the variable type. We can use the command as.date() to do so.\nThe age variable is listed as an integer, which is correct. However, we but want to change the variable type to numeric to conduct some specific analyses in the tidyverse.\nThe latitude and longitude variables are listed as numeric variables. However, they contain some different information where values such as the mean or other measures of center do not make sense.\nThe was_mental_illness_related variable is also listed as a character variable when, in fact, it should be a logical variable based on both the codebook and contents of the cells.\nThe body_camera variable is also listed as a character variable when it should be a logical variable given the contents of the cells.\n\n\n\n# fix vars\n\n  # change vars to appropriate formats\n  fatal$date &lt;- as.Date(fatal$date) # check/change to date format\n  fatal$age &lt;- as.numeric(fatal$age)\n  fatal$was_mental_illness_related &lt;- as.logical(fatal$was_mental_illness_related)\n  fatal$body_camera &lt;- as.logical(fatal$body_camera)\n  \n  # format to 20YY\n  fatal.year &lt;- format(fatal$date, format=\"20%y\") \n  fatal$year &lt;- fatal.year # add a year column to the df\n  fatal %&gt;% relocate(state, year) -&gt; fatal\n  fatal\n\n# A tibble: 10,423 × 20\n   state year     id date       threat_type flee_status armed_with city   county\n   &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;date&gt;     &lt;chr&gt;       &lt;chr&gt;       &lt;chr&gt;      &lt;chr&gt;  &lt;chr&gt; \n 1 WA    2015      3 2015-01-02 point       not         gun        Shelt… Mason \n 2 OR    2015      4 2015-01-02 point       not         gun        Aloha  Washi…\n 3 KS    2015      5 2015-01-03 move        not         unarmed    Wichi… Sedgw…\n 4 CA    2015      8 2015-01-04 point       not         replica    San F… San F…\n 5 CO    2015      9 2015-01-04 point       not         other      Evans  Weld  \n 6 OK    2015     11 2015-01-04 attack      not         gun        Guthr… Logan \n 7 AZ    2015     13 2015-01-05 shoot       car         gun        Chand… Maric…\n 8 KS    2015     15 2015-01-06 point       not         gun        Assar… Saline\n 9 IA    2015     16 2015-01-06 accident    not         unarmed    Burli… Des M…\n10 PA    2015     17 2015-01-06 point       not         replica    Knoxv… Alleg…\n# ℹ 10,413 more rows\n# ℹ 11 more variables: latitude &lt;dbl&gt;, longitude &lt;dbl&gt;,\n#   location_precision &lt;chr&gt;, name &lt;chr&gt;, age &lt;dbl&gt;, gender &lt;chr&gt;, race &lt;chr&gt;,\n#   race_source &lt;chr&gt;, was_mental_illness_related &lt;lgl&gt;, body_camera &lt;lgl&gt;,\n#   agency_ids &lt;chr&gt;\n\n\n\n\nAncillary explorations\nSometimes when analyzing data, you may want to gather quick information.\nFor example, let’s say I also want to get a count of fatal shootings by year.\n\n# get counts by year\nfatal %&gt;% count(year)\n\n# A tibble: 10 × 2\n   year      n\n   &lt;chr&gt; &lt;int&gt;\n 1 2015    995\n 2 2016    959\n 3 2017    984\n 4 2018    992\n 5 2019    994\n 6 2020   1021\n 7 2021   1050\n 8 2022   1097\n 9 2023   1164\n10 2024   1167\n\n\nI was able to do this when I added the year variable to the dataframe.\nWhat do you notice? What do you wonder?\n\n\n\n\nTransform the data\nWe should check the structure of our data again.\n\nstr(fatal)\n\ntibble [10,423 × 20] (S3: tbl_df/tbl/data.frame)\n $ state                     : chr [1:10423] \"WA\" \"OR\" \"KS\" \"CA\" ...\n $ year                      : chr [1:10423] \"2015\" \"2015\" \"2015\" \"2015\" ...\n $ id                        : int [1:10423] 3 4 5 8 9 11 13 15 16 17 ...\n $ date                      : Date[1:10423], format: \"2015-01-02\" \"2015-01-02\" ...\n $ threat_type               : chr [1:10423] \"point\" \"point\" \"move\" \"point\" ...\n $ flee_status               : chr [1:10423] \"not\" \"not\" \"not\" \"not\" ...\n $ armed_with                : chr [1:10423] \"gun\" \"gun\" \"unarmed\" \"replica\" ...\n $ city                      : chr [1:10423] \"Shelton\" \"Aloha\" \"Wichita\" \"San Francisco\" ...\n $ county                    : chr [1:10423] \"Mason\" \"Washington\" \"Sedgwick\" \"San Francisco\" ...\n $ latitude                  : num [1:10423] 47.2 45.5 37.7 37.8 40.4 ...\n $ longitude                 : num [1:10423] -123.1 -122.9 -97.3 -122.4 -104.7 ...\n $ location_precision        : chr [1:10423] \"not_available\" \"not_available\" \"not_available\" \"not_available\" ...\n $ name                      : chr [1:10423] \"Tim Elliot\" \"Lewis Lee Lembke\" \"John Paul Quintero\" \"Matthew Hoffman\" ...\n $ age                       : num [1:10423] 53 47 23 32 39 18 22 35 34 47 ...\n $ gender                    : chr [1:10423] \"male\" \"male\" \"male\" \"male\" ...\n $ race                      : chr [1:10423] \"A\" \"W\" \"H\" \"W\" ...\n $ race_source               : chr [1:10423] \"not_available\" \"not_available\" \"not_available\" \"not_available\" ...\n $ was_mental_illness_related: logi [1:10423] TRUE FALSE FALSE TRUE FALSE FALSE ...\n $ body_camera               : logi [1:10423] FALSE FALSE FALSE FALSE FALSE FALSE ...\n $ agency_ids                : chr [1:10423] \"73\" \"70\" \"238\" \"196\" ...\n\n\nWhile our initial changes are noted, observe the chr (character) variable type.\n\nWe may want to change these into fct (factor) variables types.\n\nfatal_factor &lt;- fatal %&gt;% \n  mutate_if(is.character, as.factor)\nstr(fatal_factor)\n\ntibble [10,423 × 20] (S3: tbl_df/tbl/data.frame)\n $ state                     : Factor w/ 51 levels \"AK\",\"AL\",\"AR\",..: 48 38 17 5 6 37 4 17 13 39 ...\n $ year                      : Factor w/ 10 levels \"2015\",\"2016\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ id                        : int [1:10423] 3 4 5 8 9 11 13 15 16 17 ...\n $ date                      : Date[1:10423], format: \"2015-01-02\" \"2015-01-02\" ...\n $ threat_type               : Factor w/ 9 levels \"\",\"accident\",..: 6 6 5 6 6 3 7 6 2 6 ...\n $ flee_status               : Factor w/ 5 levels \"\",\"car\",\"foot\",..: 4 4 4 4 4 4 2 4 4 4 ...\n $ armed_with                : Factor w/ 31 levels \"\",\"blunt_object\",..: 7 7 25 21 17 7 7 7 25 21 ...\n $ city                      : Factor w/ 3749 levels \"\",\"Abbeville\",..: 3074 51 3656 2966 1055 1373 561 135 429 1721 ...\n $ county                    : Factor w/ 988 levels \"\",\"Acadia\",\"Ada\",..: 555 941 803 782 951 521 548 777 255 20 ...\n $ latitude                  : num [1:10423] 47.2 45.5 37.7 37.8 40.4 ...\n $ longitude                 : num [1:10423] -123.1 -122.9 -97.3 -122.4 -104.7 ...\n $ location_precision        : Factor w/ 8 levels \"\",\"address\",\"block\",..: 5 5 5 5 5 5 5 5 5 5 ...\n $ name                      : Factor w/ 10046 levels \"\",\"A.B. Carr\",..: 9305 6205 4933 6711 7057 5788 5762 1161 778 6193 ...\n $ age                       : num [1:10423] 53 47 23 32 39 18 22 35 34 47 ...\n $ gender                    : Factor w/ 4 levels \"\",\"female\",\"male\",..: 3 3 3 3 3 3 3 3 2 3 ...\n $ race                      : Factor w/ 13 levels \"\",\"A\",\"B\",\"B;H\",..: 2 9 5 9 5 9 5 9 9 3 ...\n $ race_source               : Factor w/ 7 levels \"\",\"clip\",\"not_available\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ was_mental_illness_related: logi [1:10423] TRUE FALSE FALSE TRUE FALSE FALSE ...\n $ body_camera               : logi [1:10423] FALSE FALSE FALSE FALSE FALSE FALSE ...\n $ agency_ids                : Factor w/ 3866 levels \"\",\"1\",\"100\",\"100;3031\",..: 3550 3514 1745 1211 3225 20 1200 3253 2360 2062 ...\n\n\nTake note of the changes. It seems that this method was not the best solution.\nWhich variables should be changed or reverted?\nThe answers can be found in the Fatal Force codebook.\n\n\nDrop missing values\nFor efficiency, we will make changes to variables of interest and drop all missing values.\n\nfatal %&gt;% \n  mutate_at(c('threat_type', 'flee_status', 'armed_with', 'city', 'county', 'gender', 'race'), as.factor) %&gt;%\n  drop_na() -&gt; fatal_clean # generate a new `clean` dataframe\nstr(fatal_clean)\n\ntibble [8,981 × 20] (S3: tbl_df/tbl/data.frame)\n $ state                     : chr [1:8981] \"WA\" \"OR\" \"KS\" \"CA\" ...\n $ year                      : chr [1:8981] \"2015\" \"2015\" \"2015\" \"2015\" ...\n $ id                        : int [1:8981] 3 4 5 8 9 11 13 15 16 17 ...\n $ date                      : Date[1:8981], format: \"2015-01-02\" \"2015-01-02\" ...\n $ threat_type               : Factor w/ 9 levels \"\",\"accident\",..: 6 6 5 6 6 3 7 6 2 6 ...\n $ flee_status               : Factor w/ 5 levels \"\",\"car\",\"foot\",..: 4 4 4 4 4 4 2 4 4 4 ...\n $ armed_with                : Factor w/ 31 levels \"\",\"blunt_object\",..: 7 7 25 21 17 7 7 7 25 21 ...\n $ city                      : Factor w/ 3749 levels \"\",\"Abbeville\",..: 3074 51 3656 2966 1055 1373 561 135 429 1721 ...\n $ county                    : Factor w/ 988 levels \"\",\"Acadia\",\"Ada\",..: 555 941 803 782 951 521 548 777 255 20 ...\n $ latitude                  : num [1:8981] 47.2 45.5 37.7 37.8 40.4 ...\n $ longitude                 : num [1:8981] -123.1 -122.9 -97.3 -122.4 -104.7 ...\n $ location_precision        : chr [1:8981] \"not_available\" \"not_available\" \"not_available\" \"not_available\" ...\n $ name                      : chr [1:8981] \"Tim Elliot\" \"Lewis Lee Lembke\" \"John Paul Quintero\" \"Matthew Hoffman\" ...\n $ age                       : num [1:8981] 53 47 23 32 39 18 22 35 34 47 ...\n $ gender                    : Factor w/ 4 levels \"\",\"female\",\"male\",..: 3 3 3 3 3 3 3 3 2 3 ...\n $ race                      : Factor w/ 13 levels \"\",\"A\",\"B\",\"B;H\",..: 2 9 5 9 5 9 5 9 9 3 ...\n $ race_source               : chr [1:8981] \"not_available\" \"not_available\" \"not_available\" \"not_available\" ...\n $ was_mental_illness_related: logi [1:8981] TRUE FALSE FALSE TRUE FALSE FALSE ...\n $ body_camera               : logi [1:8981] FALSE FALSE FALSE FALSE FALSE FALSE ...\n $ agency_ids                : chr [1:8981] \"73\" \"70\" \"238\" \"196\" ...\n\n\nHowever, with more time, we should inspect the data further and change all variables.\n\n\n\n\nCheck the data\nOne more check to make sure that we have dropped any missing observations.\nI will use the sapply() function from Lab 1 to check for missing values.\nFirst, we’ll check our original data.\n\nsapply(fatal, function(x) sum(is.na(x)))\n\n                     state                       year \n                         0                          0 \n                        id                       date \n                         0                          0 \n               threat_type                flee_status \n                         0                          0 \n                armed_with                       city \n                         0                          0 \n                    county                   latitude \n                         0                       1129 \n                 longitude         location_precision \n                      1130                          0 \n                      name                        age \n                         0                        384 \n                    gender                       race \n                         0                          0 \n               race_source was_mental_illness_related \n                         0                          0 \n               body_camera                 agency_ids \n                         0                          0 \n\n\n\nThen we’ll examine our cleaned data.\n\nsapply(fatal_clean, function(x) sum(is.na(x)))\n\n                     state                       year \n                         0                          0 \n                        id                       date \n                         0                          0 \n               threat_type                flee_status \n                         0                          0 \n                armed_with                       city \n                         0                          0 \n                    county                   latitude \n                         0                          0 \n                 longitude         location_precision \n                         0                          0 \n                      name                        age \n                         0                          0 \n                    gender                       race \n                         0                          0 \n               race_source was_mental_illness_related \n                         0                          0 \n               body_camera                 agency_ids \n                         0                          0 \n\n\n\n\n\nExplore the data\nWe will begin to explore with a summary of our cleaned data.\n\nsummary(fatal_clean)\n\n    state               year                 id             date           \n Length:8981        Length:8981        Min.   :    3   Min.   :2015-01-02  \n Class :character   Class :character   1st Qu.: 2579   1st Qu.:2017-05-08  \n Mode  :character   Mode  :character   Median : 5271   Median :2019-11-13  \n                                       Mean   : 5448   Mean   :2019-12-08  \n                                       3rd Qu.: 8430   3rd Qu.:2022-09-03  \n                                       Max.   :11295   Max.   :2024-12-29  \n                                                                           \n       threat_type   flee_status         armed_with            city     \n shoot       :2507        :1147   gun         :5245   Los Angeles: 141  \n threat      :2299   car  :1360   knife       :1537   Houston    : 118  \n point       :1689   foot :1173   unarmed     : 506   Phoenix    : 115  \n attack      :1279   not  :4978   undetermined: 340   San Antonio:  93  \n move        : 513   other: 323   replica     : 305   Las Vegas  :  82  \n undetermined: 427                vehicle     : 289   Albuquerque:  68  \n (Other)     : 267                (Other)     : 759   (Other)    :8364  \n         county        latitude       longitude       location_precision\n            :3728   Min.   :19.50   Min.   :-160.01   Length:8981       \n Los Angeles: 216   1st Qu.:33.45   1st Qu.:-111.93   Class :character  \n Maricopa   : 145   Median :36.09   Median : -94.17   Mode  :character  \n Harris     :  92   Mean   :36.66   Mean   : -96.78                     \n Jefferson  :  77   3rd Qu.:40.09   3rd Qu.: -83.05                     \n Orange     :  74   Max.   :71.30   Max.   : -67.87                     \n (Other)    :4649                                                       \n     name                age               gender          race     \n Length:8981        Min.   : 0.00             :   4   W      :4079  \n Class :character   1st Qu.:28.00   female    : 411   B      :2209  \n Mode  :character   Median :35.00   male      :8561   H      :1562  \n                    Mean   :37.43   non-binary:   5          : 804  \n                    3rd Qu.:45.00                     A      : 161  \n                    Max.   :91.00                     N      : 121  \n                                                      (Other):  45  \n race_source        was_mental_illness_related body_camera    \n Length:8981        Mode :logical              Mode :logical  \n Class :character   FALSE:7144                 FALSE:7396     \n Mode  :character   TRUE :1837                 TRUE :1585     \n                                                              \n                                                              \n                                                              \n                                                              \n  agency_ids       \n Length:8981       \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n                   \n\n\n\n\nAncillary explorations.\nWith the clean data, I may be interested in getting counts by state and then year as we explore.\n\n# get counts by state then year\nfatal_clean %&gt;% \n  count(state, year)\n\n# A tibble: 500 × 3\n   state year      n\n   &lt;chr&gt; &lt;chr&gt; &lt;int&gt;\n 1 AK    2015      4\n 2 AK    2016      8\n 3 AK    2017      6\n 4 AK    2018      5\n 5 AK    2019      7\n 6 AK    2020      6\n 7 AK    2021      4\n 8 AK    2022      5\n 9 AK    2023      1\n10 AK    2024      7\n# ℹ 490 more rows\n\n\nHow might this output support our model development?\n\n\n\n\n\nTheory\nBased on our explorations up to this point, what data and relationships might you examine?\nConsider a basic path diagram to start and then build on your diagram using your theoretical constructions. Recall from previous lectures our discussions about variable types and their relationships in path diagrams (e.g., moderating vs. mediating).\n\n\n\n\n\ngraph LR\n  A[X] --&gt; B[Y]\n\n\n\n\n\n\n\nResearch questions\nWith our conceptual framing and backing theoretical work, we can proceed to examine exploratory analyses on the data. During the exploratory phase of analysis, a nice opportunity is presented that will allow you to examine the data is to generate a series of discrete research questions that help you make associations between various components of your data.\n\n\n\nNext up: Week 12",
    "crumbs": [
      "Weekly materials",
      "Week 11"
    ]
  },
  {
    "objectID": "weeks/week11-slides.html#part-i-context",
    "href": "weeks/week11-slides.html#part-i-context",
    "title": "DATA 202 - Week 11",
    "section": "Part I: Context",
    "text": "Part I: Context\nOver the next several weeks, our goals will relate to the various ways we can integrate social and theoretical concepts (the context) into decisions around measurement (the content) and analysis (the code). Importantly, we will examine various historical contexts to make sense of what has been done. In doing so, we will make sense on how to build on theory using context."
  },
  {
    "objectID": "weeks/week11-slides.html#sec-content",
    "href": "weeks/week11-slides.html#sec-content",
    "title": "DATA 202 - Week 11",
    "section": "Part II: Content",
    "text": "Part II: Content\nThis week, we will discuss a series of data sources that can be used for your papers.\nGeneral Social Survey (GSS)\nThe main site for the GSS can be found here.\n\nProblem: There was no high quality, accessible data set documenting societal change in America.\n\n\nSolution: National Opinion Research Council (NORC) launched the General Social Survey (GSS)—our longest-running, most respected project."
  },
  {
    "objectID": "weeks/week11-slides.html#regions-of-the-united-states-included-in-the-gss",
    "href": "weeks/week11-slides.html#regions-of-the-united-states-included-in-the-gss",
    "title": "DATA 202 - Week 11",
    "section": "",
    "text": "Inter-university Consortium for Political and Social Research (ICPSR)\nThe main site for the ICPSR data can be found here.\n\nICPSR is research science data and resources on topics like social media, politics, economics, social sciences, government, GIS, & more. It was established in 1962. An integral part of the infrastructure of social science research, ICPSR maintains and provides access to a vast archive of social science data for research and instruction\n\n\n\n\nICPSR logo"
  },
  {
    "objectID": "weeks/week11-slides.html#sec-code",
    "href": "weeks/week11-slides.html#sec-code",
    "title": "DATA 202 - Week 11",
    "section": "Part III: Code",
    "text": "Part III: Code\nWashington Post Fatal Force data\nThe main site for the Fatal Force data can be found here.\nThe Washington Post makes the fatal force data publicly accessible on their website and they provide a direct link to their GitHub repository (or repo). To access the data, we will pull it directly into R from Github.\nLoad the data\nWe will need to install packages and/or load libraries to import and explore the data.\n\n# install packages\ninstall.packages(\"tidyverse\", repos = \"http://cran.us.r-project.org\")\n\n# load libraries\nlibrary(tidyverse) # collection of essential packages for data science\nlibrary(dplyr) # the dplyr package makes data manipulation easier\n\nWe then load the data directly into R from GitHub.\n\nfatal &lt;- read.csv(\"https://raw.githubusercontent.com/washingtonpost/data-police-shootings/master/v2/fatal-police-shootings-data.csv\")"
  },
  {
    "objectID": "weeks/week13.html",
    "href": "weeks/week13.html",
    "title": "DATA 202 - Week 13",
    "section": "",
    "text": "Multivariate analyses may require that we consider various intersections of social justice. Namely, rather than a single lens on an issue, we may be required to take an intersectional perspective with regards to a given set of social issues.\nThere are many different definitions of social justice. Through our course, we have examined different data and statistical analyses to better engage the various connections when measuring and modeling social justice. However, it is important to develop more specific ideations and frameworks as we delve deeper into our selection of various methodologies to explore data.\nEarlier, we considered theory construction as one method to examine distinct and interrelated concepts in the development of a statistical study. Theory construction, together with the importance of understanding the phenomenon of social justice requires that we review some of the key concepts and principles that can be accented by data analysis.\nImportantly, we generally want to prioritize theory and context before methodology.\n\n\n\n\nWhen considering the different definitions of social justice, we want to consider the distinction between individual and collective frameworks for justice. This distinction allows us to explore the relationships between social issues and equality, as a foundational principle to understanding difference. However, when we consider the historical context, we are often required to extend our analyses beyond the blanket expressions for equality.\nEquality and equity are two concepts that are often discussed in the context of social justice and fairness. While they are related, they have distinct meanings.\n\n\n\n\n\n\n\nDefinitions: Equality vs. Equity\n\n\n\nEquality: Equality refers to the state of being equal, particularly in terms of rights, opportunities, and treatment. It emphasizes providing everyone with the same resources, regardless of their circumstances or needs. In other words, equality means that everyone is treated identically, without discrimination or favoritism. The goal of equality is to ensure fairness and eliminate unfair advantages or disadvantages.\nEquity: Equity is about fairness and justice by taking into account the unique circumstances and needs of individuals, as well as the historical contexts. It recognizes that not everyone starts from the same place and that treating everyone equally may not result in true fairness. Equity aims to provide individuals with the resources and support they need to overcome systemic disadvantages and achieve equality of outcome. It involves allocating resources and opportunities based on the specific needs and circumstances of individuals or groups.\n\n\nMore generally, equality focuses on treating everyone the same, while equity emphasizes fairness by addressing individual or group-specific disadvantages and providing resources accordingly. Both concepts play an important role in promoting social justice and creating a more equitable society.\n\n\n\nA surface explanation of equality vs. equity. Image from NACEweb.org\n\n\n\n\n\nThere are many ways to take a theoretical approach to social justice, and often too many to list. As a result, it may be helpful for us to consider the principles of social justice as a framework to adapt your specific analyses (using real-world data) and your theoretical framework to a subset of these principles.\n\nPrinciples of social justice\n\n\n\n\n\n\n\n\nPrinciple\nComponent 1\nComponent 2\nComponent 3\n\n\n\n\nFairness\nEqual opportunity\nSystemic disadvantage\nReducing inequality\n\n\nSolidarity\nSocial cohesion\nSelf-determination\nCollective responsibility\n\n\nDignity\nHuman rights\nWorth of all\nCombating marginalization\n\n\n\n\n\n\n\nJust as there are many theoretical entry points to social justice, there are also dimensional components that may be used to help focus your analysis. These dimensions help to clarify the specific principles noted above in a more concrete way. Importantly, these principles and dimensions are neither rigid or fixed.\n\nDimensions of social justice\n\n\n\n\n\n\n\n\nDimension\nComponent 1\nComponent 2\nComponent 3\n\n\n\n\nEconomic\nPoverty and income inequality\nFair distribution of resources and wealth\nAccess to education, employment, healthcare\n\n\nEnvironmental\nProtecting and preserving the environment\nMitigating environmental degradation\nEnsuring equitable access to natural resources\n\n\nRacial and ethnic\nCombating racism and discrimination\nPromoting racial and ethnic equality\nRecognizing and valuing cultural diversity\n\n\nGender and LGBTQ+\nAchieving gender equality and empowerment\nAddressing discrimination based on sexuality or gender expression\nEnsuring equal rights for all genders\n\n\n\n\n\n\n\nA sample framework for racial justice in education. Image from NEA.org\n\n\n\n\n\n\nAs you consider the various principles and dimensions in your theoretical framing of a particular social issue, data can be used as a method to promote social justice by examining attitudes around the various principles and dimensions (e.g., consider variables in the GSS data), examining real-world data on issues of social injustice (e.g., consider the fatal force database), and a host of other methods that center data analysis as a form of political knowledge.\nThere are, however, specific ways to think about the implications of a particular analytical study. These methods can help to further refine the role and goals of any particular modeling of a social justice issues. Importantly, considering the various contexts to which an issue intersections is also important (i.e., global, national, and/or local).\n\n\nAdvocacy and activism\n– Using data to raise awareness about social issues\n– Mobilizing for policy change with various statistical models\n– Supporting grassroots movements using quantitative information\n\n\n\nPolicy interventions\n– Analyses to help examine and implement laws and regulations\n– Quantification to help redistribute resources and opportunities\n– Models to help inform inclusive social welfare programs\n\n\n\nEducation and awareness\n– Promoting critical thinking and empathy\n– Incorporating social justice into curricula\n– Engaging in dialogue and community outreach\n– Calls to action for promoting a more just and equitable society",
    "crumbs": [
      "Weekly materials",
      "Week 13"
    ]
  },
  {
    "objectID": "weeks/week13.html#part-i-context",
    "href": "weeks/week13.html#part-i-context",
    "title": "DATA 202 - Week 13",
    "section": "",
    "text": "Multivariate analyses may require that we consider various intersections of social justice. Namely, rather than a single lens on an issue, we may be required to take an intersectional perspective with regards to a given set of social issues.\nThere are many different definitions of social justice. Through our course, we have examined different data and statistical analyses to better engage the various connections when measuring and modeling social justice. However, it is important to develop more specific ideations and frameworks as we delve deeper into our selection of various methodologies to explore data.\nEarlier, we considered theory construction as one method to examine distinct and interrelated concepts in the development of a statistical study. Theory construction, together with the importance of understanding the phenomenon of social justice requires that we review some of the key concepts and principles that can be accented by data analysis.\nImportantly, we generally want to prioritize theory and context before methodology.\n\n\n\n\nWhen considering the different definitions of social justice, we want to consider the distinction between individual and collective frameworks for justice. This distinction allows us to explore the relationships between social issues and equality, as a foundational principle to understanding difference. However, when we consider the historical context, we are often required to extend our analyses beyond the blanket expressions for equality.\nEquality and equity are two concepts that are often discussed in the context of social justice and fairness. While they are related, they have distinct meanings.\n\n\n\n\n\n\n\nDefinitions: Equality vs. Equity\n\n\n\nEquality: Equality refers to the state of being equal, particularly in terms of rights, opportunities, and treatment. It emphasizes providing everyone with the same resources, regardless of their circumstances or needs. In other words, equality means that everyone is treated identically, without discrimination or favoritism. The goal of equality is to ensure fairness and eliminate unfair advantages or disadvantages.\nEquity: Equity is about fairness and justice by taking into account the unique circumstances and needs of individuals, as well as the historical contexts. It recognizes that not everyone starts from the same place and that treating everyone equally may not result in true fairness. Equity aims to provide individuals with the resources and support they need to overcome systemic disadvantages and achieve equality of outcome. It involves allocating resources and opportunities based on the specific needs and circumstances of individuals or groups.\n\n\nMore generally, equality focuses on treating everyone the same, while equity emphasizes fairness by addressing individual or group-specific disadvantages and providing resources accordingly. Both concepts play an important role in promoting social justice and creating a more equitable society.\n\n\n\nA surface explanation of equality vs. equity. Image from NACEweb.org\n\n\n\n\n\nThere are many ways to take a theoretical approach to social justice, and often too many to list. As a result, it may be helpful for us to consider the principles of social justice as a framework to adapt your specific analyses (using real-world data) and your theoretical framework to a subset of these principles.\n\nPrinciples of social justice\n\n\n\n\n\n\n\n\nPrinciple\nComponent 1\nComponent 2\nComponent 3\n\n\n\n\nFairness\nEqual opportunity\nSystemic disadvantage\nReducing inequality\n\n\nSolidarity\nSocial cohesion\nSelf-determination\nCollective responsibility\n\n\nDignity\nHuman rights\nWorth of all\nCombating marginalization\n\n\n\n\n\n\n\nJust as there are many theoretical entry points to social justice, there are also dimensional components that may be used to help focus your analysis. These dimensions help to clarify the specific principles noted above in a more concrete way. Importantly, these principles and dimensions are neither rigid or fixed.\n\nDimensions of social justice\n\n\n\n\n\n\n\n\nDimension\nComponent 1\nComponent 2\nComponent 3\n\n\n\n\nEconomic\nPoverty and income inequality\nFair distribution of resources and wealth\nAccess to education, employment, healthcare\n\n\nEnvironmental\nProtecting and preserving the environment\nMitigating environmental degradation\nEnsuring equitable access to natural resources\n\n\nRacial and ethnic\nCombating racism and discrimination\nPromoting racial and ethnic equality\nRecognizing and valuing cultural diversity\n\n\nGender and LGBTQ+\nAchieving gender equality and empowerment\nAddressing discrimination based on sexuality or gender expression\nEnsuring equal rights for all genders\n\n\n\n\n\n\n\nA sample framework for racial justice in education. Image from NEA.org\n\n\n\n\n\n\nAs you consider the various principles and dimensions in your theoretical framing of a particular social issue, data can be used as a method to promote social justice by examining attitudes around the various principles and dimensions (e.g., consider variables in the GSS data), examining real-world data on issues of social injustice (e.g., consider the fatal force database), and a host of other methods that center data analysis as a form of political knowledge.\nThere are, however, specific ways to think about the implications of a particular analytical study. These methods can help to further refine the role and goals of any particular modeling of a social justice issues. Importantly, considering the various contexts to which an issue intersections is also important (i.e., global, national, and/or local).\n\n\nAdvocacy and activism\n– Using data to raise awareness about social issues\n– Mobilizing for policy change with various statistical models\n– Supporting grassroots movements using quantitative information\n\n\n\nPolicy interventions\n– Analyses to help examine and implement laws and regulations\n– Quantification to help redistribute resources and opportunities\n– Models to help inform inclusive social welfare programs\n\n\n\nEducation and awareness\n– Promoting critical thinking and empathy\n– Incorporating social justice into curricula\n– Engaging in dialogue and community outreach\n– Calls to action for promoting a more just and equitable society",
    "crumbs": [
      "Weekly materials",
      "Week 13"
    ]
  },
  {
    "objectID": "weeks/week13.html#part-ii-content",
    "href": "weeks/week13.html#part-ii-content",
    "title": "DATA 202 - Week 13",
    "section": "Part II: Content",
    "text": "Part II: Content\nMultiple Variable Analysis and Multivariate Analysis are two terms often used in statistics and research methodology to describe different approaches to analyzing data involving multiple variables. While they share similarities, there are distinct differences between these two concepts.\n\n\nMultivariable vs. Multivariate\nMultiple variable analysis investigates the influence of individual independent variables on a single dependent variable, while multivariate analysis explores the relationships and patterns among multiple variables simultaneously. Multiple Variable Analysis is often used when studying the effects of specific factors, while multivariate analysis is employed to uncover broader patterns and structures within a dataset. Both approaches are valuable in data analysis, and the choice between them depends on the research objectives and the nature of the data being analyzed.\n\n\n\n\n\n\nDefinitions: Multiple variable analysis vs. Multivariate analysis\n\n\n\nMultiple Variable Analysis: Multiple Variable Analysis refers to the process of examining the relationships between several independent variables and a single dependent variable. It aims to understand how each independent variable influences or predicts the dependent variable individually, while controlling for other variables. In this analysis, each independent variable is analyzed separately, often using techniques such as regression analysis or analysis of variance (ANOVA).\nMultivariate Analysis: Multivariate Analysis involves the simultaneous analysis of multiple dependent and independent variables. It aims to explore the relationships and patterns among multiple variables, considering them as a whole. This analysis technique allows for the examination of complex interactions and associations between variables, providing a more comprehensive understanding of the data.\n\n\n\n\nKey characteristics of multiple variable analysis\n\nFocus: Examining the impact of individual independent variables on a single dependent variable.\nAnalytic approach: Each independent variable is analyzed separately, allowing for isolation of their effects.\nPurpose: To identify the individual contributions and significance of multiple variables in explaining the variation in the dependent variable.\nStatistical techniques: Common techniques include simple linear regression, multiple linear regression, and ANOVA.\n\n\n\n\nKey characteristics of multivariate analysis\n\nFocus: Examining the relationships and interactions among multiple variables simultaneously.\nAnalytic approach: Considering all variables together, accounting for their joint effects and potential interdependence.\nPurpose: To explore patterns, associations, and structures within the data, identifying underlying factors or dimensions.\nStatistical techniques: Common techniques include factor analysis, principal component analysis, cluster analysis, and structural equation modeling.\n\n\n\n\n\nExamples of multivariate analysis techniques\n\nPrincipal component analysis (PCA): PCA is used to reduce the dimensionality of data by transforming it into a new set of uncorrelated variables called principal components. R functions for PCA include prcomp() and princomp().\nFactor analysis: Factor Analysis aims to identify latent factors that explain the correlations among observed variables. R offers functions like factanal() and psych::fa() for conducting factor analysis.\nCanonical correlation analysis (CCA): CCA examines the relationships between two sets of variables and identifies the linear combinations of each set that have maximum correlation with each other. The CCA() function in the stats package can be used for this analysis.\nCluster analysis: Cluster Analysis groups similar observations into clusters based on the similarity of their characteristics. R provides various clustering techniques, such as k-means clustering (kmeans()), hierarchical clustering (hclust()), and model-based clustering (Mclust()).\nDiscriminant analysis: Discriminant Analysis aims to find a linear combination of variables that maximally separate predefined groups or classes. R offers functions like lda() and qda() for performing Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA), respectively.\nMultivariate regression: Multivariate Regression extends simple linear regression to multiple response variables. The lm() function in R can be used for multivariate regression analysis.\nMultivariate analysis of variance (MANOVA): MANOVA extends the analysis of variance (ANOVA) to multiple response variables simultaneously. The manova() function in R can be used for MANOVA.\nMultidimensional scaling (MDS): MDS visualizes the similarity or dissimilarity between objects in a lower-dimensional space. R provides functions like cmdscale() and isoMDS() for performing MDS.\nStructural Equation Modeling (SEM): SEM is a comprehensive framework for testing complex relationships among variables. R packages like lavaan and sem offer functionalities for conducting SEM.\nCorrespondence Analysis: Correspondence Analysis explores the associations between categorical variables and visualizes them in a low-dimensional space. The ca() function in the ca package is commonly used for correspondence analysis.\n\nWe will consider a few of these models in our final weeks for the course.",
    "crumbs": [
      "Weekly materials",
      "Week 13"
    ]
  },
  {
    "objectID": "weeks/week13.html#part-iii-code",
    "href": "weeks/week13.html#part-iii-code",
    "title": "DATA 202 - Week 13",
    "section": "Part III: Code",
    "text": "Part III: Code\nWe will discuss specific code for your projects.\n\nNext up: Week 14",
    "crumbs": [
      "Weekly materials",
      "Week 13"
    ]
  },
  {
    "objectID": "weeks/week13-slides.html#part-i-context",
    "href": "weeks/week13-slides.html#part-i-context",
    "title": "DATA 202 - Week 13",
    "section": "Part I: Context",
    "text": "Part I: Context\nFraming social justice\nMultivariate analyses may require that we consider various intersections of social justice. Namely, rather than a single lens on an issue, we may be required to take an intersectional perspective with regards to a given set of social issues.\nThere are many different definitions of social justice. Through our course, we have examined different data and statistical analyses to better engage the various connections when measuring and modeling social justice. However, it is important to develop more specific ideations and frameworks as we delve deeper into our selection of various methodologies to explore data.\nEarlier, we considered theory construction as one method to examine distinct and interrelated concepts in the development of a statistical study. Theory construction, together with the importance of understanding the phenomenon of social justice requires that we review some of the key concepts and principles that can be accented by data analysis.\nImportantly, we generally want to prioritize theory and context before methodology."
  },
  {
    "objectID": "weeks/week13-slides.html#part-ii-content",
    "href": "weeks/week13-slides.html#part-ii-content",
    "title": "DATA 202 - Week 13",
    "section": "Part II: Content",
    "text": "Part II: Content\nMultiple Variable Analysis and Multivariate Analysis are two terms often used in statistics and research methodology to describe different approaches to analyzing data involving multiple variables. While they share similarities, there are distinct differences between these two concepts."
  },
  {
    "objectID": "weeks/week13-slides.html#part-iii-code",
    "href": "weeks/week13-slides.html#part-iii-code",
    "title": "DATA 202 - Week 13",
    "section": "Part III: Code",
    "text": "Part III: Code\nWe will discuss specific code for your projects.\nNext up: Week 14\n\n\n\n\nCourse Data GitHub"
  },
  {
    "objectID": "weeks/week08.html",
    "href": "weeks/week08.html",
    "title": "DATA 202 - Week 8",
    "section": "",
    "text": "This week we will outline exploratory data analysis through a set of case studies. Each case study provides one example of the various decision making processes needed in a critical quantitative research framework. The included cases focus on more heavily on theories and frameworks and the related practices around modeling and statistical inference.\n\n\n\nWhat do you know about a man named George Floyd and the events that occurred on May 25, 2020?\n\n\n\nImage from Time.com\n\n\n\nGeorge Floyd was murdered by a police officer in broad daylight. His murder resulted in a series of protests and expanded the already longstanding discussions around racism, state violence, and police brutality in the United States. Consider How George Floyd Was Killed in Police Custody to get a step-by-step reconstruction of the murder.\n\n\nThe Yahoo! News Race and Justice poll was conducted in 2020 after the killing of George Floyd. There is pre-loaded data set located in R. A complete description of the data is as follows:\n\nResults from a Yahoo! News poll conducted by YouGov on May 29-31, 2020. In total 1060 U.S. adults were asked a series of questions regarding race and justice in the wake of the killing of George Floyd by a police officer. Results in this data set are percentages for the question, “Do you think Blacks and Whites receive equal treatment from the police?” For this particular question there were 1059 respondents.\n\nFor efficient access, poll results are stored in a replica of the yahoo_data data frame located in the critstats package.\n\n\n\n\nThe process of exploratory data analysis will vary across projects. It is helpful if your exploratory analysis is directed by a set of clear questions and introductory tasks. The exploring data phase should not be solely used for cleaning data, although the two steps are often intertwined.\nWe will structure our exploratory analysis around three steps:\n\nInformation: Gaining a solid understanding of each variable and its observations\nShape: Gaining a better feel for the “shape” and distribution of the observations\nMeasures: Checking base assumptions for any forthcoming analysis\n\nThese steps can help guide more concrete theory building and modeling.\n\n\n\n\n\nlibrary(critstats)\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\n\n\n\nLoad the yahoo_data data frame and use the str() command.\n\nyahoo &lt;- critstats::yahoo_data\n\n\nsummary(yahoo)\n\n   race_eth           response        \n Length:1059        Length:1059       \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n\nglimpse(yahoo)\n\nRows: 1,059\nColumns: 2\n$ race_eth &lt;chr&gt; \"White\", \"White\", \"White\", \"White\", \"White\", \"White\", \"White\"…\n$ response &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\"…\n\n\n\n\nhead(yahoo)\n\n# A tibble: 6 × 2\n  race_eth response\n  &lt;chr&gt;    &lt;chr&gt;   \n1 White    Yes     \n2 White    Yes     \n3 White    Yes     \n4 White    Yes     \n5 White    Yes     \n6 White    Yes     \n\ntail(yahoo)\n\n# A tibble: 6 × 2\n  race_eth response\n  &lt;chr&gt;    &lt;chr&gt;   \n1 Other    Not sure\n2 Other    Not sure\n3 Other    Not sure\n4 Other    Not sure\n5 Other    Not sure\n6 Other    Not sure\n\n\n\n\n\n\nAs an early start, we can use the output above to start our exploration.\nTo help focus these explorations, it is helpful to outline a set of questions or tasks. These tasks and questions can help identify some of the key goals of the exploratory data analysis. Wickham, Çetinkaya-Rundel, & Grolemund (2023) talk about this at length in the Exploratory Data Analysis section of the R for Data Science text.\nFor this exploration, our goal is to understand the proportion of observations in each variable.\nTo accomplish this goal, we can outline two questions around three tasks.\n\n\n\n\n\n\n\nQuestions\nTasks & Detail\n\n\n\n\nQuestion 1: How are observations distributed?\nTask 1.1: Create a table of variable counts\n\n\n\nTask 1.2: Create a table of variable proportions\n\n\nQuestion 2: How can we visualize distributions?\nTask 2.1: Create two to three basic visualizations\n\n\n\n\n\n\n\nTask 1.1: Create a table of variable counts\nTask 1.2: Create a table of variable proportions\nTask 2.1: Create two to three basic visualizations\n\n\n\n\n\nGain a solid understanding of each variable and its observations.\n\n\n\n\ntable(yahoo$race_eth)\n\n\n   Black Hispanic    Other    White \n     101      104       82      772 \n\ntable(yahoo$response)\n\n\n      No Not sure      Yes \n     700      123      236 \n\n\n\n\n\n\nprop.table(table(yahoo$race_eth))\n\n\n     Black   Hispanic      Other      White \n0.09537299 0.09820585 0.07743154 0.72898961 \n\nprop.table(table(yahoo$response))\n\n\n       No  Not sure       Yes \n0.6610009 0.1161473 0.2228517 \n\n\n\n\n\n\nThe previous output provides more information than may be immediately noted:\n\nLists the various labels for each variable\nPresents the distribution of observations across each label\nSpecific relations can be pointed to the study framing\nEarly insight into base assumptions\n\n\n\n\n\n\n\nGain a better feel for the “shape” and distribution of the observations.\n\n\n\nWe introduce some basic features of `ggplot() here to complete the task.\n\n# start with the plot outline\nggplot(yahoo)\n\n\n\n\n\n\n\n\n\n\n# add the variable categories to the plot using aes()\nggplot(yahoo, aes(x = race_eth))\n\n\n\n\n\n\n\n\n\n\n# add the bars to the plot using + geom_bar()\nggplot(yahoo, aes(x = race_eth)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEXERCISE\n\n\n\nCreate a bar plot for the response variable.\nWhat other plots could you create?\n\n\nWickham, Çetinkaya-Rundel, & Grolemund (2023) have a detailed section on data visualization in the R for Data Science text.\n\n\n\n\n\nCheck base assumptions for any forthcoming analysis.\nTo complete this step, we delve into some of the assumptions and foundations surrounding the use of sample data versus population data. The mathematical foundations allow us to understand how to compute measures and differences. The assumptions provide direction for any further modeling practices.\nWe explore this more in part II.",
    "crumbs": [
      "Weekly materials",
      "Week 8"
    ]
  },
  {
    "objectID": "weeks/week08.html#part-i-context",
    "href": "weeks/week08.html#part-i-context",
    "title": "DATA 202 - Week 8",
    "section": "",
    "text": "This week we will outline exploratory data analysis through a set of case studies. Each case study provides one example of the various decision making processes needed in a critical quantitative research framework. The included cases focus on more heavily on theories and frameworks and the related practices around modeling and statistical inference.\n\n\n\nWhat do you know about a man named George Floyd and the events that occurred on May 25, 2020?\n\n\n\nImage from Time.com\n\n\n\nGeorge Floyd was murdered by a police officer in broad daylight. His murder resulted in a series of protests and expanded the already longstanding discussions around racism, state violence, and police brutality in the United States. Consider How George Floyd Was Killed in Police Custody to get a step-by-step reconstruction of the murder.\n\n\nThe Yahoo! News Race and Justice poll was conducted in 2020 after the killing of George Floyd. There is pre-loaded data set located in R. A complete description of the data is as follows:\n\nResults from a Yahoo! News poll conducted by YouGov on May 29-31, 2020. In total 1060 U.S. adults were asked a series of questions regarding race and justice in the wake of the killing of George Floyd by a police officer. Results in this data set are percentages for the question, “Do you think Blacks and Whites receive equal treatment from the police?” For this particular question there were 1059 respondents.\n\nFor efficient access, poll results are stored in a replica of the yahoo_data data frame located in the critstats package.\n\n\n\n\nThe process of exploratory data analysis will vary across projects. It is helpful if your exploratory analysis is directed by a set of clear questions and introductory tasks. The exploring data phase should not be solely used for cleaning data, although the two steps are often intertwined.\nWe will structure our exploratory analysis around three steps:\n\nInformation: Gaining a solid understanding of each variable and its observations\nShape: Gaining a better feel for the “shape” and distribution of the observations\nMeasures: Checking base assumptions for any forthcoming analysis\n\nThese steps can help guide more concrete theory building and modeling.\n\n\n\n\n\nlibrary(critstats)\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\n\n\n\nLoad the yahoo_data data frame and use the str() command.\n\nyahoo &lt;- critstats::yahoo_data\n\n\nsummary(yahoo)\n\n   race_eth           response        \n Length:1059        Length:1059       \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n\nglimpse(yahoo)\n\nRows: 1,059\nColumns: 2\n$ race_eth &lt;chr&gt; \"White\", \"White\", \"White\", \"White\", \"White\", \"White\", \"White\"…\n$ response &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\"…\n\n\n\n\nhead(yahoo)\n\n# A tibble: 6 × 2\n  race_eth response\n  &lt;chr&gt;    &lt;chr&gt;   \n1 White    Yes     \n2 White    Yes     \n3 White    Yes     \n4 White    Yes     \n5 White    Yes     \n6 White    Yes     \n\ntail(yahoo)\n\n# A tibble: 6 × 2\n  race_eth response\n  &lt;chr&gt;    &lt;chr&gt;   \n1 Other    Not sure\n2 Other    Not sure\n3 Other    Not sure\n4 Other    Not sure\n5 Other    Not sure\n6 Other    Not sure\n\n\n\n\n\n\nAs an early start, we can use the output above to start our exploration.\nTo help focus these explorations, it is helpful to outline a set of questions or tasks. These tasks and questions can help identify some of the key goals of the exploratory data analysis. Wickham, Çetinkaya-Rundel, & Grolemund (2023) talk about this at length in the Exploratory Data Analysis section of the R for Data Science text.\nFor this exploration, our goal is to understand the proportion of observations in each variable.\nTo accomplish this goal, we can outline two questions around three tasks.\n\n\n\n\n\n\n\nQuestions\nTasks & Detail\n\n\n\n\nQuestion 1: How are observations distributed?\nTask 1.1: Create a table of variable counts\n\n\n\nTask 1.2: Create a table of variable proportions\n\n\nQuestion 2: How can we visualize distributions?\nTask 2.1: Create two to three basic visualizations\n\n\n\n\n\n\n\nTask 1.1: Create a table of variable counts\nTask 1.2: Create a table of variable proportions\nTask 2.1: Create two to three basic visualizations\n\n\n\n\n\nGain a solid understanding of each variable and its observations.\n\n\n\n\ntable(yahoo$race_eth)\n\n\n   Black Hispanic    Other    White \n     101      104       82      772 \n\ntable(yahoo$response)\n\n\n      No Not sure      Yes \n     700      123      236 \n\n\n\n\n\n\nprop.table(table(yahoo$race_eth))\n\n\n     Black   Hispanic      Other      White \n0.09537299 0.09820585 0.07743154 0.72898961 \n\nprop.table(table(yahoo$response))\n\n\n       No  Not sure       Yes \n0.6610009 0.1161473 0.2228517 \n\n\n\n\n\n\nThe previous output provides more information than may be immediately noted:\n\nLists the various labels for each variable\nPresents the distribution of observations across each label\nSpecific relations can be pointed to the study framing\nEarly insight into base assumptions\n\n\n\n\n\n\n\nGain a better feel for the “shape” and distribution of the observations.\n\n\n\nWe introduce some basic features of `ggplot() here to complete the task.\n\n# start with the plot outline\nggplot(yahoo)\n\n\n\n\n\n\n\n\n\n\n# add the variable categories to the plot using aes()\nggplot(yahoo, aes(x = race_eth))\n\n\n\n\n\n\n\n\n\n\n# add the bars to the plot using + geom_bar()\nggplot(yahoo, aes(x = race_eth)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEXERCISE\n\n\n\nCreate a bar plot for the response variable.\nWhat other plots could you create?\n\n\nWickham, Çetinkaya-Rundel, & Grolemund (2023) have a detailed section on data visualization in the R for Data Science text.\n\n\n\n\n\nCheck base assumptions for any forthcoming analysis.\nTo complete this step, we delve into some of the assumptions and foundations surrounding the use of sample data versus population data. The mathematical foundations allow us to understand how to compute measures and differences. The assumptions provide direction for any further modeling practices.\nWe explore this more in part II.",
    "crumbs": [
      "Weekly materials",
      "Week 8"
    ]
  },
  {
    "objectID": "weeks/week08.html#part-ii-content",
    "href": "weeks/week08.html#part-ii-content",
    "title": "DATA 202 - Week 8",
    "section": "Part II: Content",
    "text": "Part II: Content\n\n\nA guiding example\nLet us begin with a summary of responses to a survey sparked by the killing of George Floyd.\n\nSummary of survey response data1\n\n\nResponse\nTotal\n\n\n\n\nNo\n700\n\n\nYes\n236\n\n\n\nThe study sample \\(n = 936\\). Please take note of the table’s footnote.\nAs of now, we know very little about the survey (i.e., the context has been stripped away).\n\nAn initial question\nIs there a difference in the survey responses?\n\n\n\nFraming proportions\nAs you learn about hypothesis testing, you want to understand the nature of variable associations. There are many approaches to how we measure associations. One we’ll explore extensively is differences in proportions of two dichotomous variables.\nFirst, we add a third column to our table by computing the proportion in each row:\n\\[ P(No) = \\frac{\\text{Number of respondents who replied No}}{n} \\]\n\\[ P(Yes) = \\frac{\\text{Number of respondents who replied Yes}}{n} \\]\n\n\n\nUnderstanding proportions\n\nSummary of survey response data with proportion column\n\n\nResponse\nTotal\nProportion\n\n\n\n\nNo\n700\n\\(0.7478\\)\n\n\nYes\n236\n\\(0.2522\\)\n\n\n\nSo we have \\(P(No) = 0.7478\\) and \\(P(Yes) = 0.2522\\).\nThis is a sample. Be mindful of the notes about sampling and sampling error.\nWhat can these proportions tell us about the data?\n\n\n\n\nA closer look\nLet’s look at a preview panel of the full data set.\n\nPreview of full survey response data set\n\n\nID\nRace\nResponse\n\n\n\n\n1\nWhite\nYes\n\n\n2\nWhite\nYes\n\n\n.\n.\n.\n\n\n773\nBlack\nYes\n\n\n774\nBlack\nYes\n\n\n..\n..\n..\n\n\n874\nHispanic\nYes\n\n\n875\nHispanic\nYes\n\n\n…\n…\n…\n\n\n\\(1059\\)\nOther\nNot sure\n\n\n\n\nWhen we format the data into tables, there are multiple categories to consider.\nIn the table below, the race variable is listed in rows.\n\n\n\nTable 1: Race in rows\n\n\n\n\n\nRace\nNo\nYes\n\n\n\n\nBlack\nBlack \\(\\cap\\) No\nBlack \\(\\cap\\) Yes\n\n\nHispanic\nHispanic \\(\\cap\\) No\nHispanic \\(\\cap\\) Yes\n\n\nOther\nOther \\(\\cap\\) No\nOther \\(\\cap\\) Yes\n\n\nWhite\nWhite \\(\\cap\\) No\nWhite \\(\\cap\\) Yes\n\n\n\n\n\n\nRecall that the \\(\\cap\\) represents “and” while the \\(\\cup\\) represents “or” in logic.\n\nIn the table below, the response variable is listed in rows.\n\n\n\nTable 2: Response in rows\n\n\n\n\n\n\n\n\n\n\n\n\nResponse\nBlack\nHispanic\nOther\nWhite\n\n\n\n\nNo\nNo \\(\\cap\\) Black\nNo \\(\\cap\\) Hispanic\nNo \\(\\cap\\) Other\nNo \\(\\cap\\) White\n\n\nYes\nYes \\(\\cap\\) Black\nYes \\(\\cap\\) Hispanic\nYes \\(\\cap\\) Other\nYes \\(\\cap\\) White\n\n\n\n\n\n\n\nWe use our understanding of probability to generate a reconstruction for the response ‘No’.\n\\[ P(No) = \\dfrac{\\text{No} \\cap \\text{Black}}{n} + \\dfrac{\\text{No} \\cap \\text{Hispanic}}{n} + \\dfrac{\\text{No} \\cap \\text{Other}}{n} + \\dfrac{\\text{No} \\cap \\text{White}}{n} \\]\nWe generate a parallel construction for the response ‘Yes’.\n\\[ P(Yes) = \\dfrac{\\text{Yes} \\cap \\text{Black}}{n} + \\dfrac{\\text{Yes} \\cap \\text{Hispanic}}{n} + \\dfrac{\\text{Yes} \\cap \\text{Other}}{n} + \\dfrac{\\text{Yes} \\cap \\text{White}}{n} \\]\n\nSummary values\nAdding values to the table will give us a more complete picture of the data.\n\nResponses to a survey question by race\n\n\nResponse\nBlack\nHispanic\nOther\nWhite\n\n\n\n\nNo\n92\n75\n47\n486\n\n\nYes\n6\n15\n14\n201\n\n\n\n\nUse the table to calculate Response probabilities:\n\nResponses to a survey question by race\n\n\nResponse\nBlack\nHispanic\nOther\nWhite\nTotal\n\n\n\n\nNo\n92\n75\n47\n486\n700\n\n\nYes\n6\n15\n14\n201\n236\n\n\n\nGather response probabilities for No by dividing each cell by the sample size.\n\\[ P(No) = \\dfrac{92}{936} + \\dfrac{75}{936} + \\dfrac{47}{936} + \\dfrac{486}{936} = \\dfrac{700}{936} = 0.7478 \\] Gather response probabilities for Yes by dividing each cell by the sample size.\n\\[ P(Yes) = \\dfrac{6}{936} + \\dfrac{15}{936} + \\dfrac{14}{936} + \\dfrac{201}{936} = \\dfrac{236}{936} = 0.2522 \\] We will use this framing to compute proportions for each cell but in R.\n\nGiven that we want to explore the data in more detail and understand any base associations, we need to revisit our framing and consider some additional tasks.\n\n\n\nReturning to the framework\nA basic analysis goes a long way in determining our next steps.\nWith this base analytic exercises, you may want to ask: What is the research question?\nWhat are the main or primary theoretical constructions?\n\nFirst, examine the literature and annotate specific citations.\nNext, construct a framework for the various interpretations.\n\nWhat are the similarities across theoretical constructions?\nWhat are the main differences across theoretical constructions?\nHow does a nuanced view of the history of the issue improve our understanding?\n\n\nBased on your background analyses, what is the hypothesis?",
    "crumbs": [
      "Weekly materials",
      "Week 8"
    ]
  },
  {
    "objectID": "weeks/week08.html#part-iii-code",
    "href": "weeks/week08.html#part-iii-code",
    "title": "DATA 202 - Week 8",
    "section": "Part III: Code",
    "text": "Part III: Code\n\n\nSetting things up\nAs a general rule, always load the appropriate libraries at the start of your analyses.\n\n# load our libraries\nlibrary(critstats)\nlibrary(tidyverse)\nlibrary(descr)\nlibrary(Hmisc)\n\nIn this case:\n\ncritstats contains the yahoo_data that we’d like to analyze.\ntidyverse contains the set of functions to help us work with our data\ndescr and Hmisc contain useful functions to analyze social science data.\n\nIf you receive an error, you may need to use the install.packages() commands first.\n\n\n\nLoad the data\nInspect the data documentation and contents of the data.\n\n# access the yahoo_data\n??yahoo_data\nyahoo_data # \"Do you think Blacks and Whites receive equal treatment from the police?\"\n\nWhile the specific wording of the question may not fit our theoretical framing (e.g. Blacks and Whites), it is useful to consider what information and insights can be gathered from the data.\nHere, theory and the literature become important components to outlining your analytic plan.\n\n\n\nClean your data\nThe data in the yahoo_data set is already cleaned and prepped. However, when you conduct your own analyses, you will need to follow the series of steps outlined in previous assignments.\nAs example, Lab 1 is a great resource on cleaning.\n\n\n\nExplore your data\nGiven that we are analyzing categorical data, we will continue from earlier when we created a series of tables. These tables will allow us to gather a sense of the data.\nOur first table will be of the variable race_eth.\n\n# create a table of the race_eth\ntable(yahoo_data$race_eth)\n\n\n   Black Hispanic    Other    White \n     101      104       82      772 \n\n\nWhat do you notice? What do you wonder?\n\n\n\nExplore your data\nOur second table will be of the variable response.\n\n# create a table of the race_eth\ntable(yahoo_data$response)\n\n\n      No Not sure      Yes \n     700      123      236 \n\n\nWhat do you notice? What do you wonder?\n\n\n\nModify your data\nMake the necessary modifications to your data.\nHere I set the yahoo_data to df and rename the race_eth variable.\n\ndf &lt;- yahoo_data # set the yahoo_data to df\ndf &lt;- df %&gt;% \n  rename(\"race\" = \"race_eth\") # rename the race_eth variable to race\n\n\n\n\nGet the sample size\nWhen looking at the table, it may be useful to get a sense of the proportions.\nWe’ll begin with getting the sample size.\n\n# get a quick count of the sample size\ncount(df) # the count function can collect the sample size, n, of a tibble\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1  1059\n\n\nThe tables give us the frequency of each category.\nThe sample size will help us get the relative frequency of each category.\n\n\n\nRelative frequencies by race\nGet the relative frequencies for the race variable.\n\ndf %&gt;% \n  count(race) %&gt;% \n  mutate(prop = prop.table(n))\n\n# A tibble: 4 × 3\n  race         n   prop\n  &lt;chr&gt;    &lt;int&gt;  &lt;dbl&gt;\n1 Black      101 0.0954\n2 Hispanic   104 0.0982\n3 Other       82 0.0774\n4 White      772 0.729 \n\n\n\n\n\nRelative frequencies by response\nGet the relative frequencies for the response variable.\n\ndf %&gt;% \n  count(response) %&gt;% \n  mutate(prop = prop.table(n))\n\n# A tibble: 3 × 3\n  response     n  prop\n  &lt;chr&gt;    &lt;int&gt; &lt;dbl&gt;\n1 No         700 0.661\n2 Not sure   123 0.116\n3 Yes        236 0.223\n\n\n\n\n\nGenerate a crosstab of race and response\n\ncrosstab(df$response, df$race, plot=F)\n\n   Cell Contents \n|-------------------------|\n|                   Count | \n|-------------------------|\n\n=======================================================\n               df$race\ndf$response    Black   Hispanic   Other   White   Total\n-------------------------------------------------------\nNo                92         75      47     486     700\n-------------------------------------------------------\nNot sure           3         14      21      85     123\n-------------------------------------------------------\nYes                6         15      14     201     236\n-------------------------------------------------------\nTotal            101        104      82     772    1059\n=======================================================\n\n\nSet plot=T if you want to see a visual plot of the data. What do you notice? What do you wonder?\n\n\n\nStandardize the frequencies\nRaw frequencies can be hard to read in a crosstab, especially since column totals are not equal.\nAdd prop.c = T to get column percentages\n\ncrosstab(df$response, df$race, plot=F, prop.c=T)\n\n   Cell Contents \n|-------------------------|\n|                   Count | \n|          Column Percent | \n|-------------------------|\n\n=======================================================\n               df$race\ndf$response    Black   Hispanic   Other   White   Total\n-------------------------------------------------------\nNo               92         75      47     486     700 \n               91.1%      72.1%   57.3%   63.0%        \n-------------------------------------------------------\nNot sure          3         14      21      85     123 \n                3.0%      13.5%   25.6%   11.0%        \n-------------------------------------------------------\nYes               6         15      14     201     236 \n                5.9%      14.4%   17.1%   26.0%        \n-------------------------------------------------------\nTotal           101        104      82     772    1059 \n                9.5%       9.8%    7.7%   72.9%        \n=======================================================\n\n\nWe also refer to crosstabs as contingency tables; the percentages provide conditional probabilities. What do you notice? What do you wonder?\n\n\n\nHypothesis testing with crosstab\nDevelop (or refine) your research question(s).\nExamining the relationship between two variables.\n\n# research inquiry: is there a relationship between the race and response variables?\n# data: survey results from the Yahoo! News race and justice poll\n# note(s): response contains three levels: Yes, No, and Not sure\n\nAdd your research inquiry/question to your preamble.\nNotes help you remember import information for writing code. You can also put them in-line.\n\n\n\nNull and alternative hypothesis for a \\(\\chi^2\\) test are:\n\n\\(H_0\\): No relationship. The two variables are statistically independent.\n\\(H_1\\): There is a relationship. The two variables are not statistically independent.\n\nNotice that \\(H_1\\) does not give us information about the direction or strength of the relationships. To test the null hypothesis, we will calculate the \\(\\chi^2\\) statistic:\n\\[\\chi^2 = \\sum \\dfrac{(O-E)^2}{E}\\]\nWhere\n\n\\(O\\) is the observed frequency for each cell\n\\(E\\) is the expected frequency for each cell\n\nThe expected frequency is what we’d expected if there is no relationship (\\(H_0\\) is true)\n\n\n\n\nCreate a crosstab with raw frequencies\n\ncrosstab(df$response, df$race, plot=F)\n\n   Cell Contents \n|-------------------------|\n|                   Count | \n|-------------------------|\n\n=======================================================\n               df$race\ndf$response    Black   Hispanic   Other   White   Total\n-------------------------------------------------------\nNo                92         75      47     486     700\n-------------------------------------------------------\nNot sure           3         14      21      85     123\n-------------------------------------------------------\nYes                6         15      14     201     236\n-------------------------------------------------------\nTotal            101        104      82     772    1059\n=======================================================\n\n\n\n\n\nSampling error\nAn important note about sampling error\nThere appears to be a relationship between the variable response and race.\nHowever, the data are from a sample. We do not and should not infer that there is a relationship at the population level, but only for the sample data we are analyzing.\nEarlier, we saw the difference in proportions by the column differences for “Yes” and “No”. Since we found that the overall population said “No” to the question of fair treatment, we want to take this into consideration when analyzing our data.\n\n\n\nGather the expected frequencies\n\ncrosstab(df$response, df$race,\n         expected=T, #Add expected frequency to each cell\n         plot=F)\n\n   Cell Contents \n|-------------------------|\n|                   Count | \n|         Expected Values | \n|-------------------------|\n\n=======================================================\n               df$race\ndf$response    Black   Hispanic   Other   White   Total\n-------------------------------------------------------\nNo                92         75      47     486     700\n                66.8       68.7    54.2   510.3        \n-------------------------------------------------------\nNot sure           3         14      21      85     123\n                11.7       12.1     9.5    89.7        \n-------------------------------------------------------\nYes                6         15      14     201     236\n                22.5       23.2    18.3   172.0        \n-------------------------------------------------------\nTotal            101        104      82     772    1059\n=======================================================\n\n\n\n\n\nAdd the chi-square contributions\n\ncrosstab(df$response, df$race,\n         expected=T, #Add expected frequency to each cell\n         prop.chisq = T, #Total contribution of each cell \n         plot=F)\n\n   Cell Contents \n|-------------------------|\n|                   Count | \n|         Expected Values | \n| Chi-square contribution | \n|-------------------------|\n\n=========================================================\n               df$race\ndf$response     Black   Hispanic    Other   White   Total\n---------------------------------------------------------\nNo                 92         75       47     486     700\n                 66.8       68.7     54.2   510.3        \n                9.542      0.569    0.957   1.156        \n---------------------------------------------------------\nNot sure            3         14       21      85     123\n                 11.7       12.1      9.5    89.7        \n                6.498      0.305   13.828   0.243        \n---------------------------------------------------------\nYes                 6         15       14     201     236\n                 22.5       23.2     18.3   172.0        \n               12.107      2.885    1.000   4.874        \n---------------------------------------------------------\nTotal             101        104       82     772    1059\n=========================================================\n\n\n\n\n\nCalculate degrees of freedom\nWe have \\[df_{\\chi^2} = (r-1)(c-1)\\]\nwhere r = number of rows in the table and c = number of columns in the table\n\nChi-square critical values\n\n\n\nChi-square critical values\n\n\n\n\n\nGather critical value of chi-square\n\nqchisq(.05, 6, lower.tail=F)\n\n[1] 12.59159\n\n\n\n\n\nGet the chi-square statistic from R\n\n## get chi-square statistic\nchisq.test(df$response, df$race)\n\n\n    Pearson's Chi-squared test\n\ndata:  df$response and df$race\nX-squared = 53.964, df = 6, p-value = 7.5e-10\n\n\n\nOur focus should be on interpreting the output values.\nIn the output, you want to examine the following:\n\nConfirm \\(\\chi^2\\) value and degrees of freedom.\nNotice more precise information about the p-value\n\nUse this value to make sense of your research inquiry\n\nDoes this p-value make sense statistically and substantively?\n\n\n\n\ndescribe some initial limitations of analysis\nAlways close out your analyses with a write up of limitations.\n\n### limitation 1: sampling error\n\n### limitation 2: category reductions\n\n### limitation 3: cases dropped\n\n### limitation 4: chi-square test\n\n\n\n\n\nNext up: Week 8",
    "crumbs": [
      "Weekly materials",
      "Week 8"
    ]
  },
  {
    "objectID": "weeks/week08.html#footnotes",
    "href": "weeks/week08.html#footnotes",
    "title": "DATA 202 - Week 8",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRespondents who selected ‘Not sure’ on the survey were moved from this analysis.↩︎",
    "crumbs": [
      "Weekly materials",
      "Week 8"
    ]
  },
  {
    "objectID": "weeks/week08-slides.html#part-i-context",
    "href": "weeks/week08-slides.html#part-i-context",
    "title": "DATA 202 - Week 8",
    "section": "Part I: Context",
    "text": "Part I: Context\nThis week we will outline exploratory data analysis through a set of case studies. Each case study provides one example of the various decision making processes needed in a critical quantitative research framework. The included cases focus on more heavily on theories and frameworks and the related practices around modeling and statistical inference."
  },
  {
    "objectID": "weeks/week08-slides.html#part-ii-content",
    "href": "weeks/week08-slides.html#part-ii-content",
    "title": "DATA 202 - Week 8",
    "section": "Part II: Content",
    "text": "Part II: Content"
  },
  {
    "objectID": "weeks/week08-slides.html#part-iii-code",
    "href": "weeks/week08-slides.html#part-iii-code",
    "title": "DATA 202 - Week 8",
    "section": "Part III: Code",
    "text": "Part III: Code"
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Assignments",
    "section": "",
    "text": "This page contains three sections:\n\nBrief overview of all course assignments.\nInformation about lab 0 (ungraded).\n\n\nOverview of course assignments\nOur course assignments will be split between reading and typing (code and analytic results).\n\n\n\n\n\n\n\n\nCourse component\nAssignment\nDate(s)\n\n\n\n\nPart I: Statistics and society\n\n\n\n\n\nLab #0\nDue: Tue Jan 21\n\n\n\nLab #1\nDue: Sun Feb 16\n\n\nPart II: Data and people\n\n\n\n\n\nPaper #1\nDue: Fri Feb 28\n\n\nPart III: Data and policy\n\n\n\n\n\nLab #2\nDue: Sun Mar 30\n\n\nPart IV: Data in practice\n\n\n\n\n\nPaper #2\nDue: Sun Apr 20\n\n\n\nPaper #3 (optional)\nDue: Sun Apr 27\n\n\n\n\n\nSample exercises\nBelow are a few sample exercises of the types of problems we’ll explore in class. This work is ungraded, and while there is no prerequisite for this course, these problems should serve as a review and a preview of content that we’ll explore for the course. Please do not hesitate to reach out with questions.\nExercise 0.1. If an individual’s projected income is to be converted to a z-score, which of these z-scores would a “rational agent” prefer: -2.00, -1.00, 0, 1.00, or 2.00? What is a “rational agent”? Explain why the “rational agent” would prefer the z-score you selected? Cite any sources.\nExercise 0.2. What is an integer? What is the sum of the first 100 positive integers? Explain your solution. Cite any sources. Bonus points (but definitely not required for our course): write a proof of the solution.\nExercise 0.3. Have you ever heard of p-hacking? P-hacking is where a researcher conducts multiple tests and only reports the significant results from those tests. What are some main issues here?\n\n\nLab 0\nFor your first lab, you will install two software programs on your local machine: the first is base R and the second is the RStudio Integrated Development Environment, or IDE. Both programs are required.\n\n\n\n\n\n\nYou must download both R and RStudio!\n\n\n\n\n\nIn order to complete assignments, you must download base R and RStudio to your computer.\n\n\n\nRStudio is now called Posit. When you are searching online you will begin to see language differences.\nIt can all get very confusing. In order to reduce confusion, I am requesting that you watch this short tutorial so that we’re all on the same page. I will discuss this more when we meet.\n\n\nNext up: Computing\nOn the next page, I will walk you through downloading the appropriate software for our work this term.",
    "crumbs": [
      "Course information",
      "Assignments"
    ]
  },
  {
    "objectID": "labs/lab3.html",
    "href": "labs/lab3.html",
    "title": "Lab 3",
    "section": "",
    "text": "For Lab 3, you should be analyzing some set of data related to your paper 1 assignment. Please see the Preparing Lab Reports at the bottom of the lab 1 assignment."
  },
  {
    "objectID": "labs/lab3.html#learning-objectives",
    "href": "labs/lab3.html#learning-objectives",
    "title": "Lab 3",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nThe Lab 3 assignment will provide you with an opportunity to self-direct your analysis to write up a substantive argument started in paper 1 for paper 3."
  },
  {
    "objectID": "labs/lab3.html#report-1.1",
    "href": "labs/lab3.html#report-1.1",
    "title": "Lab 3",
    "section": "Report 1.1",
    "text": "Report 1.1\nWhat data set have you decided to use?"
  },
  {
    "objectID": "labs/lab3.html#report-1.2",
    "href": "labs/lab3.html#report-1.2",
    "title": "Lab 3",
    "section": "Report 1.2",
    "text": "Report 1.2\nWhich two variables from your data set will be analyzed?"
  },
  {
    "objectID": "labs/lab3.html#report-1.3",
    "href": "labs/lab3.html#report-1.3",
    "title": "Lab 3",
    "section": "Report 1.3",
    "text": "Report 1.3\nWhat is your research question?"
  },
  {
    "objectID": "labs/lab3.html#report-1.4",
    "href": "labs/lab3.html#report-1.4",
    "title": "Lab 3",
    "section": "Report 1.4",
    "text": "Report 1.4\nWhat is your data analysis plan? Please be descriptive."
  },
  {
    "objectID": "labs/lab3.html#report-1.5",
    "href": "labs/lab3.html#report-1.5",
    "title": "Lab 3",
    "section": "Report 1.5",
    "text": "Report 1.5\nWhat are some potential limitations for your analysis?"
  },
  {
    "objectID": "labs/lab3.html#report-1.6",
    "href": "labs/lab3.html#report-1.6",
    "title": "Lab 3",
    "section": "Report 1.6",
    "text": "Report 1.6\nDoes your data contain missing values? If so, how have you dealt with these values?"
  },
  {
    "objectID": "labs/lab3.html#report-1.7",
    "href": "labs/lab3.html#report-1.7",
    "title": "Lab 3",
    "section": "Report 1.7",
    "text": "Report 1.7\nPlease include all code used to clean and manipulate the variables."
  },
  {
    "objectID": "labs/lab3.html#report-1.8",
    "href": "labs/lab3.html#report-1.8",
    "title": "Lab 3",
    "section": "Report 1.8",
    "text": "Report 1.8\nWhat relationship, if any, exists between the variables?"
  },
  {
    "objectID": "labs/lab3.html#report-1.9",
    "href": "labs/lab3.html#report-1.9",
    "title": "Lab 3",
    "section": "Report 1.9",
    "text": "Report 1.9\nHow do these findings relate to your research question and theory outlined in your proposal for paper 1? Please be specific."
  },
  {
    "objectID": "labs/lab3.html#report-1.10",
    "href": "labs/lab3.html#report-1.10",
    "title": "Lab 3",
    "section": "Report 1.10",
    "text": "Report 1.10\nWhat limitations exist as a result of the data analysis?"
  },
  {
    "objectID": "labs/lab1.html",
    "href": "labs/lab1.html",
    "title": "Lab 1",
    "section": "",
    "text": "For Lab 1, you should submit your solutions to Canvas as a .pdf and include your RMarkdown syntax (i.e., you can render an html file or push your code to GitHub, or share the .Rmd file). Please see the Preparing Lab Reports section at the bottom of this assignment.\nThe Lab 1 assignment will reintroduce you to data in the critstats package and add a second data set known as gss_cat. However, for the lab 1 and paper 2 assignment, you may use your own data set and we can work together to find you a data set that is ready to be analyzed. For new users, I would recommend against data that needs to be cleaned. However, by visiting the case study 2 files, you can learn more about cleaning your data.\nThat said, during this lab, you will also return to some of the functions we have used before to clean variables and manipulate data frames. Your analyses in this lab should focus on a bivariate analysis between two categorical variables, two numeric variables, or one numeric variable and one categorical variable.",
    "crumbs": [
      "Appendix",
      "Labs",
      "Lab 1"
    ]
  },
  {
    "objectID": "labs/lab1.html#learning-activities",
    "href": "labs/lab1.html#learning-activities",
    "title": "Lab 1",
    "section": "Learning Activities",
    "text": "Learning Activities\nBy the end of this lab you will be able to:\n\nLocate data sets in the critstats package\nDevelop an original research question\nClean and manipulate data for analysis\nExamine the relationship between two variables\n\nYou should submit your final output on Canvas.",
    "crumbs": [
      "Appendix",
      "Labs",
      "Lab 1"
    ]
  },
  {
    "objectID": "labs/lab1.html#task-0.1-create-a-new-rmarkdown",
    "href": "labs/lab1.html#task-0.1-create-a-new-rmarkdown",
    "title": "Lab 1",
    "section": "Task 0.1: Create a new RMarkdown",
    "text": "Task 0.1: Create a new RMarkdown\nIn your R session, navigate to: File &gt; New File &gt; R Markdown. Create a new markdown file using an appropriate title.",
    "crumbs": [
      "Appendix",
      "Labs",
      "Lab 1"
    ]
  },
  {
    "objectID": "labs/lab1.html#task-0.2-check-your-working-directory",
    "href": "labs/lab1.html#task-0.2-check-your-working-directory",
    "title": "Lab 1",
    "section": "Task 0.2: Check your working directory",
    "text": "Task 0.2: Check your working directory\nCheck your working directory by typing getwd().\nIf you are not in the desired directory, you can change your directory using the associated path. This path should be the same as the project folder that you plan to work out of for the next several weeks.\n\n# insert your desired path in the parenthesis and remove the #\n# setwd(\"/your/working/directory/goes/here\") \n\nYou can add a new sub-folder manually or under the Files tab in the RStudio IDE.",
    "crumbs": [
      "Appendix",
      "Labs",
      "Lab 1"
    ]
  },
  {
    "objectID": "labs/lab1.html#task-0.3-write-a-preamble",
    "href": "labs/lab1.html#task-0.3-write-a-preamble",
    "title": "Lab 1",
    "section": "Task 0.3: Write a preamble",
    "text": "Task 0.3: Write a preamble\nName:  Assignment: Lab 2 Date:  Purpose:",
    "crumbs": [
      "Appendix",
      "Labs",
      "Lab 1"
    ]
  },
  {
    "objectID": "labs/lab1.html#task-0.4-packages-and-libraries",
    "href": "labs/lab1.html#task-0.4-packages-and-libraries",
    "title": "Lab 1",
    "section": "Task 0.4: Packages and libraries",
    "text": "Task 0.4: Packages and libraries\n\n# install the tidyverse package\ninstall.packages(\"tidyverse\", repos = \"http://cran.us.r-project.org\")\n\n# load the libraries needed for today's analyses\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(critstats)\n\n## update packages if needed; remove # to run code\n# update.packages(\"package-name\")",
    "crumbs": [
      "Appendix",
      "Labs",
      "Lab 1"
    ]
  },
  {
    "objectID": "labs/lab1.html#report-1.1",
    "href": "labs/lab1.html#report-1.1",
    "title": "Lab 1",
    "section": "Report 1.1",
    "text": "Report 1.1\nWhat data set have you decided to use?",
    "crumbs": [
      "Appendix",
      "Labs",
      "Lab 1"
    ]
  },
  {
    "objectID": "labs/lab1.html#report-1.2",
    "href": "labs/lab1.html#report-1.2",
    "title": "Lab 1",
    "section": "Report 1.2",
    "text": "Report 1.2\nWhich two variables from your data set will be analyzed?",
    "crumbs": [
      "Appendix",
      "Labs",
      "Lab 1"
    ]
  },
  {
    "objectID": "labs/lab1.html#report-1.3",
    "href": "labs/lab1.html#report-1.3",
    "title": "Lab 1",
    "section": "Report 1.3",
    "text": "Report 1.3\nWhat is your research question?",
    "crumbs": [
      "Appendix",
      "Labs",
      "Lab 1"
    ]
  },
  {
    "objectID": "labs/lab1.html#report-1.4",
    "href": "labs/lab1.html#report-1.4",
    "title": "Lab 1",
    "section": "Report 1.4",
    "text": "Report 1.4\nWhat is your data analysis plan? Please be descriptive.",
    "crumbs": [
      "Appendix",
      "Labs",
      "Lab 1"
    ]
  },
  {
    "objectID": "labs/lab1.html#report-1.5",
    "href": "labs/lab1.html#report-1.5",
    "title": "Lab 1",
    "section": "Report 1.5",
    "text": "Report 1.5\nWhat are some potential limitations for your analysis?",
    "crumbs": [
      "Appendix",
      "Labs",
      "Lab 1"
    ]
  },
  {
    "objectID": "labs/lab1.html#report-1.6",
    "href": "labs/lab1.html#report-1.6",
    "title": "Lab 1",
    "section": "Report 1.6",
    "text": "Report 1.6\nDoes your data contain missing values? If so, how have you dealt with these values?",
    "crumbs": [
      "Appendix",
      "Labs",
      "Lab 1"
    ]
  },
  {
    "objectID": "labs/lab1.html#report-1.7",
    "href": "labs/lab1.html#report-1.7",
    "title": "Lab 1",
    "section": "Report 1.7",
    "text": "Report 1.7\nPlease include all code used to clean and manipulate the variables.",
    "crumbs": [
      "Appendix",
      "Labs",
      "Lab 1"
    ]
  },
  {
    "objectID": "labs/lab1.html#report-1.8",
    "href": "labs/lab1.html#report-1.8",
    "title": "Lab 1",
    "section": "Report 1.8",
    "text": "Report 1.8\nWhat relationship, if any, exists between the two variables?",
    "crumbs": [
      "Appendix",
      "Labs",
      "Lab 1"
    ]
  },
  {
    "objectID": "labs/lab1.html#report-1.9",
    "href": "labs/lab1.html#report-1.9",
    "title": "Lab 1",
    "section": "Report 1.9",
    "text": "Report 1.9\nHow do these findings relate to your research question and theory?",
    "crumbs": [
      "Appendix",
      "Labs",
      "Lab 1"
    ]
  },
  {
    "objectID": "labs/lab1.html#report-1.10",
    "href": "labs/lab1.html#report-1.10",
    "title": "Lab 1",
    "section": "Report 1.10",
    "text": "Report 1.10\nWhat limitations exist as a result of the data analysis?",
    "crumbs": [
      "Appendix",
      "Labs",
      "Lab 1"
    ]
  },
  {
    "objectID": "labs/lab2.html",
    "href": "labs/lab2.html",
    "title": "Lab 2",
    "section": "",
    "text": "For Lab 2, you may submit your solutions to Canvas as a .pdf or an RMarkdown.\nPlease see the Preparing Lab Reports at the bottom of the lab 1 assignment.",
    "crumbs": [
      "Appendix",
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "labs/lab2.html#learning-objectives",
    "href": "labs/lab2.html#learning-objectives",
    "title": "Lab 2",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nThe Lab 2 assignment focuses on you exploring any two variables located in one of the data frames in the critstats package or in gss_cat, or a data set that you have identified and would like to explore. You will also return to some of the functions we have used before to clean variables and manipulate data frames. Your analyses in this lab should focus on a bivariate analysis between two categorical variables, two numeric variables, or one numeric variable and one categorical variable.",
    "crumbs": [
      "Appendix",
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "labs/lab2.html#learning-activities",
    "href": "labs/lab2.html#learning-activities",
    "title": "Lab 2",
    "section": "Learning Activities",
    "text": "Learning Activities\nBy the end of this lab you will be able to:\n\nLocate data sets in the critstats package, or use gss_cat\nDevelop an original research question\nClean and manipulate data for analysis\nExamine the relationship between two variables\n\nYou should submit your final output on Canvas here.",
    "crumbs": [
      "Appendix",
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "labs/lab2.html#task-0.1-create-a-new-rmarkdown",
    "href": "labs/lab2.html#task-0.1-create-a-new-rmarkdown",
    "title": "Lab 2",
    "section": "Task 0.1: Create a new RMarkdown",
    "text": "Task 0.1: Create a new RMarkdown\nIn your R session, navigate to: File &gt; New File &gt; R Markdown. Create a new markdown file using an appropriate title.",
    "crumbs": [
      "Appendix",
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "labs/lab2.html#task-0.2-check-your-working-directory",
    "href": "labs/lab2.html#task-0.2-check-your-working-directory",
    "title": "Lab 2",
    "section": "Task 0.2: Check your working directory",
    "text": "Task 0.2: Check your working directory\nCheck your working directory by typing getwd().\nIf you are not in the desired directory, you can change your directory using the associated path. This path should be the same as the project folder that you plan to work out of for the next several weeks.\n\n# insert your desired path in the parenthesis and remove the #\n# setwd(\"/your/working/directory/goes/here\") \n\nYou can add a new sub-folder manually or under the Files tab in the RStudio IDE.",
    "crumbs": [
      "Appendix",
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "labs/lab2.html#task-0.3-write-a-preamble",
    "href": "labs/lab2.html#task-0.3-write-a-preamble",
    "title": "Lab 2",
    "section": "Task 0.3: Write a preamble",
    "text": "Task 0.3: Write a preamble\n\n## Name: &lt;include your full name&gt;\n## Assignment: Lab 2\n## Date: &lt;here you may want to add a date&gt;\n## Purpose: &lt;insert the goals or purpose of the RScript&gt;",
    "crumbs": [
      "Appendix",
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "labs/lab2.html#task-0.4-packages-and-libraries",
    "href": "labs/lab2.html#task-0.4-packages-and-libraries",
    "title": "Lab 2",
    "section": "Task 0.4: Packages and libraries",
    "text": "Task 0.4: Packages and libraries\n\n# install the tidyverse package\ninstall.packages(\"tidyverse\", repos = \"http://cran.us.r-project.org\")\n\n# load the libraries needed for today's analyses\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(critstats)\n\n## update packages if needed; remove # to run code\n# update.packages(\"package-name\")",
    "crumbs": [
      "Appendix",
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "labs/lab2.html#report-1.1",
    "href": "labs/lab2.html#report-1.1",
    "title": "Lab 2",
    "section": "Report 1.1",
    "text": "Report 1.1\nWhat data set have you decided to use?",
    "crumbs": [
      "Appendix",
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "labs/lab2.html#report-1.2",
    "href": "labs/lab2.html#report-1.2",
    "title": "Lab 2",
    "section": "Report 1.2",
    "text": "Report 1.2\nWhich two variables from your data set will be analyzed?",
    "crumbs": [
      "Appendix",
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "labs/lab2.html#report-1.3",
    "href": "labs/lab2.html#report-1.3",
    "title": "Lab 2",
    "section": "Report 1.3",
    "text": "Report 1.3\nWhat is your research question?",
    "crumbs": [
      "Appendix",
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "labs/lab2.html#report-1.4",
    "href": "labs/lab2.html#report-1.4",
    "title": "Lab 2",
    "section": "Report 1.4",
    "text": "Report 1.4\nWhat is your data analysis plan? Please be descriptive.",
    "crumbs": [
      "Appendix",
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "labs/lab2.html#report-1.5",
    "href": "labs/lab2.html#report-1.5",
    "title": "Lab 2",
    "section": "Report 1.5",
    "text": "Report 1.5\nWhat are some potential limitations for your analysis?",
    "crumbs": [
      "Appendix",
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "labs/lab2.html#report-1.6",
    "href": "labs/lab2.html#report-1.6",
    "title": "Lab 2",
    "section": "Report 1.6",
    "text": "Report 1.6\nDoes your data contain missing values? If so, how have you dealt with these values?",
    "crumbs": [
      "Appendix",
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "labs/lab2.html#report-1.7",
    "href": "labs/lab2.html#report-1.7",
    "title": "Lab 2",
    "section": "Report 1.7",
    "text": "Report 1.7\nPlease include all code used to clean and manipulate the variables.",
    "crumbs": [
      "Appendix",
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "labs/lab2.html#report-1.8",
    "href": "labs/lab2.html#report-1.8",
    "title": "Lab 2",
    "section": "Report 1.8",
    "text": "Report 1.8\nWhat relationship, if any, exists between the two variables?",
    "crumbs": [
      "Appendix",
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "labs/lab2.html#report-1.9",
    "href": "labs/lab2.html#report-1.9",
    "title": "Lab 2",
    "section": "Report 1.9",
    "text": "Report 1.9\nHow do these findings relate to your research question and theory?",
    "crumbs": [
      "Appendix",
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "labs/lab2.html#report-1.10",
    "href": "labs/lab2.html#report-1.10",
    "title": "Lab 2",
    "section": "Report 1.10",
    "text": "Report 1.10\nWhat limitations exist as a result of the data analysis?",
    "crumbs": [
      "Appendix",
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "weeks/week09-slides.html#part-i-context",
    "href": "weeks/week09-slides.html#part-i-context",
    "title": "DATA 202 - Week 9",
    "section": "Part I: Context",
    "text": "Part I: Context\nWhat is your hypothesis? We begin today’s class with a note about the different studies we are conducting and our hypotheses.\nHypothesis testing requires a solid theory to make sense of the relevant options to develop a hypothesis.\nAs a fundamental statistical tool, it is important to be critical wen dealing with social issues and the standars of hypothesis testing. QuantCrit (or critical quantitative methodologies) have been developed as a tool to help guide researchers in the quantitative social sciences, and in education, to consider how hypotheses can reinforce or challenge existing power structues and biases."
  },
  {
    "objectID": "weeks/week09-slides.html#part-ii-content",
    "href": "weeks/week09-slides.html#part-ii-content",
    "title": "DATA 202 - Week 9",
    "section": "Part II: Content",
    "text": "Part II: Content\nNull and Alternative Hypotheses\n    Null hypothesis (H₀): Statement of no effect or no difference\n    \n    Alternative hypothesis (H₁): Statement of an effect or difference"
  },
  {
    "objectID": "weeks/week09-slides.html#part-iii-code",
    "href": "weeks/week09-slides.html#part-iii-code",
    "title": "DATA 202 - Week 9",
    "section": "Part III: Code",
    "text": "Part III: Code\nTwo Categorical Variables: Chi-Square Test of Independence\n\n# Example: Testing association between gender and voting preference\n\n# Create sample data\ngender &lt;- c(rep(\"Male\", 100), rep(\"Female\", 100))\nvote &lt;- c(rep(\"Party A\", 55), rep(\"Party B\", 45), rep(\"Party A\", 65), rep(\"Party B\", 35))\ndata &lt;- data.frame(gender, vote)\n\n# Perform chi-square test\nresult &lt;- chisq.test(table(data$gender, data$vote))\n\n# Print results\nprint(result)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  table(data$gender, data$vote)\nX-squared = 1.6875, df = 1, p-value = 0.1939\n\n# Interpret results\nif (result$p.value &lt; 0.05) {\n  print(\"There is a significant association between gender and voting preference.\")\n} else {\n  print(\"There is no significant association between gender and voting preference.\")\n}\n\n[1] \"There is no significant association between gender and voting preference.\""
  },
  {
    "objectID": "weeks/week09.html",
    "href": "weeks/week09.html",
    "title": "DATA 202 - Week 9",
    "section": "",
    "text": "What is your hypothesis? We begin today’s class with a note about the different studies we are conducting and our hypotheses.\nHypothesis testing requires a solid theory to make sense of the relevant options to develop a hypothesis.\nAs a fundamental statistical tool, it is important to be critical wen dealing with social issues and the standars of hypothesis testing. QuantCrit (or critical quantitative methodologies) have been developed as a tool to help guide researchers in the quantitative social sciences, and in education, to consider how hypotheses can reinforce or challenge existing power structues and biases.\n\nSome basic but important real-world examples:\nGender pay gap: Testing whether there's a significant difference in salaries between men and women in a particular industry.\n\nRacial disparities in healthcare: Examining if there's a significant difference in health outcomes between different racial groups.\n\nEducational achievement: Investigating the relationship between socioeconomic status and academic performance.\nIn each of these cases, it’s essential to consider:\n\nWho formulated the hypothesis and why?\nWhat assumptions are built into the statistical methods?\nHow might the results be interpreted or misinterpreted?\nWhat are the potential consequences of the findings?\n\n\n\n\nAs you think about your own work, what are some of the main hypothesis that you hold as it relates to the relationship between your study variables? What are the variable types? How might you measure and make sense of the relationship between the variables?",
    "crumbs": [
      "Weekly materials",
      "Week 9"
    ]
  },
  {
    "objectID": "weeks/week09.html#part-i-context",
    "href": "weeks/week09.html#part-i-context",
    "title": "DATA 202 - Week 9",
    "section": "",
    "text": "What is your hypothesis? We begin today’s class with a note about the different studies we are conducting and our hypotheses.\nHypothesis testing requires a solid theory to make sense of the relevant options to develop a hypothesis.\nAs a fundamental statistical tool, it is important to be critical wen dealing with social issues and the standars of hypothesis testing. QuantCrit (or critical quantitative methodologies) have been developed as a tool to help guide researchers in the quantitative social sciences, and in education, to consider how hypotheses can reinforce or challenge existing power structues and biases.\n\nSome basic but important real-world examples:\nGender pay gap: Testing whether there's a significant difference in salaries between men and women in a particular industry.\n\nRacial disparities in healthcare: Examining if there's a significant difference in health outcomes between different racial groups.\n\nEducational achievement: Investigating the relationship between socioeconomic status and academic performance.\nIn each of these cases, it’s essential to consider:\n\nWho formulated the hypothesis and why?\nWhat assumptions are built into the statistical methods?\nHow might the results be interpreted or misinterpreted?\nWhat are the potential consequences of the findings?\n\n\n\n\nAs you think about your own work, what are some of the main hypothesis that you hold as it relates to the relationship between your study variables? What are the variable types? How might you measure and make sense of the relationship between the variables?",
    "crumbs": [
      "Weekly materials",
      "Week 9"
    ]
  },
  {
    "objectID": "weeks/week09.html#part-ii-content",
    "href": "weeks/week09.html#part-ii-content",
    "title": "DATA 202 - Week 9",
    "section": "Part II: Content",
    "text": "Part II: Content\nNull and Alternative Hypotheses\n    Null hypothesis (H₀): Statement of no effect or no difference\n    \n    Alternative hypothesis (H₁): Statement of an effect or difference\n\nTest Statistic\n    A measure that allows us to quantify the difference between the observed data and what we'd expect under the null hypothesis\n    \n    Common test statistics: z-score, t-statistic, F-statistic, chi-square statistic\n\nProbability Distribution\n    The distribution of the test statistic under the null hypothesis\n    \n    Examples: Normal distribution, t-distribution, F-distribution, chi-square distribution\n\np-value\nThe probability of obtaining test results at least as extreme as the observed results, assuming the null hypothesis is true\n\nCalculated as: p-value = P(test statistic ≥ observed value | H₀ is true)\n\nSignificance Level (α)\nThe threshold below which we reject the null hypothesis\n\nCommon values: 0.05, 0.01\n\nDecision Rule\nReject H₀ if p-value ≤ α\n\nFail to reject H₀ if p-value &gt; α\n\nType I and Type II Errors\nType I error: Rejecting H₀ when it's actually true (false positive)\n\nType II error: Failing to reject H₀ when it's actually false (false negative)\n\nPower\nThe probability of correctly rejecting a false null hypothesis\n\nPower = 1 - P(Type II error)",
    "crumbs": [
      "Weekly materials",
      "Week 9"
    ]
  },
  {
    "objectID": "weeks/week09.html#part-iii-code",
    "href": "weeks/week09.html#part-iii-code",
    "title": "DATA 202 - Week 9",
    "section": "Part III: Code",
    "text": "Part III: Code\n\nTwo Categorical Variables: Chi-Square Test of Independence\n\n# Example: Testing association between gender and voting preference\n\n# Create sample data\ngender &lt;- c(rep(\"Male\", 100), rep(\"Female\", 100))\nvote &lt;- c(rep(\"Party A\", 55), rep(\"Party B\", 45), rep(\"Party A\", 65), rep(\"Party B\", 35))\ndata &lt;- data.frame(gender, vote)\n\n# Perform chi-square test\nresult &lt;- chisq.test(table(data$gender, data$vote))\n\n# Print results\nprint(result)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  table(data$gender, data$vote)\nX-squared = 1.6875, df = 1, p-value = 0.1939\n\n# Interpret results\nif (result$p.value &lt; 0.05) {\n  print(\"There is a significant association between gender and voting preference.\")\n} else {\n  print(\"There is no significant association between gender and voting preference.\")\n}\n\n[1] \"There is no significant association between gender and voting preference.\"\n\n\n\n\n\nTwo Numeric Variables: Pearson Correlation Test\n\n# Example: Testing correlation between study hours and exam scores\n\n# Create sample data\nstudy_hours &lt;- rnorm(100, mean = 20, sd = 5)\nexam_scores &lt;- 2 * study_hours + rnorm(100, mean = 0, sd = 10)\ndata &lt;- data.frame(study_hours, exam_scores)\n\n# Perform correlation test\nresult &lt;- cor.test(data$study_hours, data$exam_scores)\n\n# Print results\nprint(result)\n\n\n    Pearson's product-moment correlation\n\ndata:  data$study_hours and data$exam_scores\nt = 10.318, df = 98, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.6118816 0.8040383\nsample estimates:\n      cor \n0.7215772 \n\n# Interpret results\nif (result$p.value &lt; 0.05) {\n  print(paste(\"There is a significant correlation between study hours and exam scores. Correlation coefficient:\", round(result$estimate, 2)))\n} else {\n  print(\"There is no significant correlation between study hours and exam scores.\")\n}\n\n[1] \"There is a significant correlation between study hours and exam scores. Correlation coefficient: 0.72\"\n\n\n\n\n\nOne Categorical and One Numeric Variable: Independent Samples t-test\n\n# Example: Testing difference in exam scores between two teaching methods\n\n# Create sample data\nmethod &lt;- c(rep(\"Method A\", 50), rep(\"Method B\", 50))\nscores &lt;- c(rnorm(50, mean = 75, sd = 10), rnorm(50, mean = 80, sd = 10))\ndata &lt;- data.frame(method, scores)\n\n# Perform t-test\nresult &lt;- t.test(scores ~ method, data = data)\n\n# Print results\nprint(result)\n\n\n    Welch Two Sample t-test\n\ndata:  scores by method\nt = -1.5882, df = 97.815, p-value = 0.1155\nalternative hypothesis: true difference in means between group Method A and group Method B is not equal to 0\n95 percent confidence interval:\n -8.173811  0.906603\nsample estimates:\nmean in group Method A mean in group Method B \n              74.64852               78.28212 \n\n# Interpret results\nif (result$p.value &lt; 0.05) {\n  print(\"There is a significant difference in exam scores between the two teaching methods.\")\n} else {\n  print(\"There is no significant difference in exam scores between the two teaching methods.\")\n}\n\n[1] \"There is no significant difference in exam scores between the two teaching methods.\"",
    "crumbs": [
      "Weekly materials",
      "Week 9"
    ]
  },
  {
    "objectID": "weeks/week12-slides.html#part-i-context",
    "href": "weeks/week12-slides.html#part-i-context",
    "title": "DATA 202 - Week 12",
    "section": "Part I: Context",
    "text": "Part I: Context\nThis week we focus on more examples from our darta sources; we’ll take a closer look at the General Social Survey data, or GSS.\nThe data is located in the gssr package in R (see part III). You should become very familiar with the GSS data. Please explore the website as we will be prioritizing the use of the GSS data for the remainder of our course. The use of the GSS data will allow us to consider the meaning of social justice in the context of attitudes and beliefs around social issues.\nAhead of modeling multiple variables, we’ll examine the importance of underlying theoretical anlayses ahead of solely doing statistical investigations."
  },
  {
    "objectID": "weeks/week12-slides.html#part-ii-content",
    "href": "weeks/week12-slides.html#part-ii-content",
    "title": "DATA 202 - Week 12",
    "section": "Part II: Content",
    "text": "Part II: Content\nA tradition: Fitting a straight line\nIn statistics, we often inquire about the relationship between two variables, \\(X\\) and \\(Y\\).\nThese variables are sets that contain values (observations) as noted before. To launch our discussion, we will begin with a sample line.\n\nplot(0:10, type=\"l\", main = \"A sample line\", xlab = \"X-axis\", ylab = \"Y-axis\")"
  },
  {
    "objectID": "weeks/week12-slides.html#part-iii-code",
    "href": "weeks/week12-slides.html#part-iii-code",
    "title": "DATA 202 - Week 12",
    "section": "Part III: Code",
    "text": "Part III: Code\nThis week we’ll explore the GSS and ANES data in more detail. This is a reminder to deepen your understanding of the data set you will use based on the notes provided below from week 11.\nGeneral Social Survey\nThis week we’ll return to our examination of the General Social Survey (GSS) data.\nAs a first task, please identify up to two or three variables that you can utilize to follow along with your analysis.\nNext up: Week 13\n\n\n\n\nCourse Data GitHub"
  },
  {
    "objectID": "weeks/week12.html",
    "href": "weeks/week12.html",
    "title": "DATA 202 - Week 12",
    "section": "",
    "text": "We explore the case of bivariate regression when attempting to measure and model conceptions of in/justice.",
    "crumbs": [
      "Weekly materials",
      "Week 12"
    ]
  },
  {
    "objectID": "weeks/week12.html#part-i-context",
    "href": "weeks/week12.html#part-i-context",
    "title": "DATA 202 - Week 12",
    "section": "Part I: Context",
    "text": "Part I: Context\nThis week we focus on more examples from our darta sources; we’ll take a closer look at the General Social Survey data, or GSS.\nThe data is located in the gssr package in R (see part III). You should become very familiar with the GSS data. Please explore the website as we will be prioritizing the use of the GSS data for the remainder of our course. The use of the GSS data will allow us to consider the meaning of social justice in the context of attitudes and beliefs around social issues.\nAhead of modeling multiple variables, we’ll examine the importance of underlying theoretical anlayses ahead of solely doing statistical investigations.\n\nLet us begin with a set of three variables: \\(x\\), \\(y\\), and \\(z\\).\nWe will assume that there is a hypothesized association between all three variables. However, let us also assume that we have not yet taken time to properly structure the relationships between the variables. Namely, we have yet to consider if \\(z\\), for example, is a mediator, moderator, or spurious variable.\nLet us take a look at the scatterplots between each pair of variables.\n\nplot(x, y)\n\n\n\n\n\n\n\ncor(x, y)\n\n[1] -0.09107752\n\n\nTake note of the value of the correlation coefficient.\n\nLet us take a look at the scatterplots between each pair of variables.\n\nplot(x, z)\n\n\n\n\n\n\n\ncor(x, z)\n\n[1] 0.9284953\n\n\nTake note of the value of the correlation coefficient.\n\nLet us take a look at the scatterplots between each pair of variables.\n\nplot(y, z)\n\n\n\n\n\n\n\ncor(y, z)\n\n[1] -0.04660941\n\n\nTake note of the value of the correlation coefficient.\n\n\nRegression assumptions\nIn our previous lectures, we have discussed the function of assumptions and principles in regression analysis. In progressing towards different types of tests, it is important to consider the specific assumptions for any given test.\nAnalyzing relationships among social science variables has an assumptions of linearity. However, this assumptions is not always correct. The adoption of this assumption is based on a host of factors. Most notably, that many relationships have been found to be linear when considered in the empirical sense.\nSome additional assumptions are as follows:\n\nThe sample is representative of the population\nThe variables of interest are normally distributed\nThere are no outliers in the data\nIndependence\nThere is a linear relationship between the independent variable(s) and dependent variable(s)",
    "crumbs": [
      "Weekly materials",
      "Week 12"
    ]
  },
  {
    "objectID": "weeks/week12.html#part-ii-content",
    "href": "weeks/week12.html#part-ii-content",
    "title": "DATA 202 - Week 12",
    "section": "Part II: Content",
    "text": "Part II: Content\n\nA tradition: Fitting a straight line\nIn statistics, we often inquire about the relationship between two variables, \\(X\\) and \\(Y\\).\nThese variables are sets that contain values (observations) as noted before. To launch our discussion, we will begin with a sample line.\n\nplot(0:10, type=\"l\", main = \"A sample line\", xlab = \"X-axis\", ylab = \"Y-axis\")\n\n\n\n\n\n\n\n\n\n\nMatheamtics and exact relationships\nIn mathematics, two variables, \\(x\\) and \\(y\\) may be related to each other in various ways. These relationships form the basis for many inquiries in statistics. However, mathematical modeling most often involves identifying the parameters in which we can use to assess the validity of the stated relationship. In this way, we examine the difference between exact and inexact relationships.\n\n\nA straight line\nThe line in the graphic above follows the standard notation: \\[y = a + bX\\]\n\n\n\n\nStatistical modeling and inexact relationships\nRegression analysis is a standard analysis in many statistical studies.\nThere are many different types of regression. We’ll continue with our exploration of bivariate regression analysis and focus on some of the base assumptions as it relates to study development.\nWe will begin by looking at the underlying assumptions of regression analysis.\nThese assumptions are the technical (or structural) components of our analyses, and should be checked at the initiation of a research study, starting with data collection or understanding how data was collected if it is a secondary analysis.\n\n\nAnalyzing two numeric variables\nLet us take two variables, \\(x\\) and \\(y\\).\n\ncor(x, y)\n\n[1] 0.7293926\n\n\n\n\n# Fit simple linear regression model\nmodel &lt;- lm(y ~ x)\n\n\n\n# Examine regression outputs\nsummary(model)\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4027 -1.0071  0.1154  1.0017  5.8115 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.07185    0.20452   0.351    0.726    \nx            2.36515    0.22408  10.555   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.035 on 98 degrees of freedom\nMultiple R-squared:  0.532, Adjusted R-squared:  0.5272 \nF-statistic: 111.4 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n# Check regression coefficients\ncoeff &lt;- coef(model)\ncoeff\n\n(Intercept)           x \n 0.07184615  2.36514669 \n\n\n\n\n# Check R-squared value\nrsq &lt;- summary(model)$r.squared\nrsq\n\n[1] 0.5320136\n\n\n\n\n# Generate predictions\npred &lt;- predict(model) # call this object to show the predicted values of the model\n\n\n\n# Plot the regression line\nplot(x, y)\nabline(model, col=\"blue\")\n\n\n\n\n\n\n\n\n\n\n# Check residuals\nresids &lt;- residuals(model)\nplot(x, resids)\n\n\n\n\n\n\n\n\n\n\n# Diagnostic plots\npar(mfrow=c(2,2))\nplot(model)\n\n\n\n\n\n\n\n\n\n\n# Check significance of predictor\nanova(model)\n\nAnalysis of Variance Table\n\nResponse: y\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nx          1 461.44  461.44  111.41 &lt; 2.2e-16 ***\nResiduals 98 405.91    4.14                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\nAnalyzing a categorical and a numeric variable\n\n\ndesc(df$income)\n\n [1] -25000 -35000 -42000 -31000 -27000 -38000 -29000 -40000 -32000 -39000\n\n\n\n\n# Bar plot of the categorical variable (race)\nggplot(df, aes(x = race)) +\n  geom_bar(fill = \"steelblue\") +\n  labs(x = \"Race\", y = \"Count\") +\n  ggtitle(\"Distribution of Races\")\n\n\n\n\n\n\n\n\n\n# Box plot of income by race\nggplot(df, aes(x = race, y = income)) +\n  geom_boxplot(fill = \"lightgray\", color = \"steelblue\") +\n  labs(x = \"Race\", y = \"Income\") +\n  ggtitle(\"Income by Race\")\n\n\n\n\n\n\n\n\n\n\nAnalyzing two categorical variables\nIn previous lectures, we discussed some methods to hypothesize around the relationship between two variables that are categorical in nature.",
    "crumbs": [
      "Weekly materials",
      "Week 12"
    ]
  },
  {
    "objectID": "weeks/week12.html#part-iii-code",
    "href": "weeks/week12.html#part-iii-code",
    "title": "DATA 202 - Week 12",
    "section": "Part III: Code",
    "text": "Part III: Code\nThis week we’ll explore the GSS and ANES data in more detail. This is a reminder to deepen your understanding of the data set you will use based on the notes provided below from week 11.\n\nGeneral Social Survey\nThis week we’ll return to our examination of the General Social Survey (GSS) data.\nAs a first task, please identify up to two or three variables that you can utilize to follow along with your analysis.\n\n\nNext up: Week 13",
    "crumbs": [
      "Weekly materials",
      "Week 12"
    ]
  },
  {
    "objectID": "weeks/week06-slides.html#part-i-context",
    "href": "weeks/week06-slides.html#part-i-context",
    "title": "DATA 202 - Week 6",
    "section": "Part I: Context",
    "text": "Part I: Context"
  },
  {
    "objectID": "weeks/week06-slides.html#part-ii-content",
    "href": "weeks/week06-slides.html#part-ii-content",
    "title": "DATA 202 - Week 6",
    "section": "Part II: Content",
    "text": "Part II: Content\nThis week’s topics focus on bivariate analysis.\nThe goal of a bivariate analysis is to understand the relationship between two variables.\nThere are a few common ways to perform bivariate analysis:\n\nEstimating differences in proportions\nScatter plots\nCorrelation coefficients\nSimple linear regression"
  },
  {
    "objectID": "weeks/week06-slides.html#part-iii-code",
    "href": "weeks/week06-slides.html#part-iii-code",
    "title": "DATA 202 - Week 6",
    "section": "Part III: Code",
    "text": "Part III: Code"
  },
  {
    "objectID": "weeks/week07.html",
    "href": "weeks/week07.html",
    "title": "DATA 202 - Week 6",
    "section": "",
    "text": "Building theories and empirical inquiries around various injustices requires a solid foundation.\n\nOne method is to read recent articles in peer-reviewed journals.\nAnother method is to explore the various ways one may view injustice.\nFrom a logical perspective, you may test the validity of certain claims.\n\nHow should we define social justice?\n\nIdentify two to three definitions of social justice to share.\n\nLocate a few open source articles or periodicals.\n\nWhat are the similarities between the different definitions?\nWhat are the differences between the different definitions?\n\n\n\n\n\nThere are many different frameworks that have been developed to examine social justice.\nNorth (2006), as one example, discuss three spheres of social justice in the article “More Than Words? Delving Into the Substantive Meaning(s) of Social Justice in Education.” The abstract reads as follows:\n\n“At the dawning of the 21st century, the term”social justice” is appearing in numerous public texts and discourses throughout the field of education. However, and as Gewirtz argued in 1998, the conceptual underpinnings of this catchphrase frequently remain tacit or underexplored. This article elaborates Gewirtz’s earlier “mapping” of social justice theories by examining the tensions that emerge when various conceptualizations of social justice collide and, in turn, their implications for the field of education. By presenting a model of the complex, fraught interactions among diverse claims about social justice, the author seeks to promote continued dialogue and reflexivity on the purposes and possibilities of education for social justice.”\n\n\n\n\nThree Spheres of Social Justice by North (2006)\n\n\nIn her analysis, North deals specifically with three conceptions of social justice and their implications for educational research and practice. These frameworks, however, can be extended over to other fields of study. Consider how this framework might apply to your area of study.\n\nThere are different ways to consider these interrelated systems in your theory construction.\n\n\n\nMacro: Large-scale analyses, typically on systems or observations in the aggregate\nMicro: Smaller-scale analyses, typically on individuals or localized contexts\n\n\n\n\n\nSameness: Considering homogeneous structures or characteristics; similarity\nDifference: Considering heterogeneous structures or characteristics; non-uniformity\n\n\n\n\n\nRedistribution: Primary considerations in economic or material conditions\nRecognition: Primary considerations in cultural or social conditions",
    "crumbs": [
      "Weekly materials",
      "Week 7"
    ]
  },
  {
    "objectID": "weeks/week07.html#part-i-context",
    "href": "weeks/week07.html#part-i-context",
    "title": "DATA 202 - Week 6",
    "section": "",
    "text": "Building theories and empirical inquiries around various injustices requires a solid foundation.\n\nOne method is to read recent articles in peer-reviewed journals.\nAnother method is to explore the various ways one may view injustice.\nFrom a logical perspective, you may test the validity of certain claims.\n\nHow should we define social justice?\n\nIdentify two to three definitions of social justice to share.\n\nLocate a few open source articles or periodicals.\n\nWhat are the similarities between the different definitions?\nWhat are the differences between the different definitions?\n\n\n\n\n\nThere are many different frameworks that have been developed to examine social justice.\nNorth (2006), as one example, discuss three spheres of social justice in the article “More Than Words? Delving Into the Substantive Meaning(s) of Social Justice in Education.” The abstract reads as follows:\n\n“At the dawning of the 21st century, the term”social justice” is appearing in numerous public texts and discourses throughout the field of education. However, and as Gewirtz argued in 1998, the conceptual underpinnings of this catchphrase frequently remain tacit or underexplored. This article elaborates Gewirtz’s earlier “mapping” of social justice theories by examining the tensions that emerge when various conceptualizations of social justice collide and, in turn, their implications for the field of education. By presenting a model of the complex, fraught interactions among diverse claims about social justice, the author seeks to promote continued dialogue and reflexivity on the purposes and possibilities of education for social justice.”\n\n\n\n\nThree Spheres of Social Justice by North (2006)\n\n\nIn her analysis, North deals specifically with three conceptions of social justice and their implications for educational research and practice. These frameworks, however, can be extended over to other fields of study. Consider how this framework might apply to your area of study.\n\nThere are different ways to consider these interrelated systems in your theory construction.\n\n\n\nMacro: Large-scale analyses, typically on systems or observations in the aggregate\nMicro: Smaller-scale analyses, typically on individuals or localized contexts\n\n\n\n\n\nSameness: Considering homogeneous structures or characteristics; similarity\nDifference: Considering heterogeneous structures or characteristics; non-uniformity\n\n\n\n\n\nRedistribution: Primary considerations in economic or material conditions\nRecognition: Primary considerations in cultural or social conditions",
    "crumbs": [
      "Weekly materials",
      "Week 7"
    ]
  },
  {
    "objectID": "weeks/week07.html#part-ii-content",
    "href": "weeks/week07.html#part-ii-content",
    "title": "DATA 202 - Week 6",
    "section": "Part II: Content",
    "text": "Part II: Content\nThis week’s topics focus on bivariate analysis.\nThe goal of a bivariate analysis is to understand the relationship between two variables.\nThere are a few common ways to perform bivariate analysis:\n\nEstimating differences in proportions\nScatter plots\nCorrelation coefficients\nSimple linear regression\n\n\n\nSimple linear regression\nA simple linear regression (sometimes referenced as a bivariate regression) is a linear equation describing the relationship between an explanatory variable and an outcome variable.\nThere is an assumption that the explanatory variable influences the outcome variable, and not the other way around.\nTake, for example, a variable \\(y_i\\) which denotes the income of some individual in a sample, and we index this data using \\(i\\) where \\(i \\in \\{1, 2, ..., n\\}\\). We can let some other variable in our data \\(x_i\\) represent the years of education for the same individual. A simple linear regression equation of these variables take the following form: \\[y_i = b_0 + b_{1}x_{i} + e_i\\]\nwhere \\(b_1\\) is the sample estimate of the slope of the regression line with respect to the years of education and \\(b_0\\) is the sample estimate for the vertical intercept of the regression line.\n\n\n\nCorrelation coefficients and scatterplots\nAs a reminder, correlation ranges from -1 to 1. It gives us an indication on two things:\n\nThe direction of the relationship between the two variables\nThe strength of the relationship between the two variables\n\nAny outliers can greatly impact the value of a correlation coefficient.\n\nplot(x,y)\n\n\n\n\n\n\n\n\n\n\n\nEstimating differences in proportions\nWhen generating cross tabulations, we can make sense of a few bivariate tests:\n\nDifferences in proportions\nStandard errors of the difference in proportions\nConfidence intervals for the differences\nT-test for differences in proportions",
    "crumbs": [
      "Weekly materials",
      "Week 7"
    ]
  },
  {
    "objectID": "weeks/week07.html#part-iii-code",
    "href": "weeks/week07.html#part-iii-code",
    "title": "DATA 202 - Week 6",
    "section": "Part III: Code",
    "text": "Part III: Code\n\n\nWrite preamble\n\n# ---\n# title: Exploring associations between income and political party in the US\n# subtitle: sample paper 2\n# author: Nathan Alexander\n# course: DATA 202 - fall 2023\n# ---\n\n# research inquiry: does income relate to political party support in the US?\n# data: 2020 sample data from the General Social Survey (GSS)\n# note(s): variables should be mutated and recoded into two levels\n\n\n\n\nstep 0: install packages and load libraries\n\ninstall.packages(\"tidyverse\", repos = \"http://cran.us.r-project.org\")\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(tidyr)\n\n\n\n\nstep 1: view gss_cat data in the package forcats\n\ngetwd() # check working directory\n?gss_cat # view data documentation\ngss_cat # call data frame\nglimpse(gss_cat) # glimpse data\nsummary(gss_cat) # view summary of data\n\n\n\n\nstep 2: clean and manage data\n\ngss_cat_clean &lt;- gss_cat %&gt;% \n  na.omit() %&gt;% \n  select(year, rincome, partyid) %&gt;% \n  rename(income = rincome) %&gt;% \n  rename(party = partyid)\n\ngss_cat_clean # view cleaned data\n\n\n\n\nstep 3: subset data: year == 2000\n\ndf &lt;- gss_cat_clean %&gt;% \n  filter(year==2000)\n\nhead(df) # view top of data\ntail(df) # view bottom of data\nsummary(df) # check data\n\n\n\n\nstep 4: inspect and transform income variable into two levels\n\n## create a frequency table of each level in the income variable\ndf %&gt;% count(party)\n\n## drop 'No answer', 'Don't know', 'Refused', and 'Not applicable' levels\ndf &lt;- df %&gt;%  \n  filter(income != \"No answer\",\n         income != \"Don't know\",\n         income != \"Refused\",\n         income != \"Not applicable\") %&gt;% \n  droplevels() # use droplevels() to remove levels from variable for factors\n\n## use levels() function to view levels for income variable\nlevels(df$income)\n\n## create two levels: below $20,000 and above $20,000\ndf &lt;- df %&gt;% \n  mutate(income = fct_recode(income, \n                             \"More than 20000\" = \"$25000 or more\",\n                             \"More than 20000\" = \"$20000 - 24999\",\n                             \"Less than 20000\" = \"$15000 - 19999\",\n                             \"Less than 20000\" = \"$10000 - 14999\",\n                             \"Less than 20000\" = \"$8000 to 9999\",\n                             \"Less than 20000\" = \"$7000 to 7999\",\n                             \"Less than 20000\" = \"$6000 to 6999\",\n                             \"Less than 20000\" = \"$5000 to 5999\",\n                             \"Less than 20000\" = \"$4000 to 4999\",\n                             \"Less than 20000\" = \"$3000 to 3999\",\n                             \"Less than 20000\" = \"$1000 to 2999\",\n                             \"Less than 20000\" = \"Lt $1000\"))\n\n## view a summary of your transformed data frame\nsummary(df)\n\n\n\n\nstep 5: inspect and transform party variable into two levels\n\n## create a frequency table of each level in the party variable\ndf %&gt;% count(party)\n\n## drop 'No answer', 'Independent' and 'Other Party' levels\ndf &lt;- df %&gt;%  \n  filter(party != \"No answer\",\n         party != \"Independent\",\n         party != \"Other party\") %&gt;% \n  droplevels() # use droplevels() to remove levels from variable for factors\n\n## create two levels: 'Republican' and 'Democrat'\ndf &lt;- df %&gt;% \n  mutate(party = fct_recode(party,\n                            \"Republican\" = \"Strong republican\",\n                            \"Republican\" = \"Not str republican\",\n                            \"Republican\" = \"Ind,near rep\",\n                            \"Democrat\" = \"Ind,near dem\",\n                            \"Democrat\" = \"Not str democrat\",\n                            \"Democrat\" = \"Strong democrat\"))\n\n## remove year from data frame\ndf &lt;- df %&gt;% \n  select(-year)\n\n## view a summary of the data to check for any errors\nsummary(df)\n\n\n\n\nstep 6: visualize data\n\n## create a frequency table and bar graph of income \ntable.income = table(df$income)\ntable.income\nbarplot(table.income,\n        main = \"Bar graph of Income\",\n        xlab = \"Respondent Income\",\n        ylab = \"Frequency\")\n## the below code produces the same output as above with specifications\nbarplot(table(df$income),\n        main = \"Bar graph of Income\",\n        col = \"lightgreen\",\n        xlab = \"Respondent Income\",\n        ylab = \"Frequency\",\n        ylim = c(0,650)) # this y-axis range: (0, 650) works best for my plot\n## create a frequency table and bar graph of party \ntable.party = table(df$party)\ntable.party\nbarplot(table.party,\n        main = \"Bar graph of Party\",\n        xlab = \"Respondent Party\",\n        ylab = \"Frequency\")\n## the below code produces the same output as above with specifications\nbarplot(table(df$party),\n        main = \"Bar graph of Party\",\n        col = c(\"red\",\"blue\"),\n        xlab = \"Respondent Party\",\n        ylab = \"Frequency\",\n        ylim = c(0,600)) # this y-axis range: (0, 550) works best for my plot\n## create a stacked bar plot of the proportions\n#### question: which of the two plots do you prefer, why?\nplot(df$income, df$party)\nplot(df$party, df$income)\n## the below code produces similar outputs as above with specifications\nplot(df$party, df$income,\n     main = \"Mosaic Plot of Political Party and Income\",\n     col = c(\"lightyellow\",\"lightgreen\"),\n     xlab = \"Political Party\",\n     ylab = \"Income\")\n\n\n\n\nstep 7: create a basic cross tab for manual calculations\n\n## gather sample size\nn = count(df)\nn\n\n## view a basic cross tabulation\ntable(df$income, df$party)\n\n\n\n\nstep 8: bivariate analysis\n\n## load required libraries (and packages, where needed)\ninstall.packages(\"descr\", repos = \"http://cran.us.r-project.org\")\nlibrary(descr)\n\ninstall.packages(\"Hmisc\", repos = \"http://cran.us.r-project.org\")\nlibrary(Hmisc)\n\n## create a cross tab (list dependent variable in your hypothesis first)\ncrosstab(df$party, df$income)\n\n## add column percentages to the cross tab\ncrosstab(df$party, df$income,\n         prop.c=T) # add column percentages\n## add row percentages to the cross tab\ncrosstab(df$party, df$income,\n         prop.r=T) # add row percentages\n\n## get expected frequencies and cell chi-square contributions\ncrosstab(df$party, df$income,\n         expected = T, # get expected values\n         prop.chisq=T) # get chi-square contribution\n\n## get critical value of chi-square, p=.05, df=1\n#### recall: df = (r-1)(c-1)\nqchisq(.05, 1, lower.tail=F)\n\n## get chi-square statistic\nchisq.test(df$party, df$income)\n\n\n\n\nstep 9: describe some initial limitations of analysis\n\n### limitation 1: sampling error\n# data come from a sample and there are likely differences in other samples\n\n### limitation 2: category reductions\n# creating two levels for the variables greatly impacted the diversity of responses\n\n### limitation 3: cases dropped\n# sample was further impacted by the number of values dropped in the analysis\n\n### limitation 4: chi-square test\n# the chi-square test does not tell us about the strength or direction of association\n\n\n\n\nNext up: Week 7",
    "crumbs": [
      "Weekly materials",
      "Week 7"
    ]
  },
  {
    "objectID": "weeks/week05-slides.html#part-i-context",
    "href": "weeks/week05-slides.html#part-i-context",
    "title": "DATA 202 - Week 5",
    "section": "Part I: Context",
    "text": "Part I: Context"
  },
  {
    "objectID": "weeks/week05-slides.html#image-from-the-w.e.b.-dubois-collection.-httpscredo.library.umass.edu",
    "href": "weeks/week05-slides.html#image-from-the-w.e.b.-dubois-collection.-httpscredo.library.umass.edu",
    "title": "DATA 202 - Week 5",
    "section": "",
    "text": "Example 5.4: The Atlanta University Studies\n\n\n\nImage from The W.E.B. DuBois Collection. https://credo.library.umass.edu"
  },
  {
    "objectID": "weeks/week05-slides.html#part-ii-content",
    "href": "weeks/week05-slides.html#part-ii-content",
    "title": "DATA 202 - Week 5",
    "section": "Part II: Content",
    "text": "Part II: Content"
  },
  {
    "objectID": "weeks/week05-slides.html#part-iii-code",
    "href": "weeks/week05-slides.html#part-iii-code",
    "title": "DATA 202 - Week 5",
    "section": "Part III: Code",
    "text": "Part III: Code\nOur coding tasks this week will provide an overview of Lab 1.\nLab 1 focused on univariate statistics.\nUnivariate statistics refer to analyses that describe a single variable or attribute."
  },
  {
    "objectID": "weeks/week05-slides.html#see-the-bottom-right-of-the-screen.",
    "href": "weeks/week05-slides.html#see-the-bottom-right-of-the-screen.",
    "title": "DATA 202 - Week 5",
    "section": "",
    "text": "Task 16: Run univariate statistics in R\nWe will generate some code to examine univariate statistics in R. This will help you with HW 2.\nTake the plot of data for two variables, \\(x\\) and \\(y\\) below.\n\n\n\n\n\n\n\n\nFigure 1: Correlation plot of two variables, x and y\n\n\n\n\n\nNoticing that there is a relationship between \\(x\\) and \\(y\\), a first step is to run univariate functions on each variable individually."
  },
  {
    "objectID": "weeks/week05.html",
    "href": "weeks/week05.html",
    "title": "DATA 202 - Week 5",
    "section": "",
    "text": "The methods used to collect sample data for statistical analysis is extremely important.\nIf sample data are not collected appropriately, resulting statistical analyses will be futile.\nAs a result, planning a study by identifying research questions, the population and sample of interest, and selecting the appropriate research method(s) that will be used to analyze data that is collected are all essential parts in the statistical data analysis process.\n\n\n\nThere are many different types of research studies.\nSome studies use non-traditional methods (such as oral traditions) to collect data, while others focus on more traditional methods (such as surveys) to analyze data on a sample or a population.\nThese data collection methods produce a set of observations upon which statistical analyses can be applied. We consider two core study designs in statistical data analysis: experimental studies and observational studies.\n\n\nExperimental study: In an experimental study, a treatment is applied to a sample of interest to observe its effects. There is generally a control group and a treatment group used to understand the effects of the treatment. Individual observations are referred to as experimental units whereas studies involving humans are generally defined as study subjects.\nObservational study: In an observational study, specific characteristics of a sample or population are observed and measured but individual observations or subjects of study are not influenced or modified in any way.\n\n\n\n\n\n\n\n\n\n\nDEFINITIONS: Types of studies\n\n\n\nRetrospective study: In a retrospective study, we go back in time to collect data over some past period.\nCross-sectional study: In a cross-sectional study, data are collected and measured at one point in time.\nProspective study: In a prospective study, we set up a study to go forward in time and observe groups sharing common factors.\n\n\n\n\n\n\nThere are two broad categories of selecting members of a population to generate sample data:\n\nProbability sampling\nNon-probability sampling\n\nWithin these two broad categories are other methods based on the needs of the study. Each methods is used to support statistical data analysis with some methods providing stronger evidence than others.\n\n\n\n\n\n\n\nDefinitions: Sampling methods\n\n\n\nProbability sampling: Involves the random selection of subjects in such a way that every member of a sample has the sample probability of being selected.\nNon-probability sampling: Involves the use of criteria to select data that is not based on an equal likelihood of selection.\n\n\n\n\n\n\n\n\n\n\nImage from The W.E.B. DuBois Collection. https://credo.library.umass.edu\n\n\n\n\n\n\n\n\n\nImage from The W.E.B. DuBois Collection. https://credo.library.umass.edu",
    "crumbs": [
      "Weekly materials",
      "Week 5"
    ]
  },
  {
    "objectID": "weeks/week05.html#part-i-context",
    "href": "weeks/week05.html#part-i-context",
    "title": "DATA 202 - Week 5",
    "section": "",
    "text": "The methods used to collect sample data for statistical analysis is extremely important.\nIf sample data are not collected appropriately, resulting statistical analyses will be futile.\nAs a result, planning a study by identifying research questions, the population and sample of interest, and selecting the appropriate research method(s) that will be used to analyze data that is collected are all essential parts in the statistical data analysis process.\n\n\n\nThere are many different types of research studies.\nSome studies use non-traditional methods (such as oral traditions) to collect data, while others focus on more traditional methods (such as surveys) to analyze data on a sample or a population.\nThese data collection methods produce a set of observations upon which statistical analyses can be applied. We consider two core study designs in statistical data analysis: experimental studies and observational studies.\n\n\nExperimental study: In an experimental study, a treatment is applied to a sample of interest to observe its effects. There is generally a control group and a treatment group used to understand the effects of the treatment. Individual observations are referred to as experimental units whereas studies involving humans are generally defined as study subjects.\nObservational study: In an observational study, specific characteristics of a sample or population are observed and measured but individual observations or subjects of study are not influenced or modified in any way.\n\n\n\n\n\n\n\n\n\n\nDEFINITIONS: Types of studies\n\n\n\nRetrospective study: In a retrospective study, we go back in time to collect data over some past period.\nCross-sectional study: In a cross-sectional study, data are collected and measured at one point in time.\nProspective study: In a prospective study, we set up a study to go forward in time and observe groups sharing common factors.\n\n\n\n\n\n\nThere are two broad categories of selecting members of a population to generate sample data:\n\nProbability sampling\nNon-probability sampling\n\nWithin these two broad categories are other methods based on the needs of the study. Each methods is used to support statistical data analysis with some methods providing stronger evidence than others.\n\n\n\n\n\n\n\nDefinitions: Sampling methods\n\n\n\nProbability sampling: Involves the random selection of subjects in such a way that every member of a sample has the sample probability of being selected.\nNon-probability sampling: Involves the use of criteria to select data that is not based on an equal likelihood of selection.\n\n\n\n\n\n\n\n\n\n\nImage from The W.E.B. DuBois Collection. https://credo.library.umass.edu\n\n\n\n\n\n\n\n\n\nImage from The W.E.B. DuBois Collection. https://credo.library.umass.edu",
    "crumbs": [
      "Weekly materials",
      "Week 5"
    ]
  },
  {
    "objectID": "weeks/week05.html#image-from-the-w.e.b.-dubois-collection.-httpscredo.library.umass.edu",
    "href": "weeks/week05.html#image-from-the-w.e.b.-dubois-collection.-httpscredo.library.umass.edu",
    "title": "DATA 202 - Week 5",
    "section": "",
    "text": "Example 5.4: The Atlanta University Studies\n\n\n\nImage from The W.E.B. DuBois Collection. https://credo.library.umass.edu\n\n\n\n\n\nExample 5.5: The Atlanta University Studies\n\n\n\nImage from The W.E.B. DuBois Collection. https://credo.library.umass.edu\n\n\n\n\n\nExample 5.6: The Atlanta University Studies\n\n\n\nImage from The W.E.B. DuBois Collection. https://credo.library.umass.edu\n\n\n\n\n\nExample 5.7: The Atlanta University Studies\n\n\n\nImage from The W.E.B. DuBois Collection. https://credo.library.umass.edu\n\n\n\n\n\nExample 5.8: The Atlanta University Studies\n\n\n\nImage from The W.E.B. DuBois Collection. https://credo.library.umass.edu\n\n\n\n\n\nExample 5.9: The Atlanta University Studies\n\n\n\nImage from The W.E.B. DuBois Collection. https://credo.library.umass.edu\n\n\n\n\n\nExample 5.10: The Atlanta University Studies\n\n\n\nImage from The W.E.B. DuBois Collection. https://credo.library.umass.edu\n\n\n\n\nExample 5.11: The Black Census\n\n\n\nImage from The Black Futures Lab. Blackcensus.org\n\n\n\n\n\nExample 5.12: The Black Census reports\n\n\n\nImage from The Black Futures Lab. Blackcensus.org",
    "crumbs": [
      "Weekly materials",
      "Week 5"
    ]
  },
  {
    "objectID": "weeks/week05.html#part-ii-content",
    "href": "weeks/week05.html#part-ii-content",
    "title": "DATA 202 - Week 5",
    "section": "Part II: Content",
    "text": "Part II: Content\n\n\nThe Big Picture\nPrior to jumping into more details about univariate data examples in the next section, it will be important to gather a “big picture” view of the field. This big picture view will provide us with a high-level description of the various topics we will cover in the course.\n\n\nPopulation parameter\nA number which summarizes the entire group.\n\n\nSample statistic\nA single number that summarizes a subset of data, or the sample.\n\n\n\nPopulation vs. Sample. Image from Scribbr.\n\n\n\n\n\nConfidence interval (CI)\n\nRange of likely or plausible values for a population parameter.\nBased on a sample and statistics from that sample.\nMargin of error represents the number of standard deviations on a statistic.\n\n\n\n\nPlausible values for a variable via the CI. Image from Psychologicalscience.org\n\n\n\n\n\nHypothesis test\nStatistical procedure used to test an existing claim about a population.\n\nTest is based on data; most ideally data collected via probability sampling.\n\\(H_0\\) - Null hypothesis: If data supports the claim, fail to reject \\(H_0\\)\n\\(H_a\\) - Alternative hypothesis: If data does not support claim, reject \\(H_0\\)\n\n\n\n\nHypothesis testing. Image from Towards Data Science.\n\n\n\n\n\nAnalysis of variance (ANOVA)\nComparing means of more than two populations.\nF-statistic is a ratio that is used to compare variability between sets.\n\n\n\nWithin and between group variation. Image from QCBS R Workshop Series.\n\n\n\nMultiple comparison procedures\nSet of statistical tests that compare means to each other.\n\nExamples include Turkey’s test, Least significant difference (LSD), pairwise t-test\nThese tests are only conducted if you analysis of variance identifies differences\n\n\n\n\n\nInteraction effects\nInteraction effects are relevant to statistical models that use two or more variables.\n\n\nCorrelation\nMeasures the strength and direction of a linear relationship between two variables.\n\n\nLinear regression\nHelps make predictions for one variable based on the values of another.\n\nThere are many types of regression:\n\nSimple linear regression\nMultiple linear regression\nLogistic regression\nNon-linear regression\n\n\n\n\nChi-square test\nWhen using correlation and regression analyses, one core assumption is that the variables are quantitative in nature.\nWe use a chi-square test to study categorical variables.\n\n\n\nExample research question for a chi-square test. Image from Datatab.net.\n\n\nIn the example above, the null hypothesis is that there is no relationship between gender and highest level of education; the alternative hypothesis is that there is a relationship between gender and highest level of education.",
    "crumbs": [
      "Weekly materials",
      "Week 5"
    ]
  },
  {
    "objectID": "weeks/week05.html#part-iii-code",
    "href": "weeks/week05.html#part-iii-code",
    "title": "DATA 202 - Week 5",
    "section": "Part III: Code",
    "text": "Part III: Code\nOur coding tasks this week will provide an overview of Lab 1.\nLab 1 focused on univariate statistics.\nUnivariate statistics refer to analyses that describe a single variable or attribute.\n\nOpen RStudio.\n\n\n\nOpen RStudio.\n\n\n\n\nTask 0: Start a new project\nFirst, you will need to navigate to: File &gt; New Project\nSelect the first option: New Directory.\n\n\n\nSelect New Directory.\n\n\n\nThen you will want to select the New Project option.\n\n\n\nSelect New Project\n\n\n\nSelect the directory where you wish to store your project.\n\n\n\nSelect New Directory.\n\n\n\nThen, name your project to an appropriate title. Consider stats-pt21.\n\n\n\nName your project.\n\n\n\nBe sure to place the project in your preferred directory.\nOne good option is to make a sub-folder in your Documents.\n\n\n\nPlace your project in an appropriate directory.\n\n\n\nClick the box at the bottom right of the pop-up window.\nWe generally want to start a new project in a new RStudio session.\n\n\n\nCheck the box in the bottom right of the popup box.\n\n\nThen click Create Project.\n\n\n\nTask 1: Open a new RScript\nNow we will open a new RScript.\nNavigate to File &gt; New File &gt; RScript.\nThis RScript file is what you will use to outline your analysis.\n\n\n\nOpen a new RScript.\n\n\n\n\n\nTask 2: Write a preamble\nA preamble is similar to the heading of a paper.\nThe preamble contains information that will be useful for you and your collaborators.\n\n# Use the `#` symbol to tell R to ignore the text\n## Name: &lt;include your full name&gt;\n## Date: &lt;sometimes you may want to add a date&gt;\n## Purpose: &lt;insert the goals or purpose of the RScript&gt;\n\nRemember to use the # symbol to write your preamble.\n\n\n\nTask 3: Check your working directory\nPrior to inserting any code, it is generally helpful to check your working directory.\nThis will ensure that you are in the right location to call and save files.\n\n# get the working directory\ngetwd()\n\nIf, for any reason, the working directory is different from where you saved your R-project, check in the top right of your screen. Here, you should be able to select stats-pt2 or open the project using the menu options.\n\n\n\nTask 4: Install your packages\nWe will use the install.packages function to complete a few tasks.\nAny required packages are generally placed at the very top of our code.\n\ninstall.packages(\"tidyverse\", repos = \"http://cran.us.r-project.org\")\ninstall.packages(\"remotes\", repos = \"http://cran.us.r-project.org\")\n\n\n\n\nTask 5: Load your libraries\nAfter using the install.packages function, we need to load specific libraries.\nSimilar to installing packages, we generally load libraries at the very top of our code.\n\n# load libraries\nlibrary(tidyverse)\nlibrary(remotes)\n\n\n\n\nTask 6: Call in the critstats data package\nWe will use the remotes::install_github() command to install packages to call in data that I have prepared in a package for our course. The remotes::install_github() command communicates with GitHub to access files.\n\nremotes::install_github(\"professornaite/critstats\", force=TRUE)\n\n\n\nTask 6-a: Load the critstats library\nNext, we load the critstats library which will give us access to the package’s contents.\n\nlibrary(critstats)\n\n\n\n\n\nTask 7: View datasets in the current R session\nRStudio has a lot of pre-loaded data sets that we can view or use to practice on different types of variables. We use the data() function to view all of the data sets loaded in R.\n\ndata()\n\nA new window should open in your RStudio session.\n\n\n\nTask 8: View datasets only in the critstats data package\nWe can also view only the data sets loaded into the critstats package.\n\ndata(package=\"critstats\")\n\nA new window containing only the critstats data should open in your RStudio session.\n\n\n\nTask 9: View documentation for another data package called datasets\n\ndata(package=\"datasets\")\n\n\n\n\nTask 10: View documentation for the entire critstats data package\n\n??critstats\n\n\n\n\nTask 11: View documentation for a specific data set\nWe will select the africa_data_all data set and view its documentation.\n\n??africa_data_all\n\nThis documentation is the codebook for the data. It contains more specific information about the data frame, each of the variables, and any sourcing information.\n\n\n\nTask 12: Assign a dataset to an object for efficiency\nIf we want to call a data set more efficiently, we can assign it to an object.\nThe df object is a short name for data frame.\nWe will assign df1 using our assignment operator to the data set.\n\ndf1 &lt;- africa_data_all\n\n\n\n\nTask 13: Run a simple command\nWe can run simple commands on a data frame using the shorthand object we assigned to it.\nLet’s use the glimpse() function to explore the africa_data_all data frame that we assigned to the object df1.\n\nglimpse(df1)\n\nRows: 116\nColumns: 13\n$ country           &lt;chr&gt; \"Nigeria\", \"Ethiopia\", \"Egypt\", \"DR Congo\", \"South A…\n$ pop               &lt;dbl&gt; 206139589, 114963588, 102334404, 89561403, 59308690,…\n$ pop.yearly.change &lt;dbl&gt; 2.58, 2.57, 1.94, 3.19, 1.28, 2.98, 2.28, 3.32, 1.85…\n$ pop.net.change    &lt;dbl&gt; 5175990, 2884858, 1946331, 2770836, 750420, 1728755,…\n$ density           &lt;dbl&gt; 226, 115, 103, 40, 49, 67, 94, 229, 18, 25, 83, 26, …\n$ area              &lt;dbl&gt; 910770, 1000000, 995450, 2267050, 1213090, 885800, 5…\n$ migrants          &lt;dbl&gt; -60000, 30000, -38033, 23861, 145405, -40076, -10000…\n$ fertility.rate    &lt;dbl&gt; 5.4, 4.3, 3.3, 6.0, 2.4, 4.9, 3.5, 5.0, 3.1, 4.4, 2.…\n$ med.age           &lt;dbl&gt; 18, 19, 25, 17, 28, 18, 20, 17, 29, 20, 30, 17, 22, …\n$ urban.pop         &lt;dbl&gt; 52, 21, 43, 46, 67, 37, 28, 26, 73, 35, 64, 67, 57, …\n$ world.share       &lt;dbl&gt; 2.64, 1.47, 1.31, 1.15, 0.76, 0.77, 0.69, 0.59, 0.56…\n$ pop_in_mill       &lt;dbl&gt; 206.13959, 114.96359, 102.33440, 89.56140, 59.30869,…\n$ year              &lt;dbl&gt; 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020…\n\n\n\n\n\nTask 14: View your data\nWe can use the view() function to see our entire data frame.\n\nview(df1)\n\nYou may have noticed that a new window opened so that you can view() the entire data set. Sometimes, it is inefficient to use this command. Let’s look at two other options.\n\n\nTask 14-a: View the head of your data\nWe use the head function to get a closer look at the first few observations.\n\n# use the `head` function to view the top of the data\nhead(df1)\n\n# A tibble: 6 × 13\n  country           pop pop.yearly.change pop.net.change density   area migrants\n  &lt;chr&gt;           &lt;dbl&gt;             &lt;dbl&gt;          &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n1 Nigeria        2.06e8              2.58        5175990     226 9.11e5   -60000\n2 Ethiopia       1.15e8              2.57        2884858     115 1   e6    30000\n3 Egypt          1.02e8              1.94        1946331     103 9.95e5   -38033\n4 DR Congo       8.96e7              3.19        2770836      40 2.27e6    23861\n5 South Africa   5.93e7              1.28         750420      49 1.21e6   145405\n6 Tanzania       5.97e7              2.98        1728755      67 8.86e5   -40076\n# ℹ 6 more variables: fertility.rate &lt;dbl&gt;, med.age &lt;dbl&gt;, urban.pop &lt;dbl&gt;,\n#   world.share &lt;dbl&gt;, pop_in_mill &lt;dbl&gt;, year &lt;dbl&gt;\n\n\nChange the number of observations that you want to view in the data frame.\n\n# use the `head` function to view ten (10) observations at the top of the data\nhead(df1, n=10)\n\n# A tibble: 10 × 13\n   country          pop pop.yearly.change pop.net.change density   area migrants\n   &lt;chr&gt;          &lt;dbl&gt;             &lt;dbl&gt;          &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n 1 Nigeria       2.06e8              2.58        5175990     226 9.11e5   -60000\n 2 Ethiopia      1.15e8              2.57        2884858     115 1   e6    30000\n 3 Egypt         1.02e8              1.94        1946331     103 9.95e5   -38033\n 4 DR Congo      8.96e7              3.19        2770836      40 2.27e6    23861\n 5 South Africa  5.93e7              1.28         750420      49 1.21e6   145405\n 6 Tanzania      5.97e7              2.98        1728755      67 8.86e5   -40076\n 7 Kenya         5.38e7              2.28        1197323      94 5.69e5   -10000\n 8 Uganda        4.57e7              3.32        1471413     229 2.00e5   168694\n 9 Algeria       4.39e7              1.85         797990      18 2.38e6   -10000\n10 Sudan         4.38e7              2.42        1036022      25 1.77e6   -50000\n# ℹ 6 more variables: fertility.rate &lt;dbl&gt;, med.age &lt;dbl&gt;, urban.pop &lt;dbl&gt;,\n#   world.share &lt;dbl&gt;, pop_in_mill &lt;dbl&gt;, year &lt;dbl&gt;\n\n\n\n\n\nTask 14-b: View the tail of your data\nWe use the tail function to get a closer look at the last few observations.\n\n# use the `tail` function to view the top of the data\ntail(df1)\n\n# A tibble: 6 × 13\n  country           pop pop.yearly.change pop.net.change density   area migrants\n  &lt;chr&gt;           &lt;dbl&gt;             &lt;dbl&gt;          &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n1 Cabo Verde     598682              0.93           5533     149   4030    -1227\n2 Western Sahara 587259              1.96          11273       2 266000     5600\n3 Mayotte        335995              3.03           9894     896    375        0\n4 Sao Tome & Pr… 231856              1.97           4476     242    960     -600\n5 Seychelles     107660              0.51            542     234    460     -200\n6 Saint Helena     5314             -1.12            -60      14    390        0\n# ℹ 6 more variables: fertility.rate &lt;dbl&gt;, med.age &lt;dbl&gt;, urban.pop &lt;dbl&gt;,\n#   world.share &lt;dbl&gt;, pop_in_mill &lt;dbl&gt;, year &lt;dbl&gt;\n\n\nChange the number of observations that you want to view in the data frame.\n\n# use the `tail` function to view ten (10) observations at the top of the data\ntail(df1, n=10)\n\n# A tibble: 10 × 13\n   country          pop pop.yearly.change pop.net.change density   area migrants\n   &lt;chr&gt;          &lt;dbl&gt;             &lt;dbl&gt;          &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n 1 Eswatini      1.21e6              0.76           9152      70  17200    -5268\n 2 Djibouti      1.14e6              1.39          15606      49  23180      900\n 3 Réunion       9.82e5              0.8            7744     393   2500     -630\n 4 Comoros       8.52e5              1.83          15301     458   1861    -2000\n 5 Cabo Verde    5.99e5              0.93           5533     149   4030    -1227\n 6 Western Saha… 5.87e5              1.96          11273       2 266000     5600\n 7 Mayotte       3.36e5              3.03           9894     896    375        0\n 8 Sao Tome & P… 2.32e5              1.97           4476     242    960     -600\n 9 Seychelles    1.08e5              0.51            542     234    460     -200\n10 Saint Helena  5.31e3             -1.12            -60      14    390        0\n# ℹ 6 more variables: fertility.rate &lt;dbl&gt;, med.age &lt;dbl&gt;, urban.pop &lt;dbl&gt;,\n#   world.share &lt;dbl&gt;, pop_in_mill &lt;dbl&gt;, year &lt;dbl&gt;\n\n\n\n\n\n\nTask 15: Make and save a plot\nThe goal of this last task is to learn how to save a plot.\nOn the bottom left of your RStudio session, you should notice a few tabs.\nThe second tab is the Plots tab.\n\n\n\nFind the Plots tab.\n\n\n\n\nTask 15-a: Make a plot\nWe will make a random plot using the plot command (the plot itself is not important for now).\n\nplot(df1$pop)\n\nWhenever you make a plot, you can save it in your working directory.\nBe sure to check your working directory using the getwd command prior to saving.\n\n\n\nTask 15-b: Save a plot\nWhenever you want to save a plot, you can do so manually in directory portion of your RStudio session.\n\nNavigate to the bottom right portion of your RStudio session.\nGot to the Plots tab.\n\nRun the code for your plot, or navigate to your plot using the arrows.\n\nClick Export\n\nYou can save your file as a .pdf or as an image.\n\n\nLater, we will learn to customize plots and insert them into reports and papers.",
    "crumbs": [
      "Weekly materials",
      "Week 5"
    ]
  },
  {
    "objectID": "weeks/week05.html#see-the-bottom-right-of-the-screen.",
    "href": "weeks/week05.html#see-the-bottom-right-of-the-screen.",
    "title": "DATA 202 - Week 5",
    "section": "",
    "text": "Task 16: Run univariate statistics in R\nWe will generate some code to examine univariate statistics in R. This will help you with HW 2.\nTake the plot of data for two variables, \\(x\\) and \\(y\\) below.\n\n\n\n\n\n\n\n\nFigure 1: Correlation plot of two variables, x and y\n\n\n\n\n\nNoticing that there is a relationship between \\(x\\) and \\(y\\), a first step is to run univariate functions on each variable individually.\n\n\nTask 16-a: Measures of center\n\n# mean of x\nmean(x)\n\n[1] 0.03163502\n\n\n\n# mean of y\nmean(y)\n\n[1] 4.95324\n\n\n\n# median of x\nmedian(x)\n\n[1] -0.09398661\n\n\n\n# median of y\nmedian(y)\n\n[1] 5.070926\n\n\nNote: the median of a continuous variable constitutes the point at which the function of the distribution has the value 0.5\nGiven the nature of the variables, we will not compute the mode. Take note, howevever, that a table can be used to find the mode in a discrete variable.\n\n# Let some variable V contain a set of values\nv &lt;- c(1, 1, 2, 3, 4, 5, 5, 6, 7, 7, 7, 8, 8, 8, 8, 9)\nmode(v)\n\n[1] \"numeric\"\n\n\nThe output is not what we desire. We can create a table to get the values we’d like.\n\n# Let some variable z be a set of values\ntable.v &lt;- table(v)\ntable.v\n\nv\n1 2 3 4 5 6 7 8 9 \n2 1 1 1 2 1 3 4 1 \n\n\nThe table shows us the mode of the variable.\n\n\n\nTask 16-b: Measures of variation\n\n# range of x\nmax(x)\n\n[1] 2.387233\n\nmin(x)\n\n[1] -2.183967\n\n# create an object for the range\nrange.x = max(x) - min(x)\nrange.x # recall that we only stored the value in the last line, we must call it separately\n\n[1] 4.571199\n\n\n\n# range of y\nmax(y)\n\n[1] 9.831654\n\nmin(y)\n\n[1] -0.6718717\n\n# create an object for the range\nrange.y = max(y) - min(y)\nrange.y\n\n[1] 10.50353\n\n\n\n# standard deviation of x\nsd(x)\n\n[1] 0.9452844\n\n\n\n# standard deviation of y\nsd(y)\n\n[1] 2.04768\n\n\n\n\n\nTask 16-c: Measures of relative standing\n\n# Interquartile range of x\nIQR(x) # IQR computes the spread of the middle 50 percent of the values in X\n\n[1] 1.362935\n\n\n\n# Interquartile range of y\nIQR(y) # IQR computes the spread of the middle 50 percent of the values in Y\n\n[1] 2.333997\n\n\n\n\nTask 16-d: Plots\n\n# boxplot\nboxplot(x)\n\n\n\n\n\n\n\nboxplot(y)\n\n\n\n\n\n\n\n\n\n# histogram\nhist(x)\n\n\n\n\n\n\n\nhist(y)\n\n\n\n\n\n\n\n\n\n# density curve\nplot(density(x))\n\n\n\n\n\n\n\nplot(density(y))\n\n\n\n\n\n\n\n\n\n\n\nNext up: Week 6",
    "crumbs": [
      "Weekly materials",
      "Week 5"
    ]
  },
  {
    "objectID": "weeks/week05.html#footnotes",
    "href": "weeks/week05.html#footnotes",
    "title": "DATA 202 - Week 5",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe are in part two of our course, so I have chosen stats-pt2 for this project’s title. We’ll be starting other projects in parts three and four of our course. Note: there will be no project for part one of our course.↩︎",
    "crumbs": [
      "Weekly materials",
      "Week 5"
    ]
  },
  {
    "objectID": "weeks/week10-slides.html#part-i-context",
    "href": "weeks/week10-slides.html#part-i-context",
    "title": "DATA 202 - Week 10",
    "section": "Part I: Context",
    "text": "Part I: Context\n\nIllustration by Yuko Shimizu from cigionline.org.Last week launched part 3 of our course: data and policy, as in public policies.\nOur goals this week will be to consider variations of justice for your particular area(s) of interest and study, and to cite references that allow us to understand the meaning of social justice in your specific intellectual domains."
  },
  {
    "objectID": "weeks/week10-slides.html#part-ii-content",
    "href": "weeks/week10-slides.html#part-ii-content",
    "title": "DATA 202 - Week 10",
    "section": "Part II: Content",
    "text": "Part II: Content\nThe methods used to collect sample data is extremely important.\nIf sample data are not collected in an appropriately, any resulting statistical analyses will be futile. As a result, planning a study by identifying research questions, the population and sample of interest, and considering the types of methods that will be used to analyze data that is collected are all essential parts in the statistical data analysis process."
  },
  {
    "objectID": "weeks/week10-slides.html#part-iii-code",
    "href": "weeks/week10-slides.html#part-iii-code",
    "title": "DATA 202 - Week 10",
    "section": "Part III: Code",
    "text": "Part III: Code\nCrime statistics\nData from 1973 on violent crime rates by US State in the USArrests data set.\nValues are per 100,000 residents for assault, murder, and rape.\n\nhead(USArrests)\n\n           Murder Assault UrbanPop Rape\nAlabama      13.2     236       58 21.2\nAlaska       10.0     263       48 44.5\nArizona       8.1     294       80 31.0\nArkansas      8.8     190       50 19.5\nCalifornia    9.0     276       91 40.6\nColorado      7.9     204       78 38.7\n\ntail(USArrests)\n\n              Murder Assault UrbanPop Rape\nVermont          2.2      48       32 11.2\nVirginia         8.5     156       63 20.7\nWashington       4.0     145       73 26.2\nWest Virginia    5.7      81       39  9.3\nWisconsin        2.6      53       66 10.8\nWyoming          6.8     161       60 15.6"
  },
  {
    "objectID": "weeks/week10.html",
    "href": "weeks/week10.html",
    "title": "DATA 202 - Week 10",
    "section": "",
    "text": "Illustration by Yuko Shimizu from cigionline.org.\n\n\nLast week launched part 3 of our course: data and policy, as in public policies.\nOur goals this week will be to consider variations of justice for your particular area(s) of interest and study, and to cite references that allow us to understand the meaning of social justice in your specific intellectual domains.\n\n\n\nTheory is often understood through an investigation of both the literature and the historical contexts surrounding a given issue. Take, for example, the legacies of colonization. As data and scientific developments expand into new fields, new questions arise as to what we can associate and how we understand these associations. For example, how does one theoretically relate a series of events from 150 years ago to a present set of conditions? What are the possibilities? What are the limitations? What are the cautionary tales?\nStatistics was situated as a vehicle to understand and measure observations. As it becomes an ever popular vehicle to inform social justice discourses, the many real-world associations identified in society bring to mind the well-known and readily available cautionary tale:\n\nCorrelation does not imply causation.\n\nThis popular statement relates to the ability (or inability) to deduce cause-and-effect relationships.\n\n\n\n\n\n\n\n\n\n\nCausality is a fundamental concept in scientific inquiry and statistical analysis. It refers to the relationship between cause and effect, where one event (the cause) is understood to be responsible for another event (the effect). In statistics, causal inference aims to identify and quantify these cause-effect relationships.\n\n\n\nCausality can be defined as a relationship between two events where:\n\nThe cause precedes the effect in time.\nThere is a consistent association between the cause and effect.\nAlternative explanations for the association have been ruled out.\n\n\n\n\n\nIn the traditional setting, correlation is framed as a statistical measure that relays the size and direction of the relationship between two or more variables. From a more critical framework, however, the idea of associations between variables and the attribution of relationships between variables must be both examined and interrogated.\nAssociations should be understood and initiated from the root of a theoretical framework.\nIf two variables are related, in a statistical context, we assume that their values change in some ordered fashion. These measures are often represented on an axis depicting the strength of the association.\n\n\n\nImage from statlect.com\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nAs one value of a first variable increases the other variable’s values may also increase (positive relationship). Alternatively, as the value of one variable increases the other variable’s values may decrease (negative relationship). Key questions to consider focus on how context and other factors shape any seeming bivariate relationship.\n\n\n\nAt the intersection of mathematics and statistics is the concept of the spurious correlation (Ward, 2013). A few examples:\n\nDivorce rate has been found to correlate with margarine use.\nIce cream sales have been correlated with:\n\nSwimming deaths\nShark attacks\nCrime rates\n\nThe use of the popular site Facebook has been correlated with diminished well-being.\n\nThese association help us see the need to investigate relationships further. What are possible unexplained relationships in these associations? We identify potential spurious variables when developing theoretical ideas about associations.\n\n\n\n\n\nCausality (See Sloman and Acnado (2015)) is a representation and principle of cause and effect. There are many different viewpoints of causation across disciplines and fields of study. Terminology matters in how one frames cause and effect.\nThere are four possible relationships between two variables:\n\nX causes Y\nY causes X\nX and Y are both caused by Z\nX is not related to Y\n\nWhile these relationships may exist in some combination, it is important to frame their differences.\n\n\n\n\nOur initial framing might directly link the variables X and Y.\nFirst, we have the association X causes Y.\n\n\n\n\n\ngraph LR\n  A[X] --&gt; B[Y]\n\n\n\n\n\n\nNext, we have the association Y causes X.\n\n\n\n\n\ngraph LR\n  A[Y] --&gt; B[X]\n\n\n\n\n\n\nFinally, we have the association X causes Y and Y causes X.\n\n\n\n\n\ngraph LR\n  A[X] &lt;--&gt; B[Y]\n  B --&gt; A\n\n\n\n\n\n\n\n\n\n\nHowever, after further reading, we may find that we need to integrate a third factor Z.\nIn this context, a third factor (the “third-cause fallacy”) represents a spurious relationship.\n\n\n\n\n\ngraph LR\n  A[X]\n  B[Y]\n  C[Z]\n\n\n\n\n\n\nSpurious relationships denote an observed or hypothesized association.\nThere are important differences in how we depict and deal with third factor associations.\n\n\n\n\nConfounding is a causal concept that focuses on spurious or distorted associations.\nBelow, Z is considered a confounding variable.\n\n\n\n\n\ngraph TD\n  A[Z] --&gt; B[X]\n  A --&gt; C[Y]\n\n\n\n\n\n\n\nZ is related to both the independent, \\(X\\), and dependent, \\(Y\\), variables in the causal sense.\nYour analysis should examine if the relationship between \\(X\\) and \\(Y\\) holds.\nThese interactions are described as spurious associations.\n\n\n\n\n\nA mediating variable explains the process by which two variables are related.\n\n\n\n\n\ngraph LR\n  A[X] --&gt; B[Z]\n  B --&gt; C[Y]\n\n\n\n\n\n\n\nZ provides the mechanism to relate the independent, \\(X\\), and dependent, \\(Y\\), variables.\n\n\n\n\n\nA moderating variable explains the strength by which two variables are related.\n\n\n\n\n\ngraph LR\n  A[X] --&gt; B[Z]\n  A --&gt; C[Y]\n  B --&gt; C\n\n\n\n\n\n\n\nZ explains the strength of association between the independent, \\(X\\), and dependent, \\(Y\\), variables.\n\nLet us look at a concrete example.",
    "crumbs": [
      "Weekly materials",
      "Week 10"
    ]
  },
  {
    "objectID": "weeks/week10.html#part-i-context",
    "href": "weeks/week10.html#part-i-context",
    "title": "DATA 202 - Week 10",
    "section": "",
    "text": "Illustration by Yuko Shimizu from cigionline.org.\n\n\nLast week launched part 3 of our course: data and policy, as in public policies.\nOur goals this week will be to consider variations of justice for your particular area(s) of interest and study, and to cite references that allow us to understand the meaning of social justice in your specific intellectual domains.\n\n\n\nTheory is often understood through an investigation of both the literature and the historical contexts surrounding a given issue. Take, for example, the legacies of colonization. As data and scientific developments expand into new fields, new questions arise as to what we can associate and how we understand these associations. For example, how does one theoretically relate a series of events from 150 years ago to a present set of conditions? What are the possibilities? What are the limitations? What are the cautionary tales?\nStatistics was situated as a vehicle to understand and measure observations. As it becomes an ever popular vehicle to inform social justice discourses, the many real-world associations identified in society bring to mind the well-known and readily available cautionary tale:\n\nCorrelation does not imply causation.\n\nThis popular statement relates to the ability (or inability) to deduce cause-and-effect relationships.\n\n\n\n\n\n\n\n\n\n\nCausality is a fundamental concept in scientific inquiry and statistical analysis. It refers to the relationship between cause and effect, where one event (the cause) is understood to be responsible for another event (the effect). In statistics, causal inference aims to identify and quantify these cause-effect relationships.\n\n\n\nCausality can be defined as a relationship between two events where:\n\nThe cause precedes the effect in time.\nThere is a consistent association between the cause and effect.\nAlternative explanations for the association have been ruled out.\n\n\n\n\n\nIn the traditional setting, correlation is framed as a statistical measure that relays the size and direction of the relationship between two or more variables. From a more critical framework, however, the idea of associations between variables and the attribution of relationships between variables must be both examined and interrogated.\nAssociations should be understood and initiated from the root of a theoretical framework.\nIf two variables are related, in a statistical context, we assume that their values change in some ordered fashion. These measures are often represented on an axis depicting the strength of the association.\n\n\n\nImage from statlect.com\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nAs one value of a first variable increases the other variable’s values may also increase (positive relationship). Alternatively, as the value of one variable increases the other variable’s values may decrease (negative relationship). Key questions to consider focus on how context and other factors shape any seeming bivariate relationship.\n\n\n\nAt the intersection of mathematics and statistics is the concept of the spurious correlation (Ward, 2013). A few examples:\n\nDivorce rate has been found to correlate with margarine use.\nIce cream sales have been correlated with:\n\nSwimming deaths\nShark attacks\nCrime rates\n\nThe use of the popular site Facebook has been correlated with diminished well-being.\n\nThese association help us see the need to investigate relationships further. What are possible unexplained relationships in these associations? We identify potential spurious variables when developing theoretical ideas about associations.\n\n\n\n\n\nCausality (See Sloman and Acnado (2015)) is a representation and principle of cause and effect. There are many different viewpoints of causation across disciplines and fields of study. Terminology matters in how one frames cause and effect.\nThere are four possible relationships between two variables:\n\nX causes Y\nY causes X\nX and Y are both caused by Z\nX is not related to Y\n\nWhile these relationships may exist in some combination, it is important to frame their differences.\n\n\n\n\nOur initial framing might directly link the variables X and Y.\nFirst, we have the association X causes Y.\n\n\n\n\n\ngraph LR\n  A[X] --&gt; B[Y]\n\n\n\n\n\n\nNext, we have the association Y causes X.\n\n\n\n\n\ngraph LR\n  A[Y] --&gt; B[X]\n\n\n\n\n\n\nFinally, we have the association X causes Y and Y causes X.\n\n\n\n\n\ngraph LR\n  A[X] &lt;--&gt; B[Y]\n  B --&gt; A\n\n\n\n\n\n\n\n\n\n\nHowever, after further reading, we may find that we need to integrate a third factor Z.\nIn this context, a third factor (the “third-cause fallacy”) represents a spurious relationship.\n\n\n\n\n\ngraph LR\n  A[X]\n  B[Y]\n  C[Z]\n\n\n\n\n\n\nSpurious relationships denote an observed or hypothesized association.\nThere are important differences in how we depict and deal with third factor associations.\n\n\n\n\nConfounding is a causal concept that focuses on spurious or distorted associations.\nBelow, Z is considered a confounding variable.\n\n\n\n\n\ngraph TD\n  A[Z] --&gt; B[X]\n  A --&gt; C[Y]\n\n\n\n\n\n\n\nZ is related to both the independent, \\(X\\), and dependent, \\(Y\\), variables in the causal sense.\nYour analysis should examine if the relationship between \\(X\\) and \\(Y\\) holds.\nThese interactions are described as spurious associations.\n\n\n\n\n\nA mediating variable explains the process by which two variables are related.\n\n\n\n\n\ngraph LR\n  A[X] --&gt; B[Z]\n  B --&gt; C[Y]\n\n\n\n\n\n\n\nZ provides the mechanism to relate the independent, \\(X\\), and dependent, \\(Y\\), variables.\n\n\n\n\n\nA moderating variable explains the strength by which two variables are related.\n\n\n\n\n\ngraph LR\n  A[X] --&gt; B[Z]\n  A --&gt; C[Y]\n  B --&gt; C\n\n\n\n\n\n\n\nZ explains the strength of association between the independent, \\(X\\), and dependent, \\(Y\\), variables.\n\nLet us look at a concrete example.",
    "crumbs": [
      "Weekly materials",
      "Week 10"
    ]
  },
  {
    "objectID": "weeks/week10.html#part-ii-content",
    "href": "weeks/week10.html#part-ii-content",
    "title": "DATA 202 - Week 10",
    "section": "Part II: Content",
    "text": "Part II: Content\nThe methods used to collect sample data is extremely important.\nIf sample data are not collected in an appropriately, any resulting statistical analyses will be futile. As a result, planning a study by identifying research questions, the population and sample of interest, and considering the types of methods that will be used to analyze data that is collected are all essential parts in the statistical data analysis process.\n\n\nMore on sampling methods\nFirst, let’s revisit and describe the different sampling methods:\nThere are two broad categories of selecting members of a population to generate sample data: probability sampling and non-probability sampling.\nWithin these two broad categories are other methods based on the needs of the study. Each of these methods is used to support statistical data analysis with some methods providing stronger evidence than others.\n\n\nProbability sampling\n\ninvolves the random selection of subjects in such a way that every member of a sample has the sample probability of being selected.\n\nNon-probability sampling\n\ninvolves the use of criteria to select data that is not based on an equal likelihood of selection.\n\n\n\n\n\nProbability sampling methods\n\nSimple random sample of \\(n\\) subjects ensures that every possible sample of the same size \\(n\\) has the same chance or likelihood of being chosen.\nSystematic sample of \\(n\\) subjects involves selecting every \\(k\\)th subject on some regular interval.\nStratified sample involves dividing the population up into strata (groups) with the same characteristics and then randomly sampling within those strata.\nCluster sample involves partitioning the population into clusters (groups), randomly selecting some clusters, and then selecting all members of the selected clusters.\n\n\n\n\nNon-prbability sampling methods\n\nConvenience sample is data gathered from the most accessible or convenient source. Although this is easy and efficient, the data cannot produce generalizable results.\nPurposive sample is data gathered based on the purposes of the research or the specific research question. This strategy includes clear criteria and rationale for inclusion.\nSnowball sample is data gathered via recruitment by the other participants of a study. The number of subjects included “snowballs” as more contacts are generated.\n\n\n\nMultistage sampling\nIn larger studies, sometimes multistage sampling procedures are used to generate data. In this design, different samples are selected in different stages, and each stage might use a different sampling method. The end result may represent a complicated sampling design but it is often simpler and faster than some designs, such as a simple random sample.\n\nMissing Data\nData can be missing from a set, and the total number of elements can differ between two or more sets.\nThe differences in the number of elements can be due to a host of reasons, but often it is missing data. A data value can be missing at random or not missing at random. The amount of information missing from a data set can have a minimal impact or a major impact on a statistical analysis.\n\n\n\n\n\n\nNote – Missing data\nA data value is missing completely at random if the likelihood of it being missing is independent of its value or any other values in the data set is just as likely to be missing.\nA data value is not missing at random if the missing value is related to the reason it is missing.",
    "crumbs": [
      "Weekly materials",
      "Week 10"
    ]
  },
  {
    "objectID": "weeks/week10.html#part-iii-code",
    "href": "weeks/week10.html#part-iii-code",
    "title": "DATA 202 - Week 10",
    "section": "Part III: Code",
    "text": "Part III: Code\n\nCrime statistics\nData from 1973 on violent crime rates by US State in the USArrests data set.\nValues are per 100,000 residents for assault, murder, and rape.\n\nhead(USArrests)\n\n           Murder Assault UrbanPop Rape\nAlabama      13.2     236       58 21.2\nAlaska       10.0     263       48 44.5\nArizona       8.1     294       80 31.0\nArkansas      8.8     190       50 19.5\nCalifornia    9.0     276       91 40.6\nColorado      7.9     204       78 38.7\n\ntail(USArrests)\n\n              Murder Assault UrbanPop Rape\nVermont          2.2      48       32 11.2\nVirginia         8.5     156       63 20.7\nWashington       4.0     145       73 26.2\nWest Virginia    5.7      81       39  9.3\nWisconsin        2.6      53       66 10.8\nWyoming          6.8     161       60 15.6\n\n\n\n\ncor(USArrests$UrbanPop, USArrests$Murder)\n\n[1] 0.06957262\n\nplot(USArrests$UrbanPop, USArrests$Murder,\n     xlab = \"Percent living in urban areas\",\n     ylab = \"Murders per 100,000 residents\")\n\n\n\n\n\n\n\n\nWhat are some initial takeaways from this plot?\nWithout a guiding theory, where might our analyses take us?\n\n\n\nEducation data\nThis data is borrowed from the folks at UCLA’s Statistical Methods and Data Analytics office.\n\nrequire(foreign)\nrequire(ggplot2)\nrequire(MASS)\n\neduc &lt;- read.dta(\"https://stats.idre.ucla.edu/stat/stata/dae/nb_data.dta\")\n\n\n\nhead(educ)\n\n    id gender math daysabs prog\n1 1001   male   63       4    2\n2 1002   male   27       4    2\n3 1003 female   20       2    2\n4 1004 female   16       3    2\n5 1005 female    2       3    2\n6 1006 female   71      13    2\n\ntail(educ)\n\n      id gender math daysabs prog\n309 2152 female   46       1    3\n310 2153   male   26       1    2\n311 2154 female   79       3    3\n312 2155 female   59       0    2\n313 2156 female   90       0    3\n314 2157 female   77       2    3\n\n\n\n\neduc &lt;- within(educ, {\n  prog &lt;- factor(prog, levels = 1:3, labels = c(\"General\", \"Academic\", \"Vocational\"))\n  id &lt;- factor(id)\n})\n\nsummary(educ)\n\n       id         gender         math          daysabs               prog    \n 1001   :  1   female:160   Min.   : 1.00   Min.   : 0.000   General   : 40  \n 1002   :  1   male  :154   1st Qu.:28.00   1st Qu.: 1.000   Academic  :167  \n 1003   :  1                Median :48.00   Median : 4.000   Vocational:107  \n 1004   :  1                Mean   :48.27   Mean   : 5.955                   \n 1005   :  1                3rd Qu.:70.00   3rd Qu.: 8.000                   \n 1006   :  1                Max.   :99.00   Max.   :35.000                   \n (Other):308                                                                 \n\n\n\n\nggplot(educ, aes(daysabs, fill = prog)) + geom_histogram(binwidth = 1) + facet_grid(prog ~ \n                                                                                     ., margins = TRUE, scales = \"free\")\n\n\n\n\n\n\n\n\nWhat are some initial takeaways from this plot?\nWithout a guiding theory, where might our analyses take us?\n\n\n\nNext up: Week 11",
    "crumbs": [
      "Weekly materials",
      "Week 10"
    ]
  },
  {
    "objectID": "weeks/week01-slides.html#course-description",
    "href": "weeks/week01-slides.html#course-description",
    "title": "DATA 202 - Week 1",
    "section": "Course Description",
    "text": "Course Description\nIn this course, students will develop an understanding of statistics as a research tool. Students are expected to have some basic knowledge of statistics from a prior course. Emphasis will be placed on understanding statistical concepts and applying and interpreting tests of statistical inference for real-life applications. The content will include, but not be limited to, visual representations of data, descriptive statistics, correlation and simple regression, sampling distributions, and the assumptions associated with and the application of selected inferential statistical procedures. Throughout the course, there will be a strong emphasis on how statistical modeling can be a driving force for social justice."
  },
  {
    "objectID": "weeks/week01-slides.html#course-learning-objectives",
    "href": "weeks/week01-slides.html#course-learning-objectives",
    "title": "DATA 202 - Week 1",
    "section": "Course Learning Objectives",
    "text": "Course Learning Objectives\n\nAppreciate and understand the role of statistics in your field\nEvaluate, comprehend, and explain the statistical findings in a data set\nExplore data in R\nApply appropriate application and interpretation of various inferential statistical procedures\nWrite a simple description of methodology and results from analysis\nDevelop an ability to apply appropriate statistical methods to summarize and analyze data\nMake sense of data and be able to report the results in appropriate tables or statistical terms"
  },
  {
    "objectID": "weeks/week01-slides.html#course-companion-site",
    "href": "weeks/week01-slides.html#course-companion-site",
    "title": "DATA 202 - Week 1",
    "section": "Course companion site",
    "text": "Course companion site\nThe landing page for our companion site can be found here."
  },
  {
    "objectID": "weeks/week01-slides.html#initial-assignments",
    "href": "weeks/week01-slides.html#initial-assignments",
    "title": "DATA 202 - Week 1",
    "section": "Initial assignments",
    "text": "Initial assignments\nAnnotated Bibliography\nYour annotated bibliography is due Sun Feb 2 using OpenAlex or the Web of Science (WoS).\nLab #1\nLab 1 is due Sun Feb 16.\nPaper #1\nYou can read more about paper #1 here.\nSee the instructions for paper assignments here.\nAlso, learn more about paper assignments here."
  },
  {
    "objectID": "weeks/week01-slides.html#case-study-1-tuskegee-experiement-of-untreated-syphillis",
    "href": "weeks/week01-slides.html#case-study-1-tuskegee-experiement-of-untreated-syphillis",
    "title": "DATA 202 - Week 1",
    "section": "Case Study 1: Tuskegee Experiement of Untreated Syphillis",
    "text": "Case Study 1: Tuskegee Experiement of Untreated Syphillis\nTo help you prepare for our forthcoming discussions and readings, you should explore information about our first case study. One place to start is here: “The Tuskegee Experiment: Crash Course Black American History #29” in the video below:\n\n\n\n\n\n\n\n\nCourse Data GitHub"
  },
  {
    "objectID": "weeks/week01.html",
    "href": "weeks/week01.html",
    "title": "DATA 202 - Week 1",
    "section": "",
    "text": "In this course, students will develop an understanding of statistics as a research tool. Students are expected to have some basic knowledge of statistics from a prior course. Emphasis will be placed on understanding statistical concepts and applying and interpreting tests of statistical inference for real-life applications. The content will include, but not be limited to, visual representations of data, descriptive statistics, correlation and simple regression, sampling distributions, and the assumptions associated with and the application of selected inferential statistical procedures. Throughout the course, there will be a strong emphasis on how statistical modeling can be a driving force for social justice.",
    "crumbs": [
      "Weekly materials",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week01.html#course-description",
    "href": "weeks/week01.html#course-description",
    "title": "DATA 202 - Week 1",
    "section": "",
    "text": "In this course, students will develop an understanding of statistics as a research tool. Students are expected to have some basic knowledge of statistics from a prior course. Emphasis will be placed on understanding statistical concepts and applying and interpreting tests of statistical inference for real-life applications. The content will include, but not be limited to, visual representations of data, descriptive statistics, correlation and simple regression, sampling distributions, and the assumptions associated with and the application of selected inferential statistical procedures. Throughout the course, there will be a strong emphasis on how statistical modeling can be a driving force for social justice.",
    "crumbs": [
      "Weekly materials",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week01.html#course-learning-objectives",
    "href": "weeks/week01.html#course-learning-objectives",
    "title": "DATA 202 - Week 1",
    "section": "Course Learning Objectives",
    "text": "Course Learning Objectives\n\nAppreciate and understand the role of statistics in your field\nEvaluate, comprehend, and explain the statistical findings in a data set\nExplore data in R\nApply appropriate application and interpretation of various inferential statistical procedures\nWrite a simple description of methodology and results from analysis\nDevelop an ability to apply appropriate statistical methods to summarize and analyze data\nMake sense of data and be able to report the results in appropriate tables or statistical terms",
    "crumbs": [
      "Weekly materials",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week01.html#course-companion-site",
    "href": "weeks/week01.html#course-companion-site",
    "title": "DATA 202 - Week 1",
    "section": "Course companion site",
    "text": "Course companion site\nThe landing page for our companion site can be found here.",
    "crumbs": [
      "Weekly materials",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week01.html#initial-assignments",
    "href": "weeks/week01.html#initial-assignments",
    "title": "DATA 202 - Week 1",
    "section": "Initial assignments",
    "text": "Initial assignments\n\nAnnotated Bibliography\nYour annotated bibliography is due Sun Feb 2 using OpenAlex or the Web of Science (WoS).\n\n\nLab #1\nLab 1 is due Sun Feb 16.\n\n\nPaper #1\nYou can read more about paper #1 here.\nSee the instructions for paper assignments here.\nAlso, learn more about paper assignments here.",
    "crumbs": [
      "Weekly materials",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week01.html#case-study-1-tuskegee-experiement-of-untreated-syphillis",
    "href": "weeks/week01.html#case-study-1-tuskegee-experiement-of-untreated-syphillis",
    "title": "DATA 202 - Week 1",
    "section": "Case Study 1: Tuskegee Experiement of Untreated Syphillis",
    "text": "Case Study 1: Tuskegee Experiement of Untreated Syphillis\nTo help you prepare for our forthcoming discussions and readings, you should explore information about our first case study. One place to start is here: “The Tuskegee Experiment: Crash Course Black American History #29” in the video below:",
    "crumbs": [
      "Weekly materials",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week03-slides.html#case-study",
    "href": "weeks/week03-slides.html#case-study",
    "title": "DATA 202 - Week 3",
    "section": "Case Study",
    "text": "Case Study\nTuskegee Study of Untreated Syphilis in the Negro Male\nWe begin by exploring a critical historical issue in statistics: understanding the ethics of a medical intervention or study.\n\nUninformed participants of the Tuskegee Study of Untreated Syphilis in the Negro Male“In 1932, the USPHS, working with the Tuskegee Institute, began a study to record the natural history of syphilis. It was originally called the”Tuskegee Study of Untreated Syphilis in the Negro Male” (now referred to as the “USPHS Syphilis Study at Tuskegee”). The study initially involved 600 Black men – 399 with syphilis, 201 who did not have the disease. Participants’ informed consent was not collected.” (Office of Science, Centers for Disease Control and Prevention, 2022)"
  },
  {
    "objectID": "weeks/week03-slides.html#institutional-review-board-irb",
    "href": "weeks/week03-slides.html#institutional-review-board-irb",
    "title": "DATA 202 - Week 3",
    "section": "Institutional Review Board (IRB)",
    "text": "Institutional Review Board (IRB)\nThe Tuskegee Syphilis Study conducted by the U.S Public Health Service was only one of many other past abuses that included unethical experimentation on marginalized groups. As a result of these injustices, a set of mandates were instituted and policies are governed under the IRB, which define the rules and regulations for the approval of research activities. Other countries have equivalent measures focused on ethics.\nAdditional details about can be found online.\nLearn more at https://www.cdc.gov/tuskegee/timeline.\nLearn more about the Howard University IRB at the Office of Regulatory Research Compliance."
  },
  {
    "objectID": "weeks/week03-slides.html#theory",
    "href": "weeks/week03-slides.html#theory",
    "title": "DATA 202 - Week 3",
    "section": "Theory",
    "text": "Theory\nSome definitions:\n\nPer the Oxford Languages dictionary, a theory is a supposition or a system of ideas intended to explain something, especially one based on general principles independent of the thing to be explained.\nPer Britannica, a theory is an idea or set of ideas that is intended to explain facts or events.\n\nWe will need to think more acutely about theory development as we progress through the term.\n\nFor now, how might you define a theory?"
  },
  {
    "objectID": "weeks/week03-slides.html#theory-construction",
    "href": "weeks/week03-slides.html#theory-construction",
    "title": "DATA 202 - Week 3",
    "section": "Theory construction",
    "text": "Theory construction\nPer Markovsky & Webster (2015), theory construction is the process of formulating components of a theory into a logical whole.\nWe may consider some of the following elements as we prepare theoretical statements.\n\n\nResearch inquiry\nHypotheses\nAnalysis\nEvaluation\nRevision"
  },
  {
    "objectID": "weeks/week03-slides.html#data-and-information",
    "href": "weeks/week03-slides.html#data-and-information",
    "title": "DATA 202 - Week 3",
    "section": "Data and information",
    "text": "Data and information\n\n\n\nDEFINITION: Data\n\n\nData are collections of information or observations.\nThe term data is plural, so we should say “data are…” not “data is…”.\nA single data value is referred to as a datum but this term is very rarely used.\n\n\n\n\n\nThe term “information” is a universal concept that is highly useful but it also lacks precision.\nIn every day terms, information is defined rather loosely.\nIn statistics, however, we might contend that information becomes data when it is collected and organized in some form or fashion.\nThus, it takes some structure to turn information into usable data."
  },
  {
    "objectID": "weeks/week03-slides.html#population-and-sample",
    "href": "weeks/week03-slides.html#population-and-sample",
    "title": "DATA 202 - Week 3",
    "section": "Population and sample",
    "text": "Population and sample\n\n\n\n\nDEFINITIONS: Population and sample\n\n\nA population is representative of every member of a group of interest or collection of objects.\nA sample is a sub-collection of members or objects from a selected population.\n\n\n\n\n\nPopulation and sample. Image from SimplyPsychology.org"
  },
  {
    "objectID": "weeks/week03-slides.html#what-is-a-variable",
    "href": "weeks/week03-slides.html#what-is-a-variable",
    "title": "DATA 202 - Week 3",
    "section": "What is a variable?",
    "text": "What is a variable?\n\n\n\nDEFINITIONS: Variable\n\n\nA variable is any characteristic or quantity that can be measured or counted.\n\n\n\n\nThere are many types of data and variables used in statistics.\n\nThe type of variable helps determine the appropriate methods for analysis.\n– Categorical or Quantitative\nThe level of measurement helps determine how we measure variables.\n– Nominal, Ordinal, Interval, Ratio"
  },
  {
    "objectID": "weeks/week03-slides.html#task-0-understanding-the-rstudio-ide",
    "href": "weeks/week03-slides.html#task-0-understanding-the-rstudio-ide",
    "title": "DATA 202 - Week 3",
    "section": "Task 0: Understanding the RStudio IDE",
    "text": "Task 0: Understanding the RStudio IDE\n\nRStudio IDE"
  },
  {
    "objectID": "weeks/week03-slides.html#task-1-create-an-rmarkdown-file-.rmd",
    "href": "weeks/week03-slides.html#task-1-create-an-rmarkdown-file-.rmd",
    "title": "DATA 202 - Week 3",
    "section": "Task 1: Create an RMarkdown file (.Rmd)",
    "text": "Task 1: Create an RMarkdown file (.Rmd)\nWe will conduct most of our work using what is called an RMarkdown.\nThe RMarkdown files allows us to save and annotate our code for future use.\nWhen you run code from an RMarkdownt, it will show up in the Console (bottom left pane in RStudio).\nIn the RStudio IDE, open an RMarkdown by using the following navigation:\n\nFile &gt; New File &gt; R Markdown"
  },
  {
    "objectID": "weeks/week03-slides.html#task-2-explore-different-object-types",
    "href": "weeks/week03-slides.html#task-2-explore-different-object-types",
    "title": "DATA 202 - Week 3",
    "section": "Task 2: Explore different object types",
    "text": "Task 2: Explore different object types\nFor this task, we will explore three object types: numeric, character, and logic values.\nTask 2-a: Compute a mathematical statement and create a numeric variable\n\n1 + 2\n\n[1] 3\n\n\nWe can assign a variable to this statement by using an assignment operator: &lt;-\n\na &lt;- 1 + 2\n\nWe can also use an equal sign to assign values: \\(=\\)\n\na = 1 + 2\n\nType “a” to show the value of the variable\n\na\n\n[1] 3"
  },
  {
    "objectID": "weeks/week03-slides.html#task-3-creating-vectors",
    "href": "weeks/week03-slides.html#task-3-creating-vectors",
    "title": "DATA 202 - Week 3",
    "section": "Task 3: Creating vectors",
    "text": "Task 3: Creating vectors\nWhen we want to list multiple objects or values, we use R’s available data types, such as vectors and factors.\n\nWe concatenate values in these data types using the operator c( ) to place our values in the order we desire.\nConcatenate means to place things together one after the other.\n\nVectors\nVectors are a data type we use to order values (i.e., numeric, character, logic) or mix different values.\nFor example, we can store all of the numbers from 1 to 9 in a vector by using a colon.\n\nmy.vector &lt;- c(1:9)\nmy.vector\n\n[1] 1 2 3 4 5 6 7 8 9"
  },
  {
    "objectID": "weeks/week03-slides.html#task-4-creating-data-frames",
    "href": "weeks/week03-slides.html#task-4-creating-data-frames",
    "title": "DATA 202 - Week 3",
    "section": "Task 4: Creating data frames",
    "text": "Task 4: Creating data frames\nFor this final task, we will create a list. Lists can contain anything: functions, vectors, other lists, and data frames.\nTo start, let’s create a series of vectors.\n\nvec1 &lt;- c(\"Ida B. Wells\", \"W.E.B Du Bois\",\"Mary G. Ross\", \"Jaime Escalante\",\"Etta Z. Falconer\", \"Bob Moses\", \"Ruth Gonzales\")\nvec2 &lt;- c(1862, 1868, 1908, 1930, 1933, 1935, 1970)\nvec3 &lt;- c(\"MS\", \"MA\", \"OK\", \"Bolivia\", \"MS\", \"NYC\", \"NJ\")\nvec4 &lt;- c(TRUE, TRUE, TRUE, FALSE, T, T, T)\nvec5 &lt;- c(\"African American\", \"Ghanaian American\", \"Native American\", \"Bolivian American\", \"African American\", \"African American\", \"Mexican American\")\nvec6 &lt;- c(\"Journalist\", \"Sociologist\", \"Engineer\", \"Educator\", \"Mathematician\", \"Educator\", \"Engineer\")"
  },
  {
    "objectID": "weeks/week03.html",
    "href": "weeks/week03.html",
    "title": "DATA 202 - Week 3",
    "section": "",
    "text": "Tuskegee Study of Untreated Syphilis in the Negro Male\nWe begin by exploring a critical historical issue in statistics: understanding the ethics of a medical intervention or study.\n\n\n\nUninformed participants of the Tuskegee Study of Untreated Syphilis in the Negro Male\n\n\n“In 1932, the USPHS, working with the Tuskegee Institute, began a study to record the natural history of syphilis. It was originally called the”Tuskegee Study of Untreated Syphilis in the Negro Male” (now referred to as the “USPHS Syphilis Study at Tuskegee”). The study initially involved 600 Black men – 399 with syphilis, 201 who did not have the disease. Participants’ informed consent was not collected.” (Office of Science, Centers for Disease Control and Prevention, 2022)\n\nAll images are from Examining Tuskegee by Susan Reverby.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis case study will help frame our understanding between ethics and statistics. The study will allow us to explore longstanding injustices, consider ethical practices in historical contexts, and explore the sociology of statistics. More specifically, we will use this case to understand the history of the Institutional Review Board (IRB) and discuss concepts related to scientific racism. These explorations will help in two key ways: first, we will come to understand what it can mean to be critical in the context of statistics and, second, we will be prepared to respond to problems focused on main concepts in the first few weeks of our course. We will then formalize a few terms.\n\nFraming a relationship between ethics and statistics\nThe “Tuskegee Study of Untreated Syphilis in the Negro Male” (now referred to as the “USPHS Syphilis Study at Tuskegee”) took place in Macon County, Alabama, in an area known as the “Black Belt” because of its rich soil and vast number of Black sharecroppers who were the economic backbone of the region. The research took place at the Tuskegee Institute.\n\nPurpose of the study\nThe intent of the study was to record the natural progression of syphilis in Black men. The study was related to a 1928 retrospective study, the “Oslo Study of Untreated Syphilis,” which reported on the pathological manifestations of untreated syphilis in several hundred white males. However, this original study used secondary data to piece together findings. When the study was initiated in the U.S., there were no proven treatments for the disease. Researchers told the men participating in the study that they were to be treated for “bad blood.” This term had been used by local community members to describe a host of ailments that could be diagnosed including things like anemia, fatigue, and syphilis.\n\nStudy participants\nA total of 600 men were enrolled in the study.\n\nOf this group 399, who had syphilis were a part of the experimental group, and 201 were control subjects.\nMost of the men were poor and illiterate sharecroppers from the county.\nThe participants were offered medical care and insurance.\nThey were enrolled in the study with incentives as well, including medical exams, rides to and from the clinics, meals on examination days, free treatment for minor ailments and guarantees that provisions would be made after their deaths in terms of burial stipends paid to their survivors.\n\n\nEthical issues\n\n\nThere were no proven treatments for syphilis when the study began in 1932.\nWhen penicillin became the standard treatment for the disease in 1947 the medicine was withheld as a part of the treatment for both the experimental group and control group.\nOn July 25, 1972, Jean Heller of the Associated Press broke the story that appeared in both New York and Washington, that there had been a 40-year non-therapeutic experiment called “a study” on the effects of untreated syphilis on Black men in the rural South.\nBetween the start of the study in 1932 and 1947, the date when penicillin was determined as a cure for the disease, dozens of men had died and their wives, children and untold number of others had been infected.\nThere was evidence that scientific research protocol routinely applied to human subjects was either ignored or deeply flawed to ensure the safety and well-being of the men involved. Specifically, the men were never told about or offered the research procedure called informed consent.\n\n\n\nResearchers had not informed the men of the actual name of the study, its purpose, and potential consequences of the treatment or non-treatment that they would receive during the study. The men never knew of the debilitating and life-threatening consequences of the treatments they were to receive, the impact on their partners and children they may have conceived once involved in the research. The panel also concluded that there were no choices given to the participants to quit the study when penicillin became available as a treatment and cure for syphilis. Reviewing the results of the research the panel concluded that the study was “ethically unjustified.” The panel articulated all the above findings in October of 1972 and then one month later the Assistant Secretary for Health and Scientific Affairs officially declared the end of the Tuskegee Study.\n\nClass-Action Suit\nIn the summer of 1973, Attorney Fred Gray filed a class-action suit on behalf of the men in the study, recognized partners, children, and families. It ended a settlement giving more than $9 million to the study participants. Despite these reparative measures, the effects remain.\n\n\n\n\n\n\n\n\n\n\nThe Tuskegee Syphilis Study conducted by the U.S Public Health Service was only one of many other past abuses that included unethical experimentation on marginalized groups. As a result of these injustices, a set of mandates were instituted and policies are governed under the IRB, which define the rules and regulations for the approval of research activities. Other countries have equivalent measures focused on ethics.\nAdditional details about can be found online.\nLearn more at https://www.cdc.gov/tuskegee/timeline.\nLearn more about the Howard University IRB at the Office of Regulatory Research Compliance.\n\n\n\n\n\nWas the Syphilis Study at Tuskegee an experimental or an observational study? Explain.\n\n\n\nWas data collected for this study using probability or non-probability sampling methods?\n\n\n\nBased on your knowledge of the researchers for the USPHS Syphilis Study at Tuskegee, what was the most likely sampling method used to gather data? Explain.\n\n\n\nBased on your knowledge of the study and the data tables below, list five variables that were collected during the study and explain their variable types in detail.",
    "crumbs": [
      "Weekly materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week03.html#case-study",
    "href": "weeks/week03.html#case-study",
    "title": "DATA 202 - Week 3",
    "section": "",
    "text": "Tuskegee Study of Untreated Syphilis in the Negro Male\nWe begin by exploring a critical historical issue in statistics: understanding the ethics of a medical intervention or study.\n\n\n\nUninformed participants of the Tuskegee Study of Untreated Syphilis in the Negro Male\n\n\n“In 1932, the USPHS, working with the Tuskegee Institute, began a study to record the natural history of syphilis. It was originally called the”Tuskegee Study of Untreated Syphilis in the Negro Male” (now referred to as the “USPHS Syphilis Study at Tuskegee”). The study initially involved 600 Black men – 399 with syphilis, 201 who did not have the disease. Participants’ informed consent was not collected.” (Office of Science, Centers for Disease Control and Prevention, 2022)\n\nAll images are from Examining Tuskegee by Susan Reverby.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis case study will help frame our understanding between ethics and statistics. The study will allow us to explore longstanding injustices, consider ethical practices in historical contexts, and explore the sociology of statistics. More specifically, we will use this case to understand the history of the Institutional Review Board (IRB) and discuss concepts related to scientific racism. These explorations will help in two key ways: first, we will come to understand what it can mean to be critical in the context of statistics and, second, we will be prepared to respond to problems focused on main concepts in the first few weeks of our course. We will then formalize a few terms.\n\nFraming a relationship between ethics and statistics\nThe “Tuskegee Study of Untreated Syphilis in the Negro Male” (now referred to as the “USPHS Syphilis Study at Tuskegee”) took place in Macon County, Alabama, in an area known as the “Black Belt” because of its rich soil and vast number of Black sharecroppers who were the economic backbone of the region. The research took place at the Tuskegee Institute.\n\nPurpose of the study\nThe intent of the study was to record the natural progression of syphilis in Black men. The study was related to a 1928 retrospective study, the “Oslo Study of Untreated Syphilis,” which reported on the pathological manifestations of untreated syphilis in several hundred white males. However, this original study used secondary data to piece together findings. When the study was initiated in the U.S., there were no proven treatments for the disease. Researchers told the men participating in the study that they were to be treated for “bad blood.” This term had been used by local community members to describe a host of ailments that could be diagnosed including things like anemia, fatigue, and syphilis.\n\nStudy participants\nA total of 600 men were enrolled in the study.\n\nOf this group 399, who had syphilis were a part of the experimental group, and 201 were control subjects.\nMost of the men were poor and illiterate sharecroppers from the county.\nThe participants were offered medical care and insurance.\nThey were enrolled in the study with incentives as well, including medical exams, rides to and from the clinics, meals on examination days, free treatment for minor ailments and guarantees that provisions would be made after their deaths in terms of burial stipends paid to their survivors.\n\n\nEthical issues\n\n\nThere were no proven treatments for syphilis when the study began in 1932.\nWhen penicillin became the standard treatment for the disease in 1947 the medicine was withheld as a part of the treatment for both the experimental group and control group.\nOn July 25, 1972, Jean Heller of the Associated Press broke the story that appeared in both New York and Washington, that there had been a 40-year non-therapeutic experiment called “a study” on the effects of untreated syphilis on Black men in the rural South.\nBetween the start of the study in 1932 and 1947, the date when penicillin was determined as a cure for the disease, dozens of men had died and their wives, children and untold number of others had been infected.\nThere was evidence that scientific research protocol routinely applied to human subjects was either ignored or deeply flawed to ensure the safety and well-being of the men involved. Specifically, the men were never told about or offered the research procedure called informed consent.\n\n\n\nResearchers had not informed the men of the actual name of the study, its purpose, and potential consequences of the treatment or non-treatment that they would receive during the study. The men never knew of the debilitating and life-threatening consequences of the treatments they were to receive, the impact on their partners and children they may have conceived once involved in the research. The panel also concluded that there were no choices given to the participants to quit the study when penicillin became available as a treatment and cure for syphilis. Reviewing the results of the research the panel concluded that the study was “ethically unjustified.” The panel articulated all the above findings in October of 1972 and then one month later the Assistant Secretary for Health and Scientific Affairs officially declared the end of the Tuskegee Study.\n\nClass-Action Suit\nIn the summer of 1973, Attorney Fred Gray filed a class-action suit on behalf of the men in the study, recognized partners, children, and families. It ended a settlement giving more than $9 million to the study participants. Despite these reparative measures, the effects remain.",
    "crumbs": [
      "Weekly materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week03.html#institutional-review-board-irb",
    "href": "weeks/week03.html#institutional-review-board-irb",
    "title": "DATA 202 - Week 3",
    "section": "",
    "text": "The Tuskegee Syphilis Study conducted by the U.S Public Health Service was only one of many other past abuses that included unethical experimentation on marginalized groups. As a result of these injustices, a set of mandates were instituted and policies are governed under the IRB, which define the rules and regulations for the approval of research activities. Other countries have equivalent measures focused on ethics.\nAdditional details about can be found online.\nLearn more at https://www.cdc.gov/tuskegee/timeline.\nLearn more about the Howard University IRB at the Office of Regulatory Research Compliance.\n\n\n\n\n\nWas the Syphilis Study at Tuskegee an experimental or an observational study? Explain.\n\n\n\nWas data collected for this study using probability or non-probability sampling methods?\n\n\n\nBased on your knowledge of the researchers for the USPHS Syphilis Study at Tuskegee, what was the most likely sampling method used to gather data? Explain.\n\n\n\nBased on your knowledge of the study and the data tables below, list five variables that were collected during the study and explain their variable types in detail.",
    "crumbs": [
      "Weekly materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week03.html#theory",
    "href": "weeks/week03.html#theory",
    "title": "DATA 202 - Week 3",
    "section": "Theory",
    "text": "Theory\nSome definitions:\n\nPer the Oxford Languages dictionary, a theory is a supposition or a system of ideas intended to explain something, especially one based on general principles independent of the thing to be explained.\nPer Britannica, a theory is an idea or set of ideas that is intended to explain facts or events.\n\nWe will need to think more acutely about theory development as we progress through the term.\n\nFor now, how might you define a theory?",
    "crumbs": [
      "Weekly materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week03.html#theory-construction",
    "href": "weeks/week03.html#theory-construction",
    "title": "DATA 202 - Week 3",
    "section": "Theory construction",
    "text": "Theory construction\nPer Markovsky & Webster (2015), theory construction is the process of formulating components of a theory into a logical whole.\nWe may consider some of the following elements as we prepare theoretical statements.\n\n\nResearch inquiry\nHypotheses\nAnalysis\nEvaluation\nRevision\n\n\n\n\nFraming data and information\nThere are many different ways to conceptualize data and information.\n\n\nDepending on our specific context, statistical needs, or research purposes, we can frame information as data, or data as information, or even place an equivalence statement between the two terms such that we have: \\[\\text{information} = \\text{data}\\]\nAs we continue to explore what it should mean to be critical in the context of statistics, we will need some common language and base definitions to understand the processes involved in a statistical study.\nWe defined statistics as the science of collecting, organizing, analyzing, interpreting, communicating, and visualizing data and information. “Collecting” and “organizing” – the first two steps of this process – requires that we define what we mean by data and information.",
    "crumbs": [
      "Weekly materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week03.html#data-and-information",
    "href": "weeks/week03.html#data-and-information",
    "title": "DATA 202 - Week 3",
    "section": "Data and information",
    "text": "Data and information\n\n\n\n\n\n\nDEFINITION: Data\n\n\n\nData are collections of information or observations.\nThe term data is plural, so we should say “data are…” not “data is…”.\nA single data value is referred to as a datum but this term is very rarely used.\n\n\n\n\nThe term “information” is a universal concept that is highly useful but it also lacks precision.\nIn every day terms, information is defined rather loosely.\nIn statistics, however, we might contend that information becomes data when it is collected and organized in some form or fashion.\nThus, it takes some structure to turn information into usable data.\n\n\n\nOne method of organizing collections of data or information is in a Microsoft Excel sheet.\n\n\n\nExcel spreadsheet\n\n\n\nA spreadsheet in a traditional format contains important features that will support data analysis.\n\n\nColumns are vertical arrangements of sets.\n– Columns are represented by capital letters: \\(A\\), \\(B\\), \\(C\\), and so on.\n– We relate columns to sets, which we defined as collections of elements or items.\nRows are horizontal arrangements of elements.\n– Rows are represented by numbers and relate to the index of a set.\n– The subscript \\(i \\in \\mathbb{N}\\) such that \\(i = 1, 2, 3, ...\\), indicates an element’s position.\n\n\n\n\n\n\nExcel spreadsheet with data on US states\n\n\n\n\n\n\n\n\n\nDEFINITION: Filename extension\n\n\n\nA file extension (or file name extension) is a suffix at the end of a digital file name:\n\n\n.doc\n.docx\n.txt\n\nThe extension indicates the file’s format and the organization of information.\n\n\n\n\nThere are many different file extensions.\n\nFor example, the file extension for an MS Excel document can be .xlxs, .csv, or .htm.\nCSV (.csv) files remove all formatting.\n– The removal of formatting helps to reduce errors when transferring data between computers or software programs.\n“CSV” stands for comma-separated values.\n\n\n\n\n\n\nCSV file with data on US states\n\n\n\n\nFormatting data into a data frame\n\nAlthough the CSV format removes formatting, the file is not yet in a structure that we can use to conduct efficient statistical analyses.\nThe CSV format supports a user with collecting and organizing information into a usable data structure.\nHowever, the data needs to be sent to a computer program to undergo the next step in our statistical process: analyzing data. Specifically, we re-format the CSV file to a data frame to conduct analyses.\n\n\n\n\n\n\n\n\nDEFINITION: Data frame\n\n\n\nA data frame (or dataframe) is a two-dimensional table of rows and columns.\n\n\nData frames are a common and popular way to structure, store, and share data.\n\nData frames allow analysts to store sets of observations that vary in size and content.\nEach row describes a single observation.\nEach column stores information for one set, or variable.\n\n\nRecall that a set is a collection of elements.\n\nWe extend this definition to say that a set is a collection of \\(n \\in \\mathbb{N}\\) elements, where \\(n\\) refers to the number of elements in the set.\n\n\n\n\n\n\n\nNote – Sets with \\(n\\) elements\n\n\n\nIf \\(X\\) is a set with \\(n\\) elements, then \\(X\\) can be represented as \\(X = \\{x_1, x_2, x_3, ..., x_n \\}\\).\nIf \\(Y\\) is a set with \\(n\\) elements, then \\(Y\\) can be represented as \\(Y = \\{y_1, y_2, y_3, ..., y_n \\}\\).\n\n\nThe value of \\(n\\) can be used to represent the “size” of a set.\n\nA set with no elements is referred to as the empty set and can be represented as \\(\\{ \\emptyset \\}\\).\n\nWe will need to make sense of the various objects that a set can contain.\n\nFirst, we think more mathematically about sets and data frames.\n\n\n\n\n\n\nDEFINITION: Matrix\n\n\n\nIn mathematics, a matrix is a rectangular table of entries arranged in rows and columns.\n\n\n\nA data frame containing only numbers is an \\(n \\times m\\) matrix:\n\n\\(n\\) refers to the number of rows (or observations)\n\\(m\\) refers to the number of columns (or variables)\n\nBy formatting a series of sets into a data frame, we get\n\n\\(n\\) rows (each row containing data on a single observation)\n\\(m\\) columns (which contain elements over a variable’s values)\n\n\n\nData frame containing \\(n\\) observations for \\(2\\) variables (\\(X\\) and \\(Y\\))\n\n\n\n\n\n\nEXAMPLE – A data frame containing two variables (or sets)\n\n\n\n\n\n\nX\nY\n\n\n\n\n\\(x_1\\)\n\\(y_1\\)\n\n\n\\(x_2\\)\n\\(y_2\\)\n\n\n\\(x_3\\)\n\\(y_3\\)\n\n\n.\n.\n\n\n.\n.\n\n\n.\n.\n\n\n\\(x_n\\)\n\\(y_n\\)\n\n\n\n\n\n\nTake, for example, two sets with the following ordered values:\n\n\nLet \\(X\\) contain the odd values 1, 3, 5, and 7.\n\nWe have \\(X = \\{1, 3, 5, 7 \\}\\)\n\nLet \\(Y\\) contain the even values 2, 4, 6, and 8.\n\nWe have \\(Y = \\{2, 4, 6, 8 \\}\\)\n\n\n\n\nWe set the following to be true:\n\n\\(x_1 = 1\\), \\(x_2 = 3\\), \\(x_3 = 5\\), \\(x_4 = 7\\)\n\\(y_1 = 2\\), \\(y_2 = 4\\), \\(y_3 = 6\\), \\(y_4 = 8\\)\n\nBy combining the sets \\(X\\) and \\(Y\\), we create the following data frame:\n\n\n\n\n\n\nEXAMPLE – A data frame containing a few odd and even numbers\n\n\n\n\n\n\nX\nY\n\n\n\n\n1\n2\n\n\n3\n4\n\n\n5\n6\n\n\n7\n8\n\n\n\n\n\n\nWe list the index of each element of each set using a new column – ID (for index).\n\n\n\n\n\n\nEXAMPLE – A data frame containing a few odd and even numbers\n\n\n\n\n\n\nID\nX\nY\n\n\n\n\n1\n1\n2\n\n\n2\n3\n4\n\n\n3\n5\n6\n\n\n4\n7\n8\n\n\n\n\n\nThis \\(n \\times m\\) data frame contains\n\n\n\\(n = 4\\) observations\n\\(3\\) variables\n\nID\n\\(X\\)\n\\(Y\\)\n\n\n\n\nWe can consider this structure more generally as noted below.\n\n\n\nStructure of a data set from R for Data Science by Wickham & Grolemund (2022)",
    "crumbs": [
      "Weekly materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week03.html#population-and-sample",
    "href": "weeks/week03.html#population-and-sample",
    "title": "DATA 202 - Week 3",
    "section": "Population and sample",
    "text": "Population and sample\n\n\n\n\n\n\nDEFINITIONS: Population and sample\n\n\n\nA population is representative of every member of a group of interest or collection of objects.\nA sample is a sub-collection of members or objects from a selected population.\n\n\n\n\n\nPopulation and sample. Image from SimplyPsychology.org\n\n\n\n\nThe difference between big \\(N\\) (a population) and little \\(n\\) (a sample)\n\n\n\n\n\n\nNote\n\n\n\n\nWhen referring to the size of a population, use a capital \\(N\\).\nWhen referring to the size of a sample, use a lowercase \\(n\\).\n\n\n\n\n\n\nPopulation parameter vs. Sample statistic\n\n\n\n\n\n\nDEFINITIONS: Parameter versus statistic\n\n\n\nA parameter is a numerical measurement describing some characteristic of a population.\n\nThink of a “population parameter” to remember this relationship.\n\nA statistic is a numerical measure describing some characteristic of a sample.\n\nThink of a “sample statistic” to remember this relationship.\n\n\n\n\nStatistics vs. A statistic\n\n\n\n\n\n\nNote – Statistics (plural) versus a statistic (singular)\n\n\n\nThere is a difference between the term statistics (plural) and a statistic (singular).\n\nWe previously defined statistics (with an ‘s’ at the end) as the science of collecting, organizing, analyzing, interpreting, communicating, and visualizing data and information.\nA statistic (no ‘s’ at the end) refers to a measurement or value from a test on a sample.\n\n\n\n\n\n\nCensus\n\n\n\n\n\n\nDEFINITION: Census\n\n\n\nA census is the collection of data from every member of a population.\n\n\n\n\n\nPopulation census. Image from CasaNC.org\n\n\n\nThere are many sites with free and publicly available data that can help us make better sense of how others have collected data on populations and samples.\n– Sampling of websites with freely accessible data (no downloads required)\n\nColored Conventions Project\nThe DataHub\nData.gov\nKaggle\n\nIn our course, we will focus on integrating how we structure and analyze data by using various mathematical concepts to frame the process of conducting a statistical study.",
    "crumbs": [
      "Weekly materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week03.html#what-is-a-variable",
    "href": "weeks/week03.html#what-is-a-variable",
    "title": "DATA 202 - Week 3",
    "section": "What is a variable?",
    "text": "What is a variable?\n\n\n\n\n\n\nDEFINITIONS: Variable\n\n\n\nA variable is any characteristic or quantity that can be measured or counted.\n\n\n\nThere are many types of data and variables used in statistics.\n\nThe type of variable helps determine the appropriate methods for analysis.\n– Categorical or Quantitative\nThe level of measurement helps determine how we measure variables.\n– Nominal, Ordinal, Interval, Ratio\n\n\n\n\nVariable types\nThere are two main variable types: categorical and quantitative variables.\n\n\n\n\n\nflowchart LR\n  A[Variable type] --&gt; B(Categorical)\n  A[Variable type] --&gt; C(Quantitative)\n\n\n\n\n\n\n\n\n\n\n\n\nDEFINITIONS: Variable types\n\n\n\nA categorical variable consists of qualitative values such as names or labels.\nA quantitative variable consists of numbers representing counts or measurements.\n\n\n\nData that are categorical in nature are non-numeric.\n– Categorical values are labels used to represent categories or data values.\nData that are quantitative in nature are numeric.\n– Quantitative values make use of the sets of numbers to represent counts or measures.\n\n\nWe can further distinguish between these categories using the following definitions:\n\n\n\n\n\n\nDEFINITIONS: Nominal, Ordinal, Discrete, Continuous\n\n\n\nNominal data are categorical data that cannot be arranged in some order (such as low to high). Some examples are eye color and pet names.\nOrdinal data can be arranged in some order but differences between the values are meaningless. Consider letter grades: A is higher than B but A minus B does not make sense.\nDiscrete data are data where the number of values is finite or “countable.” Some examples include number of students or days spent in the library.\nContinuous data can take on infinitely many values where the collection of values is not countable. Some examples include height or weight.\n\n\n\n\n\n\n\nflowchart LR\n  A[Variable type] --&gt; B(Qualitative)\n  A[Variable type] --&gt; C(Quantitative)\n  C --&gt; D(Discrete)\n  C --&gt; E(Continuous)\n  B --&gt; F(Nominal)\n  B --&gt; G(Ordinal)\n\n\n\n\n\n\n\n\n\nLevels of measurement\nLevels of measurement are used to describe variable types.\n\n\n\n\n\n\n\n\nLevel of measurement\nBrief description\nExamples\n\n\n\n\nNominal\nData cannot be arranged in some order. Only categories are used.\nEye color, city\n\n\nOrdinal\nData can be arranged in some order but differences cannot be found or are meaningless.\nRankings, likert scale\n\n\nInterval\nThere is not a natural zero starting point and rations are meaningless.\nTemperatures, years\n\n\nRatio\nThere is a natural zero starting point and ratios are meaningful.\nHeights, distances\n\n\n\n\n\n\n\nThe four levels of measurement. Image from Scribbr",
    "crumbs": [
      "Weekly materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week03.html#task-0-understanding-the-rstudio-ide",
    "href": "weeks/week03.html#task-0-understanding-the-rstudio-ide",
    "title": "DATA 202 - Week 3",
    "section": "Task 0: Understanding the RStudio IDE",
    "text": "Task 0: Understanding the RStudio IDE\n\n\n\nRStudio IDE",
    "crumbs": [
      "Weekly materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week03.html#task-1-create-an-rmarkdown-file-.rmd",
    "href": "weeks/week03.html#task-1-create-an-rmarkdown-file-.rmd",
    "title": "DATA 202 - Week 3",
    "section": "Task 1: Create an RMarkdown file (.Rmd)",
    "text": "Task 1: Create an RMarkdown file (.Rmd)\nWe will conduct most of our work using what is called an RMarkdown.\nThe RMarkdown files allows us to save and annotate our code for future use.\nWhen you run code from an RMarkdownt, it will show up in the Console (bottom left pane in RStudio).\nIn the RStudio IDE, open an RMarkdown by using the following navigation:\n\nFile &gt; New File &gt; R Markdown\n\n\n\nUsing the RMarkdown file\n\n\nPreamble\nThe preamble begins at the top of your RMarkdown file with three dashes. I have added some options that can be changed to your liking. For example, you can change the table of contents (toc) options or the theme; flatly is one of many different themes.\n\n---\ntitle: \"Title goes here\"\nauhtor: \"First Last\"\nalways_allow_html: true\noutput:\n html_document:\n   toc: true\n   toc_depth: 3\n   number_sections: true\n   theme: flatly\ngeometry: margin=1.0in\neditor_options:\n markdown:\n   wrap: sentence\n---\n\n\n\n\nSetup\nA set-up chunk gives the file further instructions to prepare for your analysis.\n\nknitr::opts_chunk$set(echo = FALSE, # By default, hide code; set to TRUE to see code\n  fig.pos = 'th', # Places figures at top or here\n  out.width = '100%', dpi = 300, # Figure resolution and size\n  fig.env=\"figure\"\n ) # Latex figure environment\n\ninstall.packages(\"praise\")\nlibrary(praise)\n\noptions(knitr.table.format = \"latex\") # For kable tables to write LaTeX table directly\n\n\n\n\nInserting code into an RMarkdown\n\ny &lt;- 2 + 2\n\n\n# You can use the `#` symbol to leave notes above your code.\ny &lt;- 2 + 2 # You can use the `#` symbol to leave notes in-line with your code.\n\nWe just defined an object y. We can see its value by running this syntax and typing y into the Console.\n\ny\n\n[1] 4\n\n\nThe output on your screen should match the last line above - with the hashtags.\n\n[ 1 ] indicates a single line of results.\nThe output tells us that y has a value of 4, so we say that y is numeric or that it has a numeric value.\n\nYou can run code by clicking ‘Run’ at the top of the Source window, or by typing CMD+Enter",
    "crumbs": [
      "Weekly materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week03.html#task-2-explore-different-object-types",
    "href": "weeks/week03.html#task-2-explore-different-object-types",
    "title": "DATA 202 - Week 3",
    "section": "Task 2: Explore different object types",
    "text": "Task 2: Explore different object types\nFor this task, we will explore three object types: numeric, character, and logic values.\n\nTask 2-a: Compute a mathematical statement and create a numeric variable\n\n1 + 2\n\n[1] 3\n\n\nWe can assign a variable to this statement by using an assignment operator: &lt;-\n\na &lt;- 1 + 2\n\nWe can also use an equal sign to assign values: \\(=\\)\n\na = 1 + 2\n\nType “a” to show the value of the variable\n\na\n\n[1] 3\n\n\n\nCreate a numeric variable “b” that is the product of “a” and “y”\n\nb = a*y\n\nType “b” in your console to show the product of the two variables\n\nb\n\n[1] 12\n\n\nDivide b by 4\n\nb / 4\n\n[1] 3\n\n\nTake the square root of b\n\nsqrt(b)\n\n[1] 3.464102\n\n\n\nCompute the natural log of b\n\nlog(b)\n\n[1] 2.484907\n\n\nCompute the common log of b\n\nlog10(b)\n\n[1] 1.079181\n\n\nFind 1 minus the square root of b\n\n1-sqrt(b)\n\n[1] -2.464102\n\n\n\nAttempt to find the square root of “1 minus the square root of b” - which is a negative value\n\nsqrt(1-b)\n\nWarning in sqrt(1 - b): NaNs produced\n\n\n[1] NaN\n\n\nNaN stands for “Not a number”. This occurs because there is currently no defined value to recognize the square root of negative numbers in R. But we can compute the square root on the absolute value of this difference, if needed.\n\nsqrt(abs(1-b))\n\n[1] 3.316625\n\n\n\nWe can insert longer or more complex mathematical statements too. For example, we can find the absolute value of the sum of -1 and the square root of b cubed and then subtract from that the value of 3 times the square root of b.\nNotice the use of parentheses.\n\nabs(-1+sqrt(b^3)) - 3*(sqrt(b))\n\n[1] 30.17691\n\n\nWe can override the original value of y to match the mathematical statement we generated above.\n\ny &lt;- abs(-1+sqrt(b^3)) - 3*(sqrt(b))\ny\n\n[1] 30.17691\n\n\nWe consider all of the previous objects to be numeric.\n\n\n\nTask 2-b: Create a non-numeric value\nWe can also create objects to hold non-numeric values.\nThere are two types of non-numeric values: character values and logic values.\n\nCharacter values\n\ncharacter &lt;- 'some label'\ncharacter\n\n[1] \"some label\"\n\n\nWe can create a character value using the ‘,’ or “,” quotes.\n\ncharacter &lt;- \"some label\"\ncharacter\n\n[1] \"some label\"\n\n\n\n\n\nLogic values\nLogic values can either be TRUE or FALSE\n\nlogic_true &lt;- TRUE\nlogic_false &lt;- FALSE\n\n\nlogic_true\n\n[1] TRUE\n\nlogic_false\n\n[1] FALSE\n\n\nWe can also use T for TRUE and F for FALSE.\n\nlogic_true &lt;- T\nlogic_false &lt;- F\n\n\nlogic_true\n\n[1] TRUE\n\nlogic_false\n\n[1] FALSE",
    "crumbs": [
      "Weekly materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week03.html#task-3-creating-vectors",
    "href": "weeks/week03.html#task-3-creating-vectors",
    "title": "DATA 202 - Week 3",
    "section": "Task 3: Creating vectors",
    "text": "Task 3: Creating vectors\nWhen we want to list multiple objects or values, we use R’s available data types, such as vectors and factors.\n\nWe concatenate values in these data types using the operator c( ) to place our values in the order we desire.\nConcatenate means to place things together one after the other.\n\n\nVectors\nVectors are a data type we use to order values (i.e., numeric, character, logic) or mix different values.\nFor example, we can store all of the numbers from 1 to 9 in a vector by using a colon.\n\nmy.vector &lt;- c(1:9)\nmy.vector\n\n[1] 1 2 3 4 5 6 7 8 9\n\n\n\nWhen creating vectors, we use the assignment operator and the concatenate option to generate our object.\n\nvec1 &lt;- c(\"WEB Du Bois\", 1868, \"17th\", \"MA\", \"civil rights activist\")\nvec1\n\n[1] \"WEB Du Bois\"           \"1868\"                  \"17th\"                 \n[4] \"MA\"                    \"civil rights activist\"\n\n\nNotice that my numeric and logic values do not use quotations, but a character value uses quotations ““.\n\n\n\nFactors\nFactors are a data type we use to store categorical variables for analyses and data plots. We will explore these later.",
    "crumbs": [
      "Weekly materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week03.html#task-4-creating-data-frames",
    "href": "weeks/week03.html#task-4-creating-data-frames",
    "title": "DATA 202 - Week 3",
    "section": "Task 4: Creating data frames",
    "text": "Task 4: Creating data frames\nFor this final task, we will create a list. Lists can contain anything: functions, vectors, other lists, and data frames.\nTo start, let’s create a series of vectors.\n\nvec1 &lt;- c(\"Ida B. Wells\", \"W.E.B Du Bois\",\"Mary G. Ross\", \"Jaime Escalante\",\"Etta Z. Falconer\", \"Bob Moses\", \"Ruth Gonzales\")\nvec2 &lt;- c(1862, 1868, 1908, 1930, 1933, 1935, 1970)\nvec3 &lt;- c(\"MS\", \"MA\", \"OK\", \"Bolivia\", \"MS\", \"NYC\", \"NJ\")\nvec4 &lt;- c(TRUE, TRUE, TRUE, FALSE, T, T, T)\nvec5 &lt;- c(\"African American\", \"Ghanaian American\", \"Native American\", \"Bolivian American\", \"African American\", \"African American\", \"Mexican American\")\nvec6 &lt;- c(\"Journalist\", \"Sociologist\", \"Engineer\", \"Educator\", \"Mathematician\", \"Educator\", \"Engineer\")\n\n\nOur objects vec1, vec3, and vec5 are made of character values\n\nvec1\n\n[1] \"Ida B. Wells\"     \"W.E.B Du Bois\"    \"Mary G. Ross\"     \"Jaime Escalante\" \n[5] \"Etta Z. Falconer\" \"Bob Moses\"        \"Ruth Gonzales\"   \n\nvec3\n\n[1] \"MS\"      \"MA\"      \"OK\"      \"Bolivia\" \"MS\"      \"NYC\"     \"NJ\"     \n\nvec5\n\n[1] \"African American\"  \"Ghanaian American\" \"Native American\"  \n[4] \"Bolivian American\" \"African American\"  \"African American\" \n[7] \"Mexican American\" \n\nvec6\n\n[1] \"Journalist\"    \"Sociologist\"   \"Engineer\"      \"Educator\"     \n[5] \"Mathematician\" \"Educator\"      \"Engineer\"     \n\n\nOur object vec2 is made of numeric values\n\nvec2\n\n[1] 1862 1868 1908 1930 1933 1935 1970\n\n\nOur object vec4 is a made of logic values\n\nvec4\n\n[1]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE\n\n\n\n\nLists\nLists allow us to do what the name implies - create a list of items.\n\nmy.list &lt;- list(vec1, vec2, vec3, vec4, vec5, vec6)\nmy.list\n\n[[1]]\n[1] \"Ida B. Wells\"     \"W.E.B Du Bois\"    \"Mary G. Ross\"     \"Jaime Escalante\" \n[5] \"Etta Z. Falconer\" \"Bob Moses\"        \"Ruth Gonzales\"   \n\n[[2]]\n[1] 1862 1868 1908 1930 1933 1935 1970\n\n[[3]]\n[1] \"MS\"      \"MA\"      \"OK\"      \"Bolivia\" \"MS\"      \"NYC\"     \"NJ\"     \n\n[[4]]\n[1]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE\n\n[[5]]\n[1] \"African American\"  \"Ghanaian American\" \"Native American\"  \n[4] \"Bolivian American\" \"African American\"  \"African American\" \n[7] \"Mexican American\" \n\n[[6]]\n[1] \"Journalist\"    \"Sociologist\"   \"Engineer\"      \"Educator\"     \n[5] \"Mathematician\" \"Educator\"      \"Engineer\"     \n\n\nIn this form, lists can be hard to read.\n\n\n\nData frames\nTo solve the issue with the list, we will use our vectors to generate a specific type of list known as a data.frame.\nData frames are a subtype of lists made of vectors of equal lenght.\n\ndata.frame(vec1, vec2, vec3, vec4, vec5, vec6)\n\n              vec1 vec2    vec3  vec4              vec5          vec6\n1     Ida B. Wells 1862      MS  TRUE  African American    Journalist\n2    W.E.B Du Bois 1868      MA  TRUE Ghanaian American   Sociologist\n3     Mary G. Ross 1908      OK  TRUE   Native American      Engineer\n4  Jaime Escalante 1930 Bolivia FALSE Bolivian American      Educator\n5 Etta Z. Falconer 1933      MS  TRUE  African American Mathematician\n6        Bob Moses 1935     NYC  TRUE  African American      Educator\n7    Ruth Gonzales 1970      NJ  TRUE  Mexican American      Engineer\n\n\nNotice the difference in how the information is arranged when data.frame.\n\nLet’s label this data frame and put labels at the top of the list.\n\ndf &lt;- data.frame(a=vec1, b=vec2, c=vec3, d=vec4, e=vec5, f=vec6)\ndf\n\n                 a    b       c     d                 e             f\n1     Ida B. Wells 1862      MS  TRUE  African American    Journalist\n2    W.E.B Du Bois 1868      MA  TRUE Ghanaian American   Sociologist\n3     Mary G. Ross 1908      OK  TRUE   Native American      Engineer\n4  Jaime Escalante 1930 Bolivia FALSE Bolivian American      Educator\n5 Etta Z. Falconer 1933      MS  TRUE  African American Mathematician\n6        Bob Moses 1935     NYC  TRUE  African American      Educator\n7    Ruth Gonzales 1970      NJ  TRUE  Mexican American      Engineer\n\n\nNotice that when I add labels using the equal sign operator, the vec labels dissapear and are replaced by the categorical labels that we insert. Let’s create more appropriate labels for the data we have generated.\n\ndf &lt;- data.frame(name=vec1, \n                 birthyear=vec2, \n                 birthplace=vec3, \n                 USborn=vec4, \n                 nationality=vec5, \n                 occupation=vec6)\n\n\nFor future use, we can view our data frame by just typing its name into our console or typing View(df).\n\ndf\n\n              name birthyear birthplace USborn       nationality    occupation\n1     Ida B. Wells      1862         MS   TRUE  African American    Journalist\n2    W.E.B Du Bois      1868         MA   TRUE Ghanaian American   Sociologist\n3     Mary G. Ross      1908         OK   TRUE   Native American      Engineer\n4  Jaime Escalante      1930    Bolivia  FALSE Bolivian American      Educator\n5 Etta Z. Falconer      1933         MS   TRUE  African American Mathematician\n6        Bob Moses      1935        NYC   TRUE  African American      Educator\n7    Ruth Gonzales      1970         NJ   TRUE  Mexican American      Engineer",
    "crumbs": [
      "Weekly materials",
      "Week 3"
    ]
  },
  {
    "objectID": "papers/papers-instructions.html",
    "href": "papers/papers-instructions.html",
    "title": "Instructions for each paper",
    "section": "",
    "text": "These are detailed instructions for each paper. See the syllabus for the due dates for each paper. See also another document for course papers: More information for course papers\n\n\nCourse papers require that you do original data analysis and write up your results in a manuscript using APA. All papers should be written by you (try to make the text clear but sound professional, though the research questions/theories may be simple) and should include the elements described below. Please give each paper a title!\nEach paper should be written as a narrative essay. In writing the essay, you should structure it by using section headings as indicated for each paper. This should not detract from the narrative form (it should read like a paper not like answers to questions in a problem set). Each theory (or graph) should be described fully in about a paragraph, and more when necessary. Describe and justify the statistical relationship involving the theoretical variables that will be examined (including the variable’s unit of analysis and “what’s the story,” in terms of why we care about the issue(s); this is a great place to insert historical work).\nInclude a graph of some kind along with your opening at the beginning. Next, describe the data that you will be using to examine the theory. Describe the data source and for any survey data used describe the sampling method. For nationally representative data on high school students, I might say something like “The 2009 HSLS longitudinal sample is a two-stage stratified random sample of the nation’s high school students collected at the school level.” This detailed information can usually be found in the code books for the data sets you find. Be mindful, then, that it will require that you review these files in detail as it pertains to your own variables, your own analyses, and its relation to the social issue at hand. Theory is important.\nNext, describe the specific measures you are using and how they were obtained. Comment on how well the measures fit your theoretical variables (i.e., the face validity of the measures). In the case of survey data, report the full question wordings (verbatim) and specify the response categories and report how the variables were coded and recorded. Present the frequency distributions (so your coding and treatment of “missing data” categories can be checked); if you want to comment on the frequency distributions, you should do so in no more than 2-3 sentences (we are interested more in what comes after the distribution of the members of your sample or population!).\nIn your papers, specify the hypotheses for the expected relationships (correlations/regressions) of the variables and present the operational flow graph. Then report the statistical results, describing the tables, etc. and relevant statistics and any additional calculations. Describe to what extent the evidence is consistent with your hypothesis or hypotheses and lends support to your theorizing. What can you conclude about your original theory based upon your analysis? (*Note: Due to space limitations, when presenting exploratory data analysis and in presenting relationships of variables in research and writing, it is often useful to examine and to report simple tables of means or percentages to summarize bivariate relationships as well, especially when these include nominal or ordinal level (categorical) variables. Keep this in mind in doing research and data analysis beyond this course!)",
    "crumbs": [
      "Appendix",
      "Papers",
      "Instructions for each paper"
    ]
  },
  {
    "objectID": "papers/papers-instructions.html#preparing-papers",
    "href": "papers/papers-instructions.html#preparing-papers",
    "title": "Instructions for each paper",
    "section": "",
    "text": "Course papers require that you do original data analysis and write up your results in a manuscript using APA. All papers should be written by you (try to make the text clear but sound professional, though the research questions/theories may be simple) and should include the elements described below. Please give each paper a title!\nEach paper should be written as a narrative essay. In writing the essay, you should structure it by using section headings as indicated for each paper. This should not detract from the narrative form (it should read like a paper not like answers to questions in a problem set). Each theory (or graph) should be described fully in about a paragraph, and more when necessary. Describe and justify the statistical relationship involving the theoretical variables that will be examined (including the variable’s unit of analysis and “what’s the story,” in terms of why we care about the issue(s); this is a great place to insert historical work).\nInclude a graph of some kind along with your opening at the beginning. Next, describe the data that you will be using to examine the theory. Describe the data source and for any survey data used describe the sampling method. For nationally representative data on high school students, I might say something like “The 2009 HSLS longitudinal sample is a two-stage stratified random sample of the nation’s high school students collected at the school level.” This detailed information can usually be found in the code books for the data sets you find. Be mindful, then, that it will require that you review these files in detail as it pertains to your own variables, your own analyses, and its relation to the social issue at hand. Theory is important.\nNext, describe the specific measures you are using and how they were obtained. Comment on how well the measures fit your theoretical variables (i.e., the face validity of the measures). In the case of survey data, report the full question wordings (verbatim) and specify the response categories and report how the variables were coded and recorded. Present the frequency distributions (so your coding and treatment of “missing data” categories can be checked); if you want to comment on the frequency distributions, you should do so in no more than 2-3 sentences (we are interested more in what comes after the distribution of the members of your sample or population!).\nIn your papers, specify the hypotheses for the expected relationships (correlations/regressions) of the variables and present the operational flow graph. Then report the statistical results, describing the tables, etc. and relevant statistics and any additional calculations. Describe to what extent the evidence is consistent with your hypothesis or hypotheses and lends support to your theorizing. What can you conclude about your original theory based upon your analysis? (*Note: Due to space limitations, when presenting exploratory data analysis and in presenting relationships of variables in research and writing, it is often useful to examine and to report simple tables of means or percentages to summarize bivariate relationships as well, especially when these include nominal or ordinal level (categorical) variables. Keep this in mind in doing research and data analysis beyond this course!)",
    "crumbs": [
      "Appendix",
      "Papers",
      "Instructions for each paper"
    ]
  },
  {
    "objectID": "papers/paper3.html",
    "href": "papers/paper3.html",
    "title": "Paper 3",
    "section": "",
    "text": "For review, you may also refer to more information for course papers in two additional documents:\n\nInstructions for papers\nMore information for course papers\n\n\nStep 1: Theory and logic diagram\nPaper #3 is meant for those for whom two papers was not enough!\nFor this paper, we are interested in exploring more advanced theories pertaining to the relationship between variables explored in papers 1 or 2. Specifically, we are interested in understanding the association of characteristics, co-variates, and other relationships. Your paper 3 assignment should align with any goals you have upon completing your studies.\nNote: For your paper, please remember to address the theoretical relationship that the literature has identified between the conceptual variables selected. That is, before describing the measures and the survey you will be using, explain your theory and why you think that your independent variable is having an effect on your dependent variable.\n\n\nStep 2: Variables, Measurement, Hypothesis\nAll variables and analyses should be described in detail.\n\n\nStep 3: Statistical Analysis\nPlease be sure to utilize analyses on the interactions between multiple variables.\n\n\nStep 4: Conclusion\nFor this paper, please be sure to focus on a solid conclusion based on your statistical analysis."
  },
  {
    "objectID": "papers/annotated.html",
    "href": "papers/annotated.html",
    "title": "Annotated Bibliography",
    "section": "",
    "text": "This page will support you with annotated bibliography assignment. In class, we will review how to develop a single file in Markdown format (.Rmd) to produce your annotated bibliography and course papers.\n\nPurpose\nEvery research paper should include a relevant literature review.\nLiterature reviews can help your readers understand the need for your study, outline a paper’s central thesis, and support readers in making sense of closely related results and findings. As a result, literature reviews can come in many different lengths and formats.\nDoing a literature review is similar to doing any kind of research. A literature review should identify a central question, methodology, and also report findings. One effective way to start a literature review is to create an annotated bibliography.\nPrior to starting an annotated bibliography, it is useful to develop a question that will help you explore and better understand theoretical and methodological connections.\n\n\nFraming a question\nLiterature reviews, like most research projects, can begin from a basic question.\n\nDoes income relate to the availability of resources for [a population]?\nAre years of experience for [a sample] related to their perceptions of power?\n\nOnce you have identified a suitable question, you can use keywords to find sources for an annotated bibliography. The library website can be used to search periodicals or sources like the ERIC system, and you can also utilize popular search engines that house similar works, such as Research Rabbit and Google Scholar.\n\n\nAnnotated bibliographies\nAn annotated bibliography is a list of sources about a specific research question or topic. Each source contains a short statement about the contents of the source (e.g., research paper, online article, or other scholarly and/or published works). Annotated bibliographies are not abstracts, although their length and structure may be similar.\nEach source included in an annotated bibliography should begin with an appropriate citation of the source (in APA, MLA, or Chicago style) and a brief description of the purpose and contents of the source, as well as an evaluation or reflection. The inclusion of an evaluation or reflection when summarizing a published source is one effective way to tell the difference between the abstract and the annotation.\nView samples of annotated bibliographies."
  },
  {
    "objectID": "cases/case02-pt1.html",
    "href": "cases/case02-pt1.html",
    "title": "Case Study 2, Part 1: The True Size of Africa",
    "section": "",
    "text": "In Case Study 2, we will explore the social politics of maps and globally oriented data to help us make sense of what we mean by a “population.” The case study will integrate a series of new packages, functions, and code to support our explorations.\nThere are two parts to this case study, this is part 1.",
    "crumbs": [
      "Appendix",
      "Case Studies",
      "Case Study 2, pt. 1"
    ]
  },
  {
    "objectID": "cases/case02-pt1.html#learning-objectives",
    "href": "cases/case02-pt1.html#learning-objectives",
    "title": "Case Study 2, Part 1: The True Size of Africa",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nIn this case study component, you will be introduced to the dplyr package, which is one of the many packages in the tidyverse. The tidyverse is a set of packages that will be used for cleaning and organizing data.",
    "crumbs": [
      "Appendix",
      "Case Studies",
      "Case Study 2, pt. 1"
    ]
  },
  {
    "objectID": "cases/case02-pt1.html#learning-activities",
    "href": "cases/case02-pt1.html#learning-activities",
    "title": "Case Study 2, Part 1: The True Size of Africa",
    "section": "Learning Activities",
    "text": "Learning Activities\nBy the end of this case study you will be able to:\n\nInstall and/or update R packages\nAssign data frames to different names for efficient exploration\nGenerate a set of outputs using the dplyr package\nOverwrite a data frame while using the pipe operator\nProduce simple plots using data located in an R package",
    "crumbs": [
      "Appendix",
      "Case Studies",
      "Case Study 2, pt. 1"
    ]
  },
  {
    "objectID": "cases/case02-pt1.html#developing-a-workflow",
    "href": "cases/case02-pt1.html#developing-a-workflow",
    "title": "Case Study 2, Part 1: The True Size of Africa",
    "section": "Developing a workflow",
    "text": "Developing a workflow\nCoding workflows are an essential component of project completion.\nWhen analyzing data, it is important to understand the possible intersections between the context, content, and code. The best way to explore these relationships is by conducting a literature review. Reading about what others have done is more valuable than starting a workflow prematurely.\nTime may be used inefficiently and clear but standard outputs provide narratives that can likely be confirmed by or support what is already in the research literature. Without an understanding of these connections, analytic outputs may do less to move our understanding of issues forward.\nThis case study will help you explore and create your own workflow. The goal of a coding workflow is not to simply copy and paste arguments that you find. Instead, you want to develop clear pathways to identifying solutions as you work with your data.\nThe topics in case study 2 cover one way to approach a new data set. In this case study, you’ll cover how to load data from a package, generate a set of outputs using small code chunks, and produce and submit a few simple plots.",
    "crumbs": [
      "Appendix",
      "Case Studies",
      "Case Study 2, pt. 1"
    ]
  },
  {
    "objectID": "cases/case02-pt1.html#task-0.1-check-your-working-directory",
    "href": "cases/case02-pt1.html#task-0.1-check-your-working-directory",
    "title": "Case Study 2, Part 1: The True Size of Africa",
    "section": "Task 0.1: Check your working directory",
    "text": "Task 0.1: Check your working directory\nIn your console, type in the following code to ensure you are in the desired directory:\n\ngetwd()\n\nIf you are not in the desired directory, you can change your directory using the associated path. This path should be the same as the project folder that you plan to work out of and set up in lecture five.\n\n# insert your desired path in the parenthesis and remove the #\n# setwd(\"/your/working/directory/goes/here\") \n\nYou can add a new sub-folder manually or under the Files tab in the RStudio IDE.",
    "crumbs": [
      "Appendix",
      "Case Studies",
      "Case Study 2, pt. 1"
    ]
  },
  {
    "objectID": "cases/case02-pt1.html#task-0.2-start-a-new-rmarkdown-file",
    "href": "cases/case02-pt1.html#task-0.2-start-a-new-rmarkdown-file",
    "title": "Case Study 2, Part 1: The True Size of Africa",
    "section": "Task 0.2: Start a new RMarkdown file",
    "text": "Task 0.2: Start a new RMarkdown file\nOnce you have confirmed that you are in the correct directory, start a new RMarkdown file (.Rmd) and write your preamble.\n\n---\ntitle: \"Case Study 2\"\nauthor: \"Your Full Name\"\ndate: \"2024-09-09\"\noutput:\n  pdf_document: default\n  html_document:\n    theme: flatly\neditor_options:\n  markdown:\n  wrap: sentence\n ---",
    "crumbs": [
      "Appendix",
      "Case Studies",
      "Case Study 2, pt. 1"
    ]
  },
  {
    "objectID": "cases/case02-pt1.html#task-0.3-write-setup-code-chunks",
    "href": "cases/case02-pt1.html#task-0.3-write-setup-code-chunks",
    "title": "Case Study 2, Part 1: The True Size of Africa",
    "section": "Task 0.3: Write setup code chunks",
    "text": "Task 0.3: Write setup code chunks\n\n# ```{r, eval=F}\n# install.packages(\"devtools\")\n# library(devtools)",
    "crumbs": [
      "Appendix",
      "Case Studies",
      "Case Study 2, pt. 1"
    ]
  },
  {
    "objectID": "cases/case02-pt1.html#task-0.4-packages-and-libraries",
    "href": "cases/case02-pt1.html#task-0.4-packages-and-libraries",
    "title": "Case Study 2, Part 1: The True Size of Africa",
    "section": "Task 0.4: Packages and libraries",
    "text": "Task 0.4: Packages and libraries\n\n# install package\n# install.packages(\"tidyverse\", repos = \"http://cran.us.r-project.org\")\n# install.packages(\"remotes\", repos = \"http://cran.us.r-project.org\")\n \n# load the necessary libraries\nlibrary(tidyverse) #collection of R packages designed for data science\nlibrary(dplyr)\nlibrary(remotes)\n\nThe dplyr package supports data analyst with efficient data manipulation. As a part of the tidyverse package, the functions included in dplyr we loaded earlier will help you generate efficient workflows. Though, in reality, most analysts transition between classic code found widely on the internet and the more recent dplyr commands.\nThe remotes library will allow you to remotely install critstats data.",
    "crumbs": [
      "Appendix",
      "Case Studies",
      "Case Study 2, pt. 1"
    ]
  },
  {
    "objectID": "cases/case02-pt1.html#task-1.1-explore-notes-on-the-social-politics-of-maps",
    "href": "cases/case02-pt1.html#task-1.1-explore-notes-on-the-social-politics-of-maps",
    "title": "Case Study 2, Part 1: The True Size of Africa",
    "section": "Task 1.1: Explore notes on the social politics of maps",
    "text": "Task 1.1: Explore notes on the social politics of maps\n\nWhat is a map? This National Geographic education resource presents a clear overview of maps, geography, and Geographic Information Systems (GIS).\nWhat’s the real size of Africa? is a CNN Africa Marketplace article that examines the Western foundations of maps and representations of the African continent.\nVaughan (2018) is an open access publication on the spatial dimensions of social cartography. The text contains valuable information about how maps have been used to understand health and human development issues, such as poverty, disease, housing, and the like. The text also contains notes on race and nationality, crime and disorder, and a host of references for further reading.\nManson & Matson (2017) present an overview of society and mapping with new technological tools. While doing so, the authors provide a history of maps and examine the basic social elements of maps, the technical elements of maps, and how maps have been integrated into liberal arts education.\nCrampton (2015) writes on Maps and the Social Construction of Race in a larger volume on maps produced by the University of Chicago Press.\nAlderman & Inwood (2021) describe how Black cartographers use maps to examine issues of racial inequality. The authors provide a more focused discussion on the social politics of maps, as opposed to a more general overview of their functions.\nCan maps be racist? Palmer (2014) provides some context to understand the technical aspects of maps as they relate to our social construction of the global world. In this review, the author situates the common functions of maps onto the social dimensions while attending to the particular periods of the development and construction of global maps; thus integrating the political dimension of knowledge creation via map making.\n– Britton (2021) in a blog post on the “non-racism of maps” offers a very different perspective on the Mercator projection. The author focuses on ideology and science in modern society. He argues that the original purpose of maps does not make them racist.\nHow maps distort our perception of the world is a short and focused resource written by Lee (2023) on the Anti-Racism Daily site. The author focuses on the social politics of perception.\n\n\n\n\nThe world’s continents. Image from https://www.visualcapitalist.com/map-true-size-of-africa/",
    "crumbs": [
      "Appendix",
      "Case Studies",
      "Case Study 2, pt. 1"
    ]
  },
  {
    "objectID": "cases/case02-pt1.html#task-1.2-load-the-true_size-data",
    "href": "cases/case02-pt1.html#task-1.2-load-the-true_size-data",
    "title": "Case Study 2, Part 1: The True Size of Africa",
    "section": "Task 1.2: Load the true_size data",
    "text": "Task 1.2: Load the true_size data\n\nTask 1.2.1: Install the critstats package\nTo begin, we will install and/or update the installation of critstats.\n\n# use the remote install function to call in your data\nremotes::install_github(\"professornaite/critstats\", force=TRUE)\n\n# load the `critstats` library\nlibrary(critstats)\n\n# update the `critstats` package if needed\n# update.packages(\"critstats\")\n\n\n\nTask 1.2.2: Call the true_size data\n\ncritstats::true_size\n\n\n\nTask 1.2.3: Inspect the true_size documentation\nUsing the ??data prompt, you can inspect the contents of the data frame.\n\n??critstats::true_size\n\nAs noted before, this serves as the data’s documentation and is the basis of a code book (or codebook). A codebook contains very specific details about a database, data set, and the variables each contains. We will explore codebooks more in the future.",
    "crumbs": [
      "Appendix",
      "Case Studies",
      "Case Study 2, pt. 1"
    ]
  },
  {
    "objectID": "cases/case02-pt1.html#task-1.3-explore-the-true_size-data",
    "href": "cases/case02-pt1.html#task-1.3-explore-the-true_size-data",
    "title": "Case Study 2, Part 1: The True Size of Africa",
    "section": "Task 1.3: Explore the true_size data",
    "text": "Task 1.3: Explore the true_size data\n\nTask 1.3.1: Assign the true_size data frame to df1\nUse the assignment operator to assign the true_size data frame to the object df1.\n\ndf1 &lt;- critstats::true_size\n\n\n\nTask 1.3.2: Inspect your data\nUse the str() function to inspect your data frame.\nstr() displays the structure of R objects.\n\nstr(df1)\n\ntibble [18 × 4] (S3: tbl_df/tbl/data.frame)\n $ Country       : chr [1:18] \"United States\" \"China\" \"India\" \"Mexico\" ...\n $ percent.africa: num [1:18] 32.4 31.6 10.8 6.5 4.2 2.1 1.7 1.5 1.5 1.3 ...\n $ area.sq.km    : num [1:18] 9.83 9.6 3.29 1.96 1.29 0.64 0.51 0.46 0.45 0.38 ...\n $ area.sq.mi    : num [1:18] 3.8 3.71 1.27 0.76 0.5 0.25 0.2 0.18 0.17 0.15 ...\n\n\nYou can also run similar commands separately:\n\nTask 1.3.2a: dim()\nUse the dim() function to check the dimensions of your data.\n\n# check the dimensions of your data\ndim(df1)\n\n[1] 18  4\n\n\nNote that the dimensions are reported as a \\(n \\times m\\) matrix with \\(n\\) rows and \\(m\\) columns.\n\n\nTask 1.3.2b: view()\nUse the View() function to see all of your data in a separate window.\n\n# view the data\nView(df1)\n\n\n\nTask 1.3.2c: glimpse()\nTake a glimpse of your data using the glimpse() function.\n\n# get a glimpse of your data frame\nglimpse(df1)\n\nRows: 18\nColumns: 4\n$ Country        &lt;chr&gt; \"United States\", \"China\", \"India\", \"Mexico\", \"Peru\", \"F…\n$ percent.africa &lt;dbl&gt; 32.4, 31.6, 10.8, 6.5, 4.2, 2.1, 1.7, 1.5, 1.5, 1.3, 1.…\n$ area.sq.km     &lt;dbl&gt; 9.83, 9.60, 3.29, 1.96, 1.29, 0.64, 0.51, 0.46, 0.45, 0…\n$ area.sq.mi     &lt;dbl&gt; 3.80, 3.71, 1.27, 0.76, 0.50, 0.25, 0.20, 0.18, 0.17, 0…\n\n\n\n\nTask 1.3.2d: head()\nView the first six observations in your data using the head() function.\n\n# view the \"top\" of your data\nhead(df1)\n\n# A tibble: 6 × 4\n  Country       percent.africa area.sq.km area.sq.mi\n  &lt;chr&gt;                  &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 United States           32.4       9.83       3.8 \n2 China                   31.6       9.6        3.71\n3 India                   10.8       3.29       1.27\n4 Mexico                   6.5       1.96       0.76\n5 Peru                     4.2       1.29       0.5 \n6 France                   2.1       0.64       0.25\n\n\n\n\nTask 1.3.2e: tail()\nView the last six observations in your data using the tail() function.\n\n# view the \"bottom\" of your data\ntail(df1)\n\n# A tibble: 6 × 4\n  Country        percent.africa area.sq.km area.sq.mi\n  &lt;chr&gt;                   &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 Italy                     1         0.3        0.12\n2 New Zealand               0.9       0.27       0.1 \n3 United Kingdom            0.8       0.24       0.09\n4 Nepal                     0.5       0.15       0.06\n5 Bangladesh                0.5       0.15       0.06\n6 Greece                    0.4       0.13       0.05\n\n\n\n\nTask 1.3.2f: Specify n in head() or tail()\nYou can change the number of observations viewed by being more explicit in your code.\n\nhead(df1, n = 10) # view the top 10 observations\n\n# A tibble: 10 × 4\n   Country          percent.africa area.sq.km area.sq.mi\n   &lt;chr&gt;                     &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1 United States              32.4       9.83       3.8 \n 2 China                      31.6       9.6        3.71\n 3 India                      10.8       3.29       1.27\n 4 Mexico                      6.5       1.96       0.76\n 5 Peru                        4.2       1.29       0.5 \n 6 France                      2.1       0.64       0.25\n 7 Spain                       1.7       0.51       0.2 \n 8 Papua New Guinea            1.5       0.46       0.18\n 9 Sweden                      1.5       0.45       0.17\n10 Japan                       1.3       0.38       0.15\n\ntail(df1, n = 3) # view the bottom 3 observations\n\n# A tibble: 3 × 4\n  Country    percent.africa area.sq.km area.sq.mi\n  &lt;chr&gt;               &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 Nepal                 0.5       0.15       0.06\n2 Bangladesh            0.5       0.15       0.06\n3 Greece                0.4       0.13       0.05\n\n\n\n\nTask 1.3.2g: summary()\nGet a summary of all variables in the data set.\n\n# get a summary of your data frame\nsummary(df1)\n\n   Country          percent.africa     area.sq.km       area.sq.mi    \n Length:18          Min.   : 0.400   Min.   :0.1300   Min.   :0.0500  \n Class :character   1st Qu.: 0.925   1st Qu.:0.2775   1st Qu.:0.1050  \n Mode  :character   Median : 1.400   Median :0.4150   Median :0.1600  \n                    Mean   : 5.556   Mean   :1.6850   Mean   :0.6522  \n                    3rd Qu.: 3.675   3rd Qu.:1.1275   3rd Qu.:0.4375  \n                    Max.   :32.400   Max.   :9.8300   Max.   :3.8000",
    "crumbs": [
      "Appendix",
      "Case Studies",
      "Case Study 2, pt. 1"
    ]
  },
  {
    "objectID": "cases/case02-pt1.html#task-1.4-use-dplyr-verbs-on-true_size",
    "href": "cases/case02-pt1.html#task-1.4-use-dplyr-verbs-on-true_size",
    "title": "Case Study 2, Part 1: The True Size of Africa",
    "section": "Task 1.4: Use dplyr verbs on true_size",
    "text": "Task 1.4: Use dplyr verbs on true_size\nIn this section, we will review the dplyr verbs that help us get our data into a format that works for our analysis. These verbs can be used in any order. We can also use the pipe operator %&gt;% and use multiple verbs in a single code chunk.\n\nTask 1.4.1: Subset columns (variables) using select()\nThe select() command retains only those columns that are listed.\nIt uses the logic select(data, variable1, variable2, ...).\n\n# keep only the Country and percent.africa columns\nselect(df1, Country, percent.africa) \n\n# A tibble: 18 × 2\n   Country          percent.africa\n   &lt;chr&gt;                     &lt;dbl&gt;\n 1 United States              32.4\n 2 China                      31.6\n 3 India                      10.8\n 4 Mexico                      6.5\n 5 Peru                        4.2\n 6 France                      2.1\n 7 Spain                       1.7\n 8 Papua New Guinea            1.5\n 9 Sweden                      1.5\n10 Japan                       1.3\n11 Germany                     1.2\n12 Norway                      1.1\n13 Italy                       1  \n14 New Zealand                 0.9\n15 United Kingdom              0.8\n16 Nepal                       0.5\n17 Bangladesh                  0.5\n18 Greece                      0.4\n\n# keep only the Country and area.sq.mi columns\nselect(df1, Country, area.sq.mi)\n\n# A tibble: 18 × 2\n   Country          area.sq.mi\n   &lt;chr&gt;                 &lt;dbl&gt;\n 1 United States          3.8 \n 2 China                  3.71\n 3 India                  1.27\n 4 Mexico                 0.76\n 5 Peru                   0.5 \n 6 France                 0.25\n 7 Spain                  0.2 \n 8 Papua New Guinea       0.18\n 9 Sweden                 0.17\n10 Japan                  0.15\n11 Germany                0.14\n12 Norway                 0.13\n13 Italy                  0.12\n14 New Zealand            0.1 \n15 United Kingdom         0.09\n16 Nepal                  0.06\n17 Bangladesh             0.06\n18 Greece                 0.05\n\n\nWe can also subset columns by deleting others\nTo do so, we use a minus sign ahead of the column names.\n\n# remove the area.sq.mi variable\nselect(df1, -area.sq.km)\n\n# A tibble: 18 × 3\n   Country          percent.africa area.sq.mi\n   &lt;chr&gt;                     &lt;dbl&gt;      &lt;dbl&gt;\n 1 United States              32.4       3.8 \n 2 China                      31.6       3.71\n 3 India                      10.8       1.27\n 4 Mexico                      6.5       0.76\n 5 Peru                        4.2       0.5 \n 6 France                      2.1       0.25\n 7 Spain                       1.7       0.2 \n 8 Papua New Guinea            1.5       0.18\n 9 Sweden                      1.5       0.17\n10 Japan                       1.3       0.15\n11 Germany                     1.2       0.14\n12 Norway                      1.1       0.13\n13 Italy                       1         0.12\n14 New Zealand                 0.9       0.1 \n15 United Kingdom              0.8       0.09\n16 Nepal                       0.5       0.06\n17 Bangladesh                  0.5       0.06\n18 Greece                      0.4       0.05\n\n# remove the listed variables in the data frame\nselect(df1, -area.sq.km, -area.sq.mi) \n\n# A tibble: 18 × 2\n   Country          percent.africa\n   &lt;chr&gt;                     &lt;dbl&gt;\n 1 United States              32.4\n 2 China                      31.6\n 3 India                      10.8\n 4 Mexico                      6.5\n 5 Peru                        4.2\n 6 France                      2.1\n 7 Spain                       1.7\n 8 Papua New Guinea            1.5\n 9 Sweden                      1.5\n10 Japan                       1.3\n11 Germany                     1.2\n12 Norway                      1.1\n13 Italy                       1  \n14 New Zealand                 0.9\n15 United Kingdom              0.8\n16 Nepal                       0.5\n17 Bangladesh                  0.5\n18 Greece                      0.4\n\n\n\n\nTask 1.4.2: Filter rows (cases) using filter()\nfilter() allows us to select rows based on specific criteria.\n\n# keep only those rows where the percent.africa value is grater than 30\nfilter(df1, percent.africa &gt; 30)\n\n# A tibble: 2 × 4\n  Country       percent.africa area.sq.km area.sq.mi\n  &lt;chr&gt;                  &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 United States           32.4       9.83       3.8 \n2 China                   31.6       9.6        3.71\n\n# keep only those rows where the percent.africa value is less than 1\nfilter(df1, percent.africa &lt; 1)\n\n# A tibble: 5 × 4\n  Country        percent.africa area.sq.km area.sq.mi\n  &lt;chr&gt;                   &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 New Zealand               0.9       0.27       0.1 \n2 United Kingdom            0.8       0.24       0.09\n3 Nepal                     0.5       0.15       0.06\n4 Bangladesh                0.5       0.15       0.06\n5 Greece                    0.4       0.13       0.05\n\n# keep only those rows where percent.africa is less than 10 and greater than 1\nfilter(df1, percent.africa &lt; 10 & percent.africa &gt; 1)\n\n# A tibble: 9 × 4\n  Country          percent.africa area.sq.km area.sq.mi\n  &lt;chr&gt;                     &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 Mexico                      6.5       1.96       0.76\n2 Peru                        4.2       1.29       0.5 \n3 France                      2.1       0.64       0.25\n4 Spain                       1.7       0.51       0.2 \n5 Papua New Guinea            1.5       0.46       0.18\n6 Sweden                      1.5       0.45       0.17\n7 Japan                       1.3       0.38       0.15\n8 Germany                     1.2       0.36       0.14\n9 Norway                      1.1       0.32       0.13\n\n# keep only those rows where Country is equal to \"China\"\nfilter(df1, Country == \"China\")\n\n# A tibble: 1 × 4\n  Country percent.africa area.sq.km area.sq.mi\n  &lt;chr&gt;            &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 China             31.6        9.6       3.71\n\n\nNotice the use of a == equal sign when referencing a cell’s value. This will become an important component of base logic and analysis when writing code.\n\n\nTask 1.4.3: Add/remove columns (variables) using mutate()\nWe use mutate() to create new variables from existing variables.\nIt is not clear what area.sq.mi and area.sq.km refer to in our data. Further inspection and a bit of internet searching will show us that these values are in the millions and that they represent estimates.\nWe can transform these as follows:\n\n# add a new column with a more accurate label for land area estimate (sq mi)\nmutate(df1, est.square.miles = df1$area.sq.mi * 1000000) \n\n# A tibble: 18 × 5\n   Country          percent.africa area.sq.km area.sq.mi est.square.miles\n   &lt;chr&gt;                     &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;            &lt;dbl&gt;\n 1 United States              32.4       9.83       3.8           3800000\n 2 China                      31.6       9.6        3.71          3710000\n 3 India                      10.8       3.29       1.27          1270000\n 4 Mexico                      6.5       1.96       0.76           760000\n 5 Peru                        4.2       1.29       0.5            500000\n 6 France                      2.1       0.64       0.25           250000\n 7 Spain                       1.7       0.51       0.2            200000\n 8 Papua New Guinea            1.5       0.46       0.18           180000\n 9 Sweden                      1.5       0.45       0.17           170000\n10 Japan                       1.3       0.38       0.15           150000\n11 Germany                     1.2       0.36       0.14           140000\n12 Norway                      1.1       0.32       0.13           130000\n13 Italy                       1         0.3        0.12           120000\n14 New Zealand                 0.9       0.27       0.1            100000\n15 United Kingdom              0.8       0.24       0.09            90000\n16 Nepal                       0.5       0.15       0.06            60000\n17 Bangladesh                  0.5       0.15       0.06            60000\n18 Greece                      0.4       0.13       0.05            50000\n\n\n\nTask 1.4.3a: Add new variables using the pipe %&gt;%\nWe can also use the pipe operator to mutate the variable.\n\n# we can create the same output when using the %&gt;% (pipe)\ndf1 %&gt;%\n  mutate(est.square.miles = df1$area.sq.mi * 1000000) \n\n# A tibble: 18 × 5\n   Country          percent.africa area.sq.km area.sq.mi est.square.miles\n   &lt;chr&gt;                     &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;            &lt;dbl&gt;\n 1 United States              32.4       9.83       3.8           3800000\n 2 China                      31.6       9.6        3.71          3710000\n 3 India                      10.8       3.29       1.27          1270000\n 4 Mexico                      6.5       1.96       0.76           760000\n 5 Peru                        4.2       1.29       0.5            500000\n 6 France                      2.1       0.64       0.25           250000\n 7 Spain                       1.7       0.51       0.2            200000\n 8 Papua New Guinea            1.5       0.46       0.18           180000\n 9 Sweden                      1.5       0.45       0.17           170000\n10 Japan                       1.3       0.38       0.15           150000\n11 Germany                     1.2       0.36       0.14           140000\n12 Norway                      1.1       0.32       0.13           130000\n13 Italy                       1         0.3        0.12           120000\n14 New Zealand                 0.9       0.27       0.1            100000\n15 United Kingdom              0.8       0.24       0.09            90000\n16 Nepal                       0.5       0.15       0.06            60000\n17 Bangladesh                  0.5       0.15       0.06            60000\n18 Greece                      0.4       0.13       0.05            50000\n\n# add a new column with a more accurate label for land area estimate (sq km)\ndf1 %&gt;% \n  mutate(est.square.km = df1$area.sq.km * 1000000)\n\n# A tibble: 18 × 5\n   Country          percent.africa area.sq.km area.sq.mi est.square.km\n   &lt;chr&gt;                     &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;\n 1 United States              32.4       9.83       3.8        9830000\n 2 China                      31.6       9.6        3.71       9600000\n 3 India                      10.8       3.29       1.27       3290000\n 4 Mexico                      6.5       1.96       0.76       1960000\n 5 Peru                        4.2       1.29       0.5        1290000\n 6 France                      2.1       0.64       0.25        640000\n 7 Spain                       1.7       0.51       0.2         510000\n 8 Papua New Guinea            1.5       0.46       0.18        460000\n 9 Sweden                      1.5       0.45       0.17        450000\n10 Japan                       1.3       0.38       0.15        380000\n11 Germany                     1.2       0.36       0.14        360000\n12 Norway                      1.1       0.32       0.13        320000\n13 Italy                       1         0.3        0.12        300000\n14 New Zealand                 0.9       0.27       0.1         270000\n15 United Kingdom              0.8       0.24       0.09        240000\n16 Nepal                       0.5       0.15       0.06        150000\n17 Bangladesh                  0.5       0.15       0.06        150000\n18 Greece                      0.4       0.13       0.05        130000\n\n\n\n\nTask 1.4.3b: Add new variables and remove old variables\nIt is easiest to put all of the verbs together in a single chunk of code.\n\n# remove the old column; use the pipe command to do both operations at once\ndf1 %&gt;%\n  mutate(est.square.miles = df1$area.sq.mi * 1000000) %&gt;% \n  mutate(est.square.km = df1$area.sq.km * 1000000) %&gt;% \n  select(-area.sq.mi, -area.sq.km) # remove the columns we do not want\n\n# A tibble: 18 × 4\n   Country          percent.africa est.square.miles est.square.km\n   &lt;chr&gt;                     &lt;dbl&gt;            &lt;dbl&gt;         &lt;dbl&gt;\n 1 United States              32.4          3800000       9830000\n 2 China                      31.6          3710000       9600000\n 3 India                      10.8          1270000       3290000\n 4 Mexico                      6.5           760000       1960000\n 5 Peru                        4.2           500000       1290000\n 6 France                      2.1           250000        640000\n 7 Spain                       1.7           200000        510000\n 8 Papua New Guinea            1.5           180000        460000\n 9 Sweden                      1.5           170000        450000\n10 Japan                       1.3           150000        380000\n11 Germany                     1.2           140000        360000\n12 Norway                      1.1           130000        320000\n13 Italy                       1             120000        300000\n14 New Zealand                 0.9           100000        270000\n15 United Kingdom              0.8            90000        240000\n16 Nepal                       0.5            60000        150000\n17 Bangladesh                  0.5            60000        150000\n18 Greece                      0.4            50000        130000\n\n\nThese values seem to represent the data more clearly.\n\n\n\nTask 1.4.4: Rename a column using rename()\nWe can also use rename() to modify a column’s label.\n\ndf1 %&gt;%\n  mutate(est.square.miles = df1$area.sq.mi * 1000000) %&gt;% \n  mutate(est.square.km = df1$area.sq.km * 1000000) %&gt;% \n  select(-area.sq.mi, -area.sq.km)  %&gt;% \n  rename(country = Country)  # make the 'C' in country lowercase\n\n# A tibble: 18 × 4\n   country          percent.africa est.square.miles est.square.km\n   &lt;chr&gt;                     &lt;dbl&gt;            &lt;dbl&gt;         &lt;dbl&gt;\n 1 United States              32.4          3800000       9830000\n 2 China                      31.6          3710000       9600000\n 3 India                      10.8          1270000       3290000\n 4 Mexico                      6.5           760000       1960000\n 5 Peru                        4.2           500000       1290000\n 6 France                      2.1           250000        640000\n 7 Spain                       1.7           200000        510000\n 8 Papua New Guinea            1.5           180000        460000\n 9 Sweden                      1.5           170000        450000\n10 Japan                       1.3           150000        380000\n11 Germany                     1.2           140000        360000\n12 Norway                      1.1           130000        320000\n13 Italy                       1             120000        300000\n14 New Zealand                 0.9           100000        270000\n15 United Kingdom              0.8            90000        240000\n16 Nepal                       0.5            60000        150000\n17 Bangladesh                  0.5            60000        150000\n18 Greece                      0.4            50000        130000\n\n\n\n\nTask 1.4.5: Use relocate() to reorder the columns\n\ndf1 %&gt;%\n  mutate(est.square.miles = df1$area.sq.mi * 1000000) %&gt;% \n  mutate(est.square.km = df1$area.sq.km * 1000000) %&gt;% \n  select(-area.sq.mi, -area.sq.km)  %&gt;% \n  rename(country = Country) %&gt;% \n  relocate(country, percent.africa, est.square.miles) # reorder the columns\n\n# A tibble: 18 × 4\n   country          percent.africa est.square.miles est.square.km\n   &lt;chr&gt;                     &lt;dbl&gt;            &lt;dbl&gt;         &lt;dbl&gt;\n 1 United States              32.4          3800000       9830000\n 2 China                      31.6          3710000       9600000\n 3 India                      10.8          1270000       3290000\n 4 Mexico                      6.5           760000       1960000\n 5 Peru                        4.2           500000       1290000\n 6 France                      2.1           250000        640000\n 7 Spain                       1.7           200000        510000\n 8 Papua New Guinea            1.5           180000        460000\n 9 Sweden                      1.5           170000        450000\n10 Japan                       1.3           150000        380000\n11 Germany                     1.2           140000        360000\n12 Norway                      1.1           130000        320000\n13 Italy                       1             120000        300000\n14 New Zealand                 0.9           100000        270000\n15 United Kingdom              0.8            90000        240000\n16 Nepal                       0.5            60000        150000\n17 Bangladesh                  0.5            60000        150000\n18 Greece                      0.4            50000        130000\n\n\n\n\nTask 1.4.6: Overwrite your data frame\nNow that we’ve restructured the data into a format that is more accurate, we can reassign our data frame using the pipe operator. Take note of how the code is built with new commands starting on a new line and ending with the %&gt;% operator.\n\ntrue_size_modified &lt;- df1 %&gt;%\n  mutate(est.square.miles = df1$area.sq.mi * 1000000) %&gt;% \n  mutate(est.square.km = df1$area.sq.km * 1000000) %&gt;% \n  select(-area.sq.mi, -area.sq.km)  %&gt;% \n  rename(country = Country) %&gt;% \n  relocate(country, percent.africa, est.square.miles) # reorder the columns\n\nNow view your modified data frame.\n\ntrue_size_modified\n\n# A tibble: 18 × 4\n   country          percent.africa est.square.miles est.square.km\n   &lt;chr&gt;                     &lt;dbl&gt;            &lt;dbl&gt;         &lt;dbl&gt;\n 1 United States              32.4          3800000       9830000\n 2 China                      31.6          3710000       9600000\n 3 India                      10.8          1270000       3290000\n 4 Mexico                      6.5           760000       1960000\n 5 Peru                        4.2           500000       1290000\n 6 France                      2.1           250000        640000\n 7 Spain                       1.7           200000        510000\n 8 Papua New Guinea            1.5           180000        460000\n 9 Sweden                      1.5           170000        450000\n10 Japan                       1.3           150000        380000\n11 Germany                     1.2           140000        360000\n12 Norway                      1.1           130000        320000\n13 Italy                       1             120000        300000\n14 New Zealand                 0.9           100000        270000\n15 United Kingdom              0.8            90000        240000\n16 Nepal                       0.5            60000        150000\n17 Bangladesh                  0.5            60000        150000\n18 Greece                      0.4            50000        130000\n\n\n\n\nTask 1.4.7: Explore summarise(), arrange(), and other verbs\nThere are other verbs in the dplyr package.\nFor example, we will use the summarise() and arrange() commands in the next part of the case study.\nYou can learn more about other dplyr data transformations here.\n\nNow that you have a few tools to explore and modify data frames, we return to the africa_data_all data in the critstats package. We will use this data frame to work more with dplyr and explore univariate analyses.",
    "crumbs": [
      "Appendix",
      "Case Studies",
      "Case Study 2, pt. 1"
    ]
  },
  {
    "objectID": "cases/case02-pt1.html#report-1.1",
    "href": "cases/case02-pt1.html#report-1.1",
    "title": "Case Study 2, Part 1: The True Size of Africa",
    "section": "Report 1.1",
    "text": "Report 1.1\nWhat are some of the main concepts in Vaughan (2018)? Could the data in the true_size data be used to respond to and advance our understanding of the concepts in Vaughan (2018)? If so, how might it be used to examine both the social context (period) and historical consequences (uses) of inequity in map making? If not, explain.",
    "crumbs": [
      "Appendix",
      "Case Studies",
      "Case Study 2, pt. 1"
    ]
  },
  {
    "objectID": "cases/case02-pt1.html#report-1.2",
    "href": "cases/case02-pt1.html#report-1.2",
    "title": "Case Study 2, Part 1: The True Size of Africa",
    "section": "Report 1.2",
    "text": "Report 1.2\nIf the data in true_size_modified is accurate, what is the expected correlation between the percent.africa and est.square.miles variables? Specifically, why do we get the below scatter plot based on the data values?",
    "crumbs": [
      "Appendix",
      "Case Studies",
      "Case Study 2, pt. 1"
    ]
  },
  {
    "objectID": "cases/case02-pt1.html#report-1.3",
    "href": "cases/case02-pt1.html#report-1.3",
    "title": "Case Study 2, Part 1: The True Size of Africa",
    "section": "Report 1.3",
    "text": "Report 1.3\nBased on the original values in the true_size data, is there enough information to confirm if the claims about the size of Africa and the social politics of maps is true? Explain why or why not.",
    "crumbs": [
      "Appendix",
      "Case Studies",
      "Case Study 2, pt. 1"
    ]
  },
  {
    "objectID": "cases/case02-pt1.html#report-1.4",
    "href": "cases/case02-pt1.html#report-1.4",
    "title": "Case Study 2, Part 1: The True Size of Africa",
    "section": "Report 1.4",
    "text": "Report 1.4\nWrite code to add a new variable to the true_size_modified data called proportion.\nUse mutate() and arithmetic to generate the new variable.\nHint: If \\(Y\\) = proportion, then \\(y_i = \\dfrac{x_i}{sum(x)}\\) for some variable \\(X\\) in data set.\n\n\nRows: 18\nColumns: 5\n$ country          &lt;chr&gt; \"United States\", \"China\", \"India\", \"Mexico\", \"Peru\", …\n$ percent.africa   &lt;dbl&gt; 32.4, 31.6, 10.8, 6.5, 4.2, 2.1, 1.7, 1.5, 1.5, 1.3, …\n$ est.square.miles &lt;dbl&gt; 3800000, 3710000, 1270000, 760000, 500000, 250000, 20…\n$ est.square.km    &lt;dbl&gt; 9830000, 9600000, 3290000, 1960000, 1290000, 640000, …\n$ proportion       &lt;dbl&gt; 0.323679727, 0.316013629, 0.108177172, 0.064735945, 0…\n\n\n# A tibble: 6 × 5\n  country       percent.africa est.square.miles est.square.km proportion\n  &lt;chr&gt;                  &lt;dbl&gt;            &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n1 United States           32.4          3800000       9830000     0.324 \n2 China                   31.6          3710000       9600000     0.316 \n3 India                   10.8          1270000       3290000     0.108 \n4 Mexico                   6.5           760000       1960000     0.0647\n5 Peru                     4.2           500000       1290000     0.0426\n6 France                   2.1           250000        640000     0.0213\n\n\nOverwrite true_size_modified with a new data framed titled true_size_updated.\nYou should be able to call the data as follows:\n\ntrue_size_updated\n\n# A tibble: 18 × 5\n   country          percent.africa est.square.miles est.square.km proportion\n   &lt;chr&gt;                     &lt;dbl&gt;            &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n 1 United States              32.4          3800000       9830000    0.324  \n 2 China                      31.6          3710000       9600000    0.316  \n 3 India                      10.8          1270000       3290000    0.108  \n 4 Mexico                      6.5           760000       1960000    0.0647 \n 5 Peru                        4.2           500000       1290000    0.0426 \n 6 France                      2.1           250000        640000    0.0213 \n 7 Spain                       1.7           200000        510000    0.0170 \n 8 Papua New Guinea            1.5           180000        460000    0.0153 \n 9 Sweden                      1.5           170000        450000    0.0145 \n10 Japan                       1.3           150000        380000    0.0128 \n11 Germany                     1.2           140000        360000    0.0119 \n12 Norway                      1.1           130000        320000    0.0111 \n13 Italy                       1             120000        300000    0.0102 \n14 New Zealand                 0.9           100000        270000    0.00852\n15 United Kingdom              0.8            90000        240000    0.00767\n16 Nepal                       0.5            60000        150000    0.00511\n17 Bangladesh                  0.5            60000        150000    0.00511\n18 Greece                      0.4            50000        130000    0.00426",
    "crumbs": [
      "Appendix",
      "Case Studies",
      "Case Study 2, pt. 1"
    ]
  },
  {
    "objectID": "cases/case02-pt1.html#report-1.5",
    "href": "cases/case02-pt1.html#report-1.5",
    "title": "Case Study 2, Part 1: The True Size of Africa",
    "section": "Report 1.5",
    "text": "Report 1.5\nTo confirm that you have generated the new variable and overwritten the data frame, create the plot below using the following code. Be sure to add your first name and last name in the quotes.\nThe code should be verbatim with the exception of your name.\n\nplot(true_size_updated$proportion, true_size_updated$percent.africa)\ntitle(\"first.name last.name\") # Add your first name and last name\n\n\n\n\n\n\n\n\nFor example, my code would read:\n\nplot(true_size_updated$proportion, true_size_updated$percent.africa)\ntitle(\"Nathan Alexander\") # Add your first name and last name\n\n\n\n\n\n\n\n\nNotice the use of quotes when using title() under a plot.\nIf you experience issues making the plot, it is likely that you have made an error in the previous steps. Remember that R is case sensitive in all instances, and space sensitive in some instances.\nPlease be sure to go back and carefully check your code.\n{Save this plot as a pdf and submit it with case study files.",
    "crumbs": [
      "Appendix",
      "Case Studies",
      "Case Study 2, pt. 1"
    ]
  },
  {
    "objectID": "cases/case01.html",
    "href": "cases/case01.html",
    "title": "Case Study 1: Tuskegee Study of Untreated Syphillis in the Negro Male",
    "section": "",
    "text": "In Case Study 1, we will explore the ethics and quantification.",
    "crumbs": [
      "Appendix",
      "Case Studies",
      "Case Study 1"
    ]
  },
  {
    "objectID": "cases/case01.html#learning-objectives",
    "href": "cases/case01.html#learning-objectives",
    "title": "Case Study 1: Tuskegee Study of Untreated Syphillis in the Negro Male",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nThe goal of this lab is to introduce you to or remind you of the historical case that contributed to the development of the Institutional Review Board. This case is the Tuskegee Study of Untreated Syphilis in the Negro Male. Case files can be found below and in week 1.",
    "crumbs": [
      "Appendix",
      "Case Studies",
      "Case Study 1"
    ]
  },
  {
    "objectID": "cases/case01.html#learning-activities",
    "href": "cases/case01.html#learning-activities",
    "title": "Case Study 1: Tuskegee Study of Untreated Syphillis in the Negro Male",
    "section": "Learning Activities",
    "text": "Learning Activities\nBy the end of this case study you will be able to:\n\nIdentify historical issues of ethics and quantification\nDiscuss the Institutional Review Board (IRB) and its purposes",
    "crumbs": [
      "Appendix",
      "Case Studies",
      "Case Study 1"
    ]
  },
  {
    "objectID": "cases/case01.html#case-study-1-tuskegee-experiement-of-untreated-syphillis",
    "href": "cases/case01.html#case-study-1-tuskegee-experiement-of-untreated-syphillis",
    "title": "Case Study 1: Tuskegee Study of Untreated Syphillis in the Negro Male",
    "section": "Case Study 1: Tuskegee Experiement of Untreated Syphillis",
    "text": "Case Study 1: Tuskegee Experiement of Untreated Syphillis\nTo help you prepare for our forthcoming discussions and readings, you should explore information about our first case study. One place to start is here: “The Tuskegee Experiment: Crash Course Black American History #29” in the video below:",
    "crumbs": [
      "Appendix",
      "Case Studies",
      "Case Study 1"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to DATA 202",
    "section": "",
    "text": "Part\nWeek\nDay\nTopics\nLabs\nPapers\nMain reading\n\n\n\n\nI\n1\nWed, Jan 15\nIntroduction\nLab 0\n\n\n\n\n\n2\nWed, Jan 22\nFoundations\n\n\nPelham (2013, ch. 1)\n\n\n\n3\nWed, Jan 29\nTheory construction\n\n\nCase study 1 files\n\n\n\n4\nWed, Feb 5\nProbability theory\n\n\nFrisby (2024)\n\n\nII\n5\nWed, Feb 12\nUnivariate analysis\nLab 1\n\nCase study 2 files\n\n\n\n6\nWed, Feb 19\nBivariate analysis\n\n\n\n\n\n\n7\nWed, Feb 26\nExploratory analysis\n\nPaper 1\n\n\n\n\n8\nWed, Mar 12\nHypothesis testing\n\n\n\n\n\nIII\n9\nWed, Mar 19\nNotes on causal theories\n\n\nCase study 3 files\n\n\n\n10\nWed, Mar 26\nModeling social in/justice\nLab 2\n\n\n\n\n\n11\nWed, Apr 2\nOn regression\n\n\n\n\n\n\n12\nWed, Apr 9\nMore on regression\n\n\n\n\n\nIV\n13\nWed, Apr 16\nStatistical learning\n\nPaper 2\nCase study 4 files\n\n\n\n14\nWed, Apr 23\nOverview of additional models",
    "crumbs": [
      "Course information",
      "Course landing page"
    ]
  },
  {
    "objectID": "index.html#site-description",
    "href": "index.html#site-description",
    "title": "Welcome to DATA 202",
    "section": "Site Description",
    "text": "Site Description\nThe pages on this site outline the coding components of the DATA 202: Statistically Measuring and Modeling Social Justice course at Howard University, taught by me: Professor Nathan Alexander.\nThe information on this page will serve as a technical companion to our Canvas site.\nThis page will support you in our course in the following ways:\n\nThis page will help you work with data sets and complete small tasks over time.\nThis page will walk you through the measurement aspect of research questions relevant to modeling issues of social justice.\nThis page will help you integrate mathematical ideas and technical formulas into your work.\nThis page will support your use of theory in your statistical practice, and increase your ability to document reproducible code around a set of research questions.\n\nOur canvas site (click here) contains readings and is where you will submit all assignments.\nThis site should be paired with Canvas to help you complete course assignments.\nAnd just for good measure: all assignments should be turned in via Canvas.\nWhen turning in assignments on Canvas:\n\nGenerally, PDF submissions are the way to go.\nI will not download any files not uploaded to Canvas. I will only click on a clearly visible Github repository (i.e., no shortened or bit.ly links). Article citations that include DOIs are preferred.\nWe will discuss Quarto Markdown files which integrate code, formulas, and text. Quarto is actually how I created this site for our course, and it is a great way to communicate your technical ideas.\nThis is a Canvas companion site/page (I’ll use these terms interchangeably) and it will serve as your guide to completing all technical assignments.\n\n\nGiving credit\nThe below websites were used when building this course site. I am thankful for those who have provided open source resources such as these that allow others to generate new products.\n\nDuke University’s STAT 101 course site\nNoli Brazil’s Quantitative Methods in Community Research\n\n\n\nNext up: Overview\nThe overview in the next section will walk you through the technical flow of the course.\nLike this section, a preview of forthcoming topics will be included at the bottom of each page; this means that you can treat this site like a textbook that you would read in order from start to finish. Though I encourage you to explore what else is out there and make use of other resources. That said, submissions should follow the formal guidelines outlined on our syllabus, Canvas, and this site.\n\nThe syllabus provides a high-level view of the course.\nCanvas is the go-to for submissions and provides readings and assignments.\nThis course site provides technical items and code to help you complete your assignments.\n\nComing up next are the four parts of the course and an overview of the assignments for the term. Course topics are outlined in more detail and integrated into the assignment submission schedule on the next page; see you over there!",
    "crumbs": [
      "Course information",
      "Course landing page"
    ]
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "This page will contain resources to support you in our course.",
    "crumbs": [
      "Appendix",
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#the-organic-chemistry-tutors-channel-on-youtube",
    "href": "resources.html#the-organic-chemistry-tutors-channel-on-youtube",
    "title": "Resources",
    "section": "The Organic Chemistry Tutor’s Channel on YouTube",
    "text": "The Organic Chemistry Tutor’s Channel on YouTube\nDespite the profile’s title, this channel offers a wealth of resources spanning mathematics and science. The resources offer detailed examples and explanations that you can follow along with as you watch the video. The below series of video review basic statistics.",
    "crumbs": [
      "Appendix",
      "Resources"
    ]
  }
]