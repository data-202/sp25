{
  "hash": "834f3d4f2dcd1dbcc77d3dca8dae07cd",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"DATA 202 - Week 12\"\nsubtitle: \"On regression\"\nauthor: \"Nathan Alexander, PhD\"\ninstitute: \"Center for Applied Data Science and Analytics\"\nformat: \n  html: default\n  revealjs:\n    output-file: week12-slides.html\n    height: 900\n    width: 1600\n    smaller: false\n    scrollable: true\n    slide-number: c/t #< collapsted/total\n    logo: \"img/howard-logo.jpg\"\n    footer: \"[Course Data GitHub](https://github.com/data-202)\"\n    toc: false\n    echo: true\n    incremental: false\n    self-contained: true\n---\n\n\n\n\n\n\nWe explore the case of bivariate regression when attempting to measure and model conceptions of in/justice.\n\n## Part I: Context\n\nThis week we focus on more examples from our darta sources; we'll take a closer look at the General Social Survey data, or GSS.\n\nThe data is located in the `gssr` package in R (see part III). You should become very familiar with the GSS data. Please explore the [website](https://gss.norc.org/) as we will be prioritizing the use of the GSS data for the remainder of our course. The use of the GSS data will allow us to consider the meaning of *social justice* in the context of attitudes and beliefs around social issues.\n\nAhead of modeling multiple variables, we'll examine the importance of underlying theoretical anlayses ahead of solely doing statistical investigations.\n\n------------------------------------------------------------------------\n\nLet us begin with a set of three variables: $x$, $y$, and $z$.\n\nWe will assume that there is a hypothesized association between all three variables. However, let us also assume that we have *not* yet taken time to properly structure the relationships between the variables. Namely, we have yet to consider if $z$, for example, is a mediator, moderator, or spurious variable.\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\nLet us take a look at the scatterplots between each pair of variables.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(x, y)\n```\n\n::: {.cell-output-display}\n![](week12_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n\n```{.r .cell-code}\ncor(x, y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.09107752\n```\n\n\n:::\n:::\n\n\n\n\n\n\nTake note of the value of the correlation coefficient.\n\n------------------------------------------------------------------------\n\nLet us take a look at the scatterplots between each pair of variables.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(x, z)\n```\n\n::: {.cell-output-display}\n![](week12_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n```{.r .cell-code}\ncor(x, z)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9284953\n```\n\n\n:::\n:::\n\n\n\n\n\n\nTake note of the value of the correlation coefficient.\n\n------------------------------------------------------------------------\n\nLet us take a look at the scatterplots between each pair of variables.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(y, z)\n```\n\n::: {.cell-output-display}\n![](week12_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n```{.r .cell-code}\ncor(y, z)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.04660941\n```\n\n\n:::\n:::\n\n\n\n\n\n\nTake note of the value of the correlation coefficient.\n\n------------------------------------------------------------------------\n\n### Regression assumptions\n\nIn our previous lectures, we have discussed the function of assumptions and principles in regression analysis. In progressing towards different types of tests, it is important to consider the specific assumptions for any given test.\n\nAnalyzing relationships among social science variables has an assumptions of linearity. However, this assumptions is not always correct. The adoption of this assumption is based on a host of factors. Most notably, that many relationships have been found to be linear when considered in the empirical sense.\n\nSome additional assumptions are as follows:\n\n-   The sample is representative of the population\n\n-   The variables of interest are normally distributed\n\n-   There are no outliers in the data\n\n-   Independence\n\n-   There is a linear relationship between the independent variable(s) and dependent variable(s)\n\n------------------------------------------------------------------------\n\n## Part II: Content\n\n### A tradition: Fitting a straight line\n\nIn statistics, we often inquire about the relationship between two variables, $X$ and $Y$.\n\nThese variables are sets that contain values (observations) as noted before. To launch our discussion, we will begin with a sample line.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(0:10, type=\"l\", main = \"A sample line\", xlab = \"X-axis\", ylab = \"Y-axis\")\n```\n\n::: {.cell-output-display}\n![](week12_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n------------------------------------------------------------------------\n\n#### Matheamtics and exact relationships\n\nIn mathematics, two variables, $x$ and $y$ may be related to each other in various ways. These relationships form the basis for many inquiries in statistics. However, mathematical modeling most often involves identifying the parameters in which we can use to assess the validity of the stated relationship. In this way, we examine the difference between exact and inexact relationships.\n\n------------------------------------------------------------------------\n\n##### A straight line\n\nThe line in the graphic above follows the standard notation: $$y = a + bX$$\n\n------------------------------------------------------------------------\n\n#### Statistical modeling and inexact relationships\n\nRegression analysis is a standard analysis in many statistical studies.\n\nThere are many different types of regression. We'll continue with our exploration of bivariate regression analysis and focus on some of the base assumptions as it relates to study development.\n\nWe will begin by looking at the underlying assumptions of regression analysis.\n\nThese assumptions are the technical (or structural) components of our analyses, and should be checked at the initiation of a research study, starting with data collection or understanding how data was collected if it is a secondary analysis.\n\n------------------------------------------------------------------------\n\n##### Analyzing two numeric variables\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\nLet us take two variables, $x$ and $y$.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(x, y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.7293926\n```\n\n\n:::\n:::\n\n\n\n\n\n\n------------------------------------------------------------------------\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit simple linear regression model\nmodel <- lm(y ~ x)\n```\n:::\n\n\n\n\n\n\n------------------------------------------------------------------------\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Examine regression outputs\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4027 -1.0071  0.1154  1.0017  5.8115 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.07185    0.20452   0.351    0.726    \nx            2.36515    0.22408  10.555   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.035 on 98 degrees of freedom\nMultiple R-squared:  0.532,\tAdjusted R-squared:  0.5272 \nF-statistic: 111.4 on 1 and 98 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\n\n\n------------------------------------------------------------------------\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Check regression coefficients\ncoeff <- coef(model)\ncoeff\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)           x \n 0.07184615  2.36514669 \n```\n\n\n:::\n:::\n\n\n\n\n\n\n------------------------------------------------------------------------\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Check R-squared value\nrsq <- summary(model)$r.squared\nrsq\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5320136\n```\n\n\n:::\n:::\n\n\n\n\n\n\n------------------------------------------------------------------------\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Generate predictions\npred <- predict(model) # call this object to show the predicted values of the model\n```\n:::\n\n\n\n\n\n\n------------------------------------------------------------------------\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot the regression line\nplot(x, y)\nabline(model, col=\"blue\")\n```\n\n::: {.cell-output-display}\n![](week12_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n------------------------------------------------------------------------\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Check residuals\nresids <- residuals(model)\nplot(x, resids)\n```\n\n::: {.cell-output-display}\n![](week12_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n------------------------------------------------------------------------\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Diagnostic plots\npar(mfrow=c(2,2))\nplot(model)\n```\n\n::: {.cell-output-display}\n![](week12_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n------------------------------------------------------------------------\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Check significance of predictor\nanova(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nResponse: y\n          Df Sum Sq Mean Sq F value    Pr(>F)    \nx          1 461.44  461.44  111.41 < 2.2e-16 ***\nResiduals 98 405.91    4.14                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n\n\n\n------------------------------------------------------------------------\n\n#### Analyzing a categorical and a numeric variable\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n------------------------------------------------------------------------\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndesc(df$income)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] -25000 -35000 -42000 -31000 -27000 -38000 -29000 -40000 -32000 -39000\n```\n\n\n:::\n:::\n\n\n\n\n\n\n------------------------------------------------------------------------\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Bar plot of the categorical variable (race)\nggplot(df, aes(x = race)) +\n  geom_bar(fill = \"steelblue\") +\n  labs(x = \"Race\", y = \"Count\") +\n  ggtitle(\"Distribution of Races\")\n```\n\n::: {.cell-output-display}\n![](week12_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Box plot of income by race\nggplot(df, aes(x = race, y = income)) +\n  geom_boxplot(fill = \"lightgray\", color = \"steelblue\") +\n  labs(x = \"Race\", y = \"Income\") +\n  ggtitle(\"Income by Race\")\n```\n\n::: {.cell-output-display}\n![](week12_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n#### Analyzing two categorical variables\n\nIn previous lectures, we discussed some methods to hypothesize around the relationship between two variables that are categorical in nature.\n\n## Part III: Code\n\nThis week we'll explore the GSS and ANES data in more detail. This is a reminder to deepen your understanding of the data set you will use based on the notes provided below from [week 11](week11.qmd).\n\n### General Social Survey\n\nThis week we'll return to our examination of the General Social Survey (GSS) data.\n\nAs a first task, please identify up to two or three variables that you can utilize to follow along with your analysis.\n\n### **Next up**: Week 13\n",
    "supporting": [
      "week12_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}