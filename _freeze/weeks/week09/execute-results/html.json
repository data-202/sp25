{
  "hash": "b7e19b2861eb38e952104a2b30208c65",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"DATA 202 - Week 9\"\nsubtitle: \"Hypothesis Testing\"\nauthor: \"Nathan Alexander, PhD\"\ninstitute: \"Center for Applied Data Science and Analytics\"\nformat: \n  html: default\n  revealjs:\n    output-file: week09-slides.html\n    height: 900\n    width: 1600\n    smaller: false\n    scrollable: true\n    slide-number: c/t #< collapsted/total\n    logo: \"img/howard-logo.jpg\"\n    footer: \"[Course Data GitHub](https://github.com/data-202)\"\n    toc: false\n    echo: true\n    incremental: false\n---\n\n\n\n\n\n\n## Part I: Context\n\nWhat is your hypothesis? We begin today's class with a note about the different studies we are conducting and our hypotheses.\n\nHypothesis testing requires a solid theory to make sense of the relevant options to develop a hypothesis. \n\nAs a fundamental statistical tool, it is important to be critical wen dealing with social issues and the standars of hypothesis testing. QuantCrit (or critical quantitative methodologies) have been developed as a tool to help guide researchers in the quantitative social sciences, and in education, to consider how hypotheses can reinforce or challenge existing power structues and biases.\n\n---\n\nSome basic but important real-world examples:\n\n    Gender pay gap: Testing whether there's a significant difference in salaries between men and women in a particular industry.\n    \n    Racial disparities in healthcare: Examining if there's a significant difference in health outcomes between different racial groups.\n    \n    Educational achievement: Investigating the relationship between socioeconomic status and academic performance.\n\nIn each of these cases, it's essential to consider:\n\n- Who formulated the hypothesis and why?\n\n- What assumptions are built into the statistical methods?\n\n- How might the results be interpreted or misinterpreted?\n\n- What are the potential consequences of the findings?\n\n---\n\n### Considering your work\n\nAs you think about your own work, what are some of the main hypothesis that you hold as it relates to the relationship between your study variables? What are the variable types? How might you measure and make sense of the relationship between the variables?\n    \n## Part II: Content\n\nNull and Alternative Hypotheses\n\n        Null hypothesis (H₀): Statement of no effect or no difference\n        \n        Alternative hypothesis (H₁): Statement of an effect or difference\n\n---\n\nTest Statistic\n\n        A measure that allows us to quantify the difference between the observed data and what we'd expect under the null hypothesis\n        \n        Common test statistics: z-score, t-statistic, F-statistic, chi-square statistic\n\n---\n\nProbability Distribution\n\n        The distribution of the test statistic under the null hypothesis\n        \n        Examples: Normal distribution, t-distribution, F-distribution, chi-square distribution\n\n---\n\np-value\n\n    The probability of obtaining test results at least as extreme as the observed results, assuming the null hypothesis is true\n    \n    Calculated as: p-value = P(test statistic ≥ observed value | H₀ is true)\n\n---\n\nSignificance Level (α)\n\n    The threshold below which we reject the null hypothesis\n    \n    Common values: 0.05, 0.01\n\n---\n\nDecision Rule\n\n    Reject H₀ if p-value ≤ α\n    \n    Fail to reject H₀ if p-value > α\n\n---\n\nType I and Type II Errors\n\n    Type I error: Rejecting H₀ when it's actually true (false positive)\n    \n    Type II error: Failing to reject H₀ when it's actually false (false negative)\n\n---\n\nPower\n\n    The probability of correctly rejecting a false null hypothesis\n    \n    Power = 1 - P(Type II error)\n\n## Part III: Code\n\n### Two Categorical Variables: Chi-Square Test of Independence\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example: Testing association between gender and voting preference\n\n# Create sample data\ngender <- c(rep(\"Male\", 100), rep(\"Female\", 100))\nvote <- c(rep(\"Party A\", 55), rep(\"Party B\", 45), rep(\"Party A\", 65), rep(\"Party B\", 35))\ndata <- data.frame(gender, vote)\n\n# Perform chi-square test\nresult <- chisq.test(table(data$gender, data$vote))\n\n# Print results\nprint(result)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's Chi-squared test with Yates' continuity correction\n\ndata:  table(data$gender, data$vote)\nX-squared = 1.6875, df = 1, p-value = 0.1939\n```\n\n\n:::\n\n```{.r .cell-code}\n# Interpret results\nif (result$p.value < 0.05) {\n  print(\"There is a significant association between gender and voting preference.\")\n} else {\n  print(\"There is no significant association between gender and voting preference.\")\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"There is no significant association between gender and voting preference.\"\n```\n\n\n:::\n:::\n\n\n\n\n\n\n---\n\n### Two Numeric Variables: Pearson Correlation Test\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example: Testing correlation between study hours and exam scores\n\n# Create sample data\nstudy_hours <- rnorm(100, mean = 20, sd = 5)\nexam_scores <- 2 * study_hours + rnorm(100, mean = 0, sd = 10)\ndata <- data.frame(study_hours, exam_scores)\n\n# Perform correlation test\nresult <- cor.test(data$study_hours, data$exam_scores)\n\n# Print results\nprint(result)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's product-moment correlation\n\ndata:  data$study_hours and data$exam_scores\nt = 10.318, df = 98, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.6118816 0.8040383\nsample estimates:\n      cor \n0.7215772 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Interpret results\nif (result$p.value < 0.05) {\n  print(paste(\"There is a significant correlation between study hours and exam scores. Correlation coefficient:\", round(result$estimate, 2)))\n} else {\n  print(\"There is no significant correlation between study hours and exam scores.\")\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"There is a significant correlation between study hours and exam scores. Correlation coefficient: 0.72\"\n```\n\n\n:::\n:::\n\n\n\n\n\n\n---\n\n### One Categorical and One Numeric Variable: Independent Samples t-test\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example: Testing difference in exam scores between two teaching methods\n\n# Create sample data\nmethod <- c(rep(\"Method A\", 50), rep(\"Method B\", 50))\nscores <- c(rnorm(50, mean = 75, sd = 10), rnorm(50, mean = 80, sd = 10))\ndata <- data.frame(method, scores)\n\n# Perform t-test\nresult <- t.test(scores ~ method, data = data)\n\n# Print results\nprint(result)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  scores by method\nt = -1.5882, df = 97.815, p-value = 0.1155\nalternative hypothesis: true difference in means between group Method A and group Method B is not equal to 0\n95 percent confidence interval:\n -8.173811  0.906603\nsample estimates:\nmean in group Method A mean in group Method B \n              74.64852               78.28212 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Interpret results\nif (result$p.value < 0.05) {\n  print(\"There is a significant difference in exam scores between the two teaching methods.\")\n} else {\n  print(\"There is no significant difference in exam scores between the two teaching methods.\")\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"There is no significant difference in exam scores between the two teaching methods.\"\n```\n\n\n:::\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}