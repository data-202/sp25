---
title: "DATA 202 - Week 10"
subtitle: "Notes on causal theories"
author: "Nathan Alexander, PhD"
institute: "Center for Applied Data Science and Analytics"
format: 
  html: default
  revealjs:
    output-file: week10-slides.html
    height: 900
    width: 1600
    smaller: false
    scrollable: true
    slide-number: c/t #< collapsted/total
    logo: "img/howard-logo.jpg"
    footer: "[Course Data GitHub](https://github.com/data-202)"
    toc: false
    echo: true
    incremental: false
---

## Part I: Context

![Illustration by Yuko Shimizu from [cigionline.org](https://www.cigionline.org/articles/as-geopolitical-blocs-vie-for-primacy-in-space-the-history-of-colonization-looms-large/){target="_blank"}.](img/wk8-a-3.jpg)

Last week launched part 3 of our course: data and policy, as in public policies.

Our goals this week will be to consider variations of justice for your particular area(s) of interest and study, and to cite references that allow us to understand the meaning of social justice in your specific intellectual domains.

---

### History, cause, and effect

Theory is often understood through an investigation of both the literature and the historical contexts surrounding a given issue. Take, for example, the legacies of colonization. As data and scientific developments expand into new fields, new questions arise as to what we can associate and how we understand these associations. For example, how does one theoretically relate a series of events from 150 years ago to a present set of conditions? What are the possibilities? What are the limitations? What are the cautionary tales?

Statistics was situated as a vehicle to understand and measure observations. As it becomes an ever popular vehicle to inform social justice discourses, the many real-world associations identified in society bring to mind the well-known and readily available cautionary tale:

> Correlation does not imply causation.

This popular statement relates to the ability (or inability) to deduce cause-and-effect relationships.

------------------------------------------------------------------------

### Causation

::: {.callout-note apperance="simple"}
**Causality** is a fundamental concept in scientific inquiry and statistical analysis. It refers to the relationship between cause and effect, where one event (the cause) is understood to be responsible for another event (the effect). In statistics, causal inference aims to identify and quantify these cause-effect relationships.
:::

Causality can be defined as a relationship between two events where:

- The cause precedes the effect in time.

- There is a consistent association between the cause and effect.

- Alternative explanations for the association have been ruled out.

---

### Correlation

In the traditional setting, correlation is framed as a statistical measure that relays the size and direction of the relationship between two or more variables. From a more critical framework, however, the idea of associations between variables and the attribution of relationships between variables must be both examined and interrogated.

Associations should be understood and initiated from the root of a theoretical framework.

If two variables are related, in a statistical context, we assume that their values *change* in some ordered fashion. These measures are often represented on an axis depicting the strength of the association.

![Image from statlect.com](img/wk8-a-2.png)

---

```{r}
#| echo: false
#| output: false
#| warning: false

library(ggplot2)
library(gridExtra)

# Function to generate correlated data
generate_correlated_data <- function(n, r) {
  x <- rnorm(n)
  y <- r * x + sqrt(1 - r^2) * rnorm(n)
  data.frame(x = x, y = y)
}

# Create plots for different correlation coefficients
create_plot <- function(r) {
  data <- generate_correlated_data(100, r)
  ggplot(data, aes(x, y)) +
    geom_point() +
    geom_smooth(method = "lm", se = FALSE, color = "red") +
    ggtitle(paste("r =", r)) +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.25))
}

```

```{r}
#| echo: false
#| output: true
#| warning: false

# Generate plots
plots <- lapply(seq(-1, 1, 0.25), create_plot)

# Arrange plots in a grid
grid.arrange(grobs = plots, ncol = 3)
```         


As one value of a first variable increases the other variable's values may also increase (positive relationship). Alternatively, as the value of one variable increases the other variable's values may decrease (negative relationship). Key questions to consider focus on how context and other factors shape any seeming bivariate relationship.

------------------------------------------------------------------------

#### Spurious correlation

At the intersection of mathematics and statistics is the concept of the [*spurious correlation*](https://www.jstor.org/stable/42001453){target="_blank"} (Ward, 2013). A few examples:

-   [Divorce rate has been found to correlate with margarine use](https://blogs.ams.org/blogonmathblogs/2017/04/10/divorce-and-margarine/){target="_blank"}.

-   Ice cream sales have been correlated with:

    -   Swimming deaths

    -   Shark attacks

    -   Crime rates

-   The use of the popular site [Facebook has been correlated with diminished well-being](https://academic.oup.com/jcmc/article/21/4/265/4161784){target="_blank"}.

These association help us see the need to investigate relationships further. What are possible unexplained relationships in these associations? We identify potential spurious variables when developing theoretical ideas about associations.

------------------------------------------------------------------------

### Causation

Causality ([See Sloman and Acnado (2015)](https://www.annualreviews.org/doi/full/10.1146/annurev-psych-010814-015135)) is a representation and principle of cause and effect. There are many different viewpoints of causation across disciplines and fields of study. Terminology matters in how one frames cause and effect.

There are four possible relationships between two variables:

-   X causes Y

-   Y causes X

-   X and Y are both caused by Z

-   X is not related to Y

While these relationships may exist in some combination, it is important to frame their differences.

------------------------------------------------------------------------

### Associating X and Y

Our initial framing might directly link the variables `X` and `Y`.

First, we have the association X causes Y.

```{mermaid}
%%| echo: false
graph LR
  A[X] --> B[Y]
```

Next, we have the association Y causes X.

```{mermaid}
%%| echo: false
graph LR
  A[Y] --> B[X]
```

Finally, we have the association X causes Y and Y causes X.

```{mermaid}
%%| echo: false
graph LR
  A[X] <--> B[Y]
  B --> A
```

------------------------------------------------------------------------

### Framing spurious relationships

However, after further reading, we may find that we need to integrate a third factor Z.

In this context, a third factor (the "third-cause fallacy") represents a spurious relationship.

```{mermaid}
%%| echo: false
graph LR
  A[X]
  B[Y]
  C[Z]
```

Spurious relationships denote an observed or hypothesized association.

There are important differences in how we depict and deal with third factor associations.

------------------------------------------------------------------------

### Confounding variables

Confounding is a causal concept that focuses on spurious or distorted associations.

Below, Z is considered a [confounding variable](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4017459/){target="_blank"}.

```{mermaid}
%%| echo: false
graph TD
  A[Z] --> B[X]
  A --> C[Y]
```

-   Z is related to both the independent, $X$, and dependent, $Y$, variables in the causal sense.

-   Your analysis should examine if the relationship between $X$ and $Y$ holds.

-   These interactions are described as *spurious associations*.

------------------------------------------------------------------------

### Mediating variables

A [mediating variable](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4165346/){target="_blank"} explains the process by which two variables are related.

```{mermaid}
%%| echo: false
graph LR
  A[X] --> B[Z]
  B --> C[Y]
```

-   Z provides the *mechanism* to relate the independent, $X$, and dependent, $Y$, variables.

------------------------------------------------------------------------

### Moderating variables

A [moderating variable](https://www.researchgate.net/profile/Hugh-Arnold/publication/240438039_Moderator_Variables_A_Clarification_of_Conceptual_Analytic_and_Psychometric_Issues/links/5ee391fb92851ce9e7dce2ae/Moderator-Variables-A-Clarification-of-Conceptual-Analytic-and-Psychometric-Issues.pdf){target="_blank"} explains the strength by which two variables are related.

```{mermaid}
%%| echo: false
graph LR
  A[X] --> B[Z]
  A --> C[Y]
  B --> C
```

-   Z explains the *strength of association* between the independent, $X$, and dependent, $Y$, variables.

Let us look at a concrete example.

------------------------------------------------------------------------

## Part II: Content

The methods used to collect sample data is extremely important.

If sample data are not collected in an appropriately, any resulting statistical analyses will be futile. As a result, planning a study by identifying research questions, the population and sample of interest, and considering the types of methods that will be used to analyze data that is collected are all essential parts in the statistical data analysis process.

------------------------------------------------------------------------

#### More on sampling methods

First, let's revisit and describe the different sampling methods:

There are two broad categories of selecting members of a population to generate sample data: probability sampling and non-probability sampling.

Within these two broad categories are other methods based on the needs of the study. Each of these methods is used to support statistical data analysis with some methods providing stronger evidence than others.

::: callout-DEFINITIONS

Probability sampling

:   involves the random selection of subjects in such a way that every member of a sample has the sample probability of being selected.

Non-probability sampling

:   involves the use of criteria to select data that is not based on an equal likelihood of selection.
:::

------------------------------------------------------------------------

##### Probability sampling methods

-   **Simple random sample** of $n$ subjects ensures that every possible sample of the same size $n$ has the same chance or likelihood of being chosen.

-   **Systematic sample** of $n$ subjects involves selecting every $k$th subject on some regular interval.

-   **Stratified sample** involves dividing the population up into strata (groups) with the same characteristics and then randomly sampling within those strata.

-   **Cluster sample** involves partitioning the population into clusters (groups), randomly selecting some clusters, and then selecting all members of the selected clusters.

------------------------------------------------------------------------

##### Non-prbability sampling methods

-   **Convenience sample** is data gathered from the most accessible or convenient source. Although this is easy and efficient, the data cannot produce generalizable results.

-   **Purposive sample** is data gathered based on the purposes of the research or the specific research question. This strategy includes clear criteria and rationale for inclusion.

-   **Snowball sample** is data gathered via recruitment by the other participants of a study. The number of subjects included "snowballs" as more contacts are generated.

------------------------------------------------------------------------

###### Multistage sampling

In larger studies, sometimes multistage sampling procedures are used to generate data. In this design, different samples are selected in different stages, and each stage might use a different sampling method. The end result may represent a complicated sampling design but it is often simpler and faster than some designs, such as a simple random sample.

------------------------------------------------------------------------

**Missing Data**

Data can be missing from a set, and the total number of elements can differ between two or more sets.

The differences in the number of elements can be due to a host of reasons, but often it is missing data. A data value can be *missing at random* or *not missing at random*. The amount of information missing from a data set can have a minimal impact or a major impact on a statistical analysis.

::: {.callout-note apperance="simple"}
**Note -- Missing data**

A data value is *missing completely at random* if the likelihood of it being missing is independent of its value or any other values in the data set is just as likely to be missing.

A data value is *not missing at random* if the missing value is related to the reason it is missing.
:::

------------------------------------------------------------------------

## Part III: Code

### Crime statistics

Data from 1973 on violent crime rates by US State in the `USArrests` data set.

Values are per 100,000 residents for assault, murder, and rape.

```{r}
head(USArrests)
tail(USArrests)
```

---

```{r}
cor(USArrests$UrbanPop, USArrests$Murder)
plot(USArrests$UrbanPop, USArrests$Murder,
     xlab = "Percent living in urban areas",
     ylab = "Murders per 100,000 residents")
```

What are some initial takeaways from this plot?

Without a guiding theory, where might our analyses take us?

---

### Education data

This data is borrowed from the folks at UCLA's Statistical Methods and Data Analytics office.

```{r}
#| echo: true
#| output: false
#| warning: false
require(foreign)
require(ggplot2)
require(MASS)

educ <- read.dta("https://stats.idre.ucla.edu/stat/stata/dae/nb_data.dta")
```

---

```{r}
#| echo: true
#| output: true
#| warning: false
head(educ)
tail(educ)

```

---

```{r}
#| echo: true
#| output: true
#| warning: false

educ <- within(educ, {
  prog <- factor(prog, levels = 1:3, labels = c("General", "Academic", "Vocational"))
  id <- factor(id)
})

summary(educ)
```

---

```{r}
#| echo: true
#| output: true
#| warning: false
ggplot(educ, aes(daysabs, fill = prog)) + geom_histogram(binwidth = 1) + facet_grid(prog ~ 
                                                                                     ., margins = TRUE, scales = "free")
```

What are some initial takeaways from this plot?

Without a guiding theory, where might our analyses take us?

---

### **Next up**: Week 11
